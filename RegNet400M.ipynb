{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train CIFAR10 with PyTorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torchsummary\n",
    "import os\n",
    "import time\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
    "# parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
    "# parser.add_argument('--resume', '-r', action='store_true',\n",
    "#                     help='resume from checkpoint')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define augmentation strategies\n",
    "\n",
    "def gauss_noise_tensor(img):\n",
    "    assert isinstance(img, torch.Tensor)\n",
    "    dtype = img.dtype\n",
    "    if not img.is_floating_point():\n",
    "        img = img.to(torch.float32)\n",
    "    \n",
    "    sigma = 25.0\n",
    "    \n",
    "    out = img + sigma * torch.randn_like(img)\n",
    "    \n",
    "    if out.dtype != dtype:\n",
    "        out = out.to(dtype)\n",
    "        \n",
    "    return out\n",
    "\n",
    "group_a=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "group_b=transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "group_c=transforms.Compose([\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)), \n",
    "    transforms.RandomRotation(degrees=(0, 180)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "group_d=transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=.5, hue=.3), \n",
    "    transforms.ToTensor(),\n",
    "    gauss_noise_tensor,\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "group_e=transforms.Compose([\n",
    "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "test_names=['A', 'B', 'C', 'D', 'E']\n",
    "test_groups=[group_a, group_b, group_c, group_d, group_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "def prepareData(group_i):\n",
    "  T=test_groups[group_i]\n",
    "  trainset=torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=T)\n",
    "  testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=test_groups[0])\n",
    "  \n",
    "  #trainset=torch.utils.data.Subset(trainset, range(int(len(trainset)/40)))\n",
    "  #testset=torch.utils.data.Subset(testset, range(int(len(testset)/20)))\n",
    "  \n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=4)\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=4)\n",
    "  return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cSE(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(cSE, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class sSE(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.Conv1x1 = nn.Conv2d(in_channels, 1, kernel_size=1, bias=False)\n",
    "        self.norm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, U):\n",
    "        q = self.Conv1x1(U) # U:[bs,c,h,w] to q:[bs,1,h,w]\n",
    "        q = self.norm(q)\n",
    "        return U * q # 广播机制\n",
    "\n",
    "class scSE(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.cSE = cSE(in_channels)\n",
    "        self.sSE = sSE(in_channels)\n",
    "\n",
    "    def forward(self, U):\n",
    "        U_sse = self.sSE(U)\n",
    "        U_cse = self.cSE(U)\n",
    "        return U_cse+U_sse\n",
    "\n",
    "class SE(nn.Module):\n",
    "    '''Squeeze-and-Excitation block.'''\n",
    "\n",
    "    def __init__(self, in_planes, se_planes):\n",
    "        super(SE, self).__init__()\n",
    "        self.se1 = nn.Conv2d(in_planes, se_planes, kernel_size=1, bias=True)\n",
    "        self.se2 = nn.Conv2d(se_planes, in_planes, kernel_size=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        out = F.relu(self.se1(out))\n",
    "        out = self.se2(out).sigmoid()\n",
    "        out = x * out\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, w_in, w_out, stride, group_width, bottleneck_ratio, se_ratio):\n",
    "        super(Block, self).__init__()\n",
    "        # 1x1\n",
    "        w_b = int(round(w_out * bottleneck_ratio))\n",
    "        self.conv1 = nn.Conv2d(w_in, w_b, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(w_b)\n",
    "        # 3x3\n",
    "        num_groups = w_b // group_width\n",
    "        self.conv2 = nn.Conv2d(w_b, w_b, kernel_size=3,\n",
    "                               stride=stride, padding=1, groups=num_groups, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(w_b)\n",
    "        # se\n",
    "        self.with_se = se_ratio > 0\n",
    "        if self.with_se:\n",
    "            w_se = int(round(w_in * se_ratio))\n",
    "            self.se = SE(w_b, w_se)\n",
    "        # 1x1\n",
    "        self.conv3 = nn.Conv2d(w_b, w_out, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(w_out)\n",
    "        self.scSE = scSE(w_out)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or w_in != w_out:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(w_in, w_out,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(w_out)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # out = F.relu(self.scSE(self.bn1(self.conv1(x))))\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        if self.with_se:\n",
    "            out = self.se(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        # out = F.relu(out)\n",
    "        out = F.relu(self.scSE(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class RegNet(nn.Module):\n",
    "    def __init__(self, cfg, num_classes=10):\n",
    "        super(RegNet, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(0)\n",
    "        self.layer2 = self._make_layer(1)\n",
    "        self.layer3 = self._make_layer(2)\n",
    "        self.layer4 = self._make_layer(3)\n",
    "        self.linear = nn.Linear(self.cfg['widths'][-1], num_classes)\n",
    "\n",
    "    def _make_layer(self, idx):\n",
    "        depth = self.cfg['depths'][idx]\n",
    "        width = self.cfg['widths'][idx]\n",
    "        stride = self.cfg['strides'][idx]\n",
    "        group_width = self.cfg['group_width']\n",
    "        bottleneck_ratio = self.cfg['bottleneck_ratio']\n",
    "        se_ratio = self.cfg['se_ratio']\n",
    "\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            s = stride if i == 0 else 1\n",
    "            layers.append(Block(self.in_planes, width,\n",
    "                                s, group_width, bottleneck_ratio, se_ratio))\n",
    "            self.in_planes = width\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "        'depths': [1, 2, 5, 11],\n",
    "        'widths': [32, 64, 160, 368],\n",
    "        'strides': [1, 1, 2, 2],\n",
    "        'group_width': 16,\n",
    "        'bottleneck_ratio': 1,\n",
    "        'se_ratio': 0.25,\n",
    "        'lr': 0.1,\n",
    "        'resume': False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "if cfg['resume']:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=args.lr, betas=(0.9, 0.999),\n",
    "#                        eps=1e-08,weight_decay=0,amsgrad=False)       \n",
    "optimizer = optim.SGD(net.parameters(), lr=cfg['lr'],\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 20, T_mult=3, eta_min = 0.0001, last_epoch = -1, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "\n",
    "    # print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        # progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        #              % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc = 100.*correct / total\n",
    "\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            # progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            #              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        test_loss = test_loss / len(testloader)\n",
    "        test_acc = 100.*correct/total\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "EPOCHS = 80\n",
    "avg_best_accs=[]\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "for i in range(1, len(test_groups)):\n",
    "    trainloader, testloader = prepareData(i)\n",
    "    test_acc_history = []\n",
    "\n",
    "    print('======= start test group:', i, '=======')\n",
    "    for epoch in range(start_epoch, EPOCHS):\n",
    "        start_time = time.monotonic()\n",
    "\n",
    "        train_loss, train_acc = train(epoch)\n",
    "        test_loss, test_acc = test(epoch)\n",
    "        train_loss_history.append(train_loss)\n",
    "        test_loss_history.append(test_loss)\n",
    "        test_acc_history.append(test_acc)\n",
    "\n",
    "        end_time = time.monotonic()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        scheduler.step()\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')\n",
    "        print(f'\\t Test. Loss: {test_loss:.3f} |  Test. Acc: {test_acc:.2f} | Best Acc:{best_acc:.2f}%')\n",
    "    test_acc_history.sort(reverse=True)\n",
    "    avg_best_accs.append(sum(test_acc_history[:5])/5.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the best model\n",
    "checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "best_acc = checkpoint['acc']\n",
    "start_epoch = checkpoint['epoch']\n",
    "test_loss, test_acc = test(1)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the loss history as txt file\n",
    "filename = './output/lr/se_ba=95.6_lr=0.1_t0=200_bs=256.txt'\n",
    "output = open(filename,'w',encoding = 'gbk')\n",
    "for i in range(len(train_loss_history)):\n",
    "    rowtxt = '{},{}'.format(train_loss_history[i],test_loss_history[i])\n",
    "    output.write(rowtxt)\n",
    "    output.write('\\n')\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(range(len(train_loss_history)), train_loss_history,'-', linewidth=3, label = 'Train error')\n",
    "plt.plot(range(len(test_loss_history)), test_loss_history, '-', linewidth=3, label = 'Test error')\n",
    "plt.ylim(0,3)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('se_ba=95.6_lr=0.1_t0=200_bs=256')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('eye_track')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "027e191732b1bcb3be46d89a75078cd83ade06f1387bc98c3557c15162693679"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
