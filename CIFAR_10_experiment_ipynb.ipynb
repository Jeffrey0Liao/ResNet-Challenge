{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HOj6Zez5ENo4"
      },
      "outputs": [],
      "source": [
        "# import packets\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "\n",
        "from sklearn import decomposition\n",
        "from sklearn import manifold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparas\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "LR=5e-3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1NYcnL_c_rr",
        "outputId": "93a93805-508b-4a93-c1c6-c3f96d4d76ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define augmentation strategies\n",
        "\n",
        "def gauss_noise_tensor(img):\n",
        "    assert isinstance(img, torch.Tensor)\n",
        "    dtype = img.dtype\n",
        "    if not img.is_floating_point():\n",
        "        img = img.to(torch.float32)\n",
        "    \n",
        "    sigma = 25.0\n",
        "    \n",
        "    out = img + sigma * torch.randn_like(img)\n",
        "    \n",
        "    if out.dtype != dtype:\n",
        "        out = out.to(dtype)\n",
        "        \n",
        "    return out\n",
        "\n",
        "group_a=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "group_b=transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "group_c=transforms.Compose([\n",
        "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)), \n",
        "    transforms.RandomRotation(degrees=(0, 180)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "group_d=transforms.Compose([\n",
        "    transforms.ColorJitter(brightness=.5, hue=.3), \n",
        "    transforms.ToTensor(),\n",
        "    gauss_noise_tensor,\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "group_e=transforms.Compose([\n",
        "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10), \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "test_names=['A', 'B', 'C', 'D', 'E']\n",
        "test_groups=[group_a, group_b, group_c, group_d, group_e]"
      ],
      "metadata": {
        "id": "r1P47aLcjX1h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare data\n",
        "\n",
        "def prepareData(group_i):\n",
        "  T=test_groups[group_i]\n",
        "  trainset=torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=T)\n",
        "  testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=test_groups[0])\n",
        "  \n",
        "  #trainset=torch.utils.data.Subset(trainset, range(int(len(trainset)/40)))\n",
        "  #testset=torch.utils.data.Subset(testset, range(int(len(testset)/20)))\n",
        "  \n",
        "  trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=4)\n",
        "  testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=4)\n",
        "  return trainloader, testloader"
      ],
      "metadata": {
        "id": "FSP3AoZ8b_q3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model structure\n",
        "\n",
        "class SE(nn.Module):\n",
        "    '''Squeeze-and-Excitation block.'''\n",
        "\n",
        "    def __init__(self, in_planes, se_planes):\n",
        "        super(SE, self).__init__()\n",
        "        self.se1 = nn.Conv2d(in_planes, se_planes, kernel_size=1, bias=True)\n",
        "        self.se2 = nn.Conv2d(se_planes, in_planes, kernel_size=1, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        out = F.relu(self.se1(out))\n",
        "        out = self.se2(out).sigmoid()\n",
        "        out = x * out\n",
        "        return out\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, w_in, w_out, stride, group_width, bottleneck_ratio, se_ratio):\n",
        "        super(Block, self).__init__()\n",
        "        # 1x1\n",
        "        w_b = int(round(w_out * bottleneck_ratio))\n",
        "        self.conv1 = nn.Conv2d(w_in, w_b, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(w_b)\n",
        "        # 3x3\n",
        "        num_groups = w_b // group_width\n",
        "        self.conv2 = nn.Conv2d(w_b, w_b, kernel_size=3,\n",
        "                               stride=stride, padding=1, groups=num_groups, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(w_b)\n",
        "        # se\n",
        "        self.with_se = se_ratio > 0\n",
        "        if self.with_se:\n",
        "            w_se = int(round(w_in * se_ratio))\n",
        "            self.se = SE(w_b, w_se)\n",
        "        # 1x1\n",
        "        self.conv3 = nn.Conv2d(w_b, w_out, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(w_out)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or w_in != w_out:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(w_in, w_out,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(w_out)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        if self.with_se:\n",
        "            out = self.se(out)\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class RegNet(nn.Module):\n",
        "    def __init__(self, cfg, num_classes=10):\n",
        "        super(RegNet, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        self.in_planes = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(0)\n",
        "        self.layer2 = self._make_layer(1)\n",
        "        self.layer3 = self._make_layer(2)\n",
        "        self.layer4 = self._make_layer(3)\n",
        "        self.linear = nn.Linear(self.cfg['widths'][-1], num_classes)\n",
        "\n",
        "    def _make_layer(self, idx):\n",
        "        depth = self.cfg['depths'][idx]\n",
        "        width = self.cfg['widths'][idx]\n",
        "        stride = self.cfg['strides'][idx]\n",
        "        group_width = self.cfg['group_width']\n",
        "        bottleneck_ratio = self.cfg['bottleneck_ratio']\n",
        "        se_ratio = self.cfg['se_ratio']\n",
        "\n",
        "        layers = []\n",
        "        for i in range(depth):\n",
        "            s = stride if i == 0 else 1\n",
        "            layers.append(Block(self.in_planes, width,\n",
        "                                s, group_width, bottleneck_ratio, se_ratio))\n",
        "            self.in_planes = width\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def RegNetX_200MF():\n",
        "    cfg = {\n",
        "        'depths': [1, 1, 4, 7],\n",
        "        'widths': [24, 56, 152, 368],\n",
        "        'strides': [1, 1, 2, 2],\n",
        "        'group_width': 8,\n",
        "        'bottleneck_ratio': 1,\n",
        "        'se_ratio': 0,\n",
        "    }\n",
        "    return RegNet(cfg)\n"
      ],
      "metadata": {
        "id": "RmPngwKSdhv8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get model instance\n",
        "\n",
        "model=RegNetX_200MF().to(device)\n",
        "print(model)\n",
        "summary(model, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIYlY4mBiHTb",
        "outputId": "83bd85c2-c1da-4af0-e5ad-156f74cb1fae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RegNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): Block(\n",
            "      (conv1): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3, bias=False)\n",
            "      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Block(\n",
            "      (conv1): Conv2d(24, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=7, bias=False)\n",
            "      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(56, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(24, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Block(\n",
            "      (conv1): Conv2d(56, 152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(152, 152, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=19, bias=False)\n",
            "      (bn2): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(152, 152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(56, 152, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Block(\n",
            "      (conv1): Conv2d(152, 152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(152, 152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=19, bias=False)\n",
            "      (bn2): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(152, 152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Block(\n",
            "      (conv1): Conv2d(152, 152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(152, 152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=19, bias=False)\n",
            "      (bn2): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(152, 152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (3): Block(\n",
            "      (conv1): Conv2d(152, 152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(152, 152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=19, bias=False)\n",
            "      (bn2): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(152, 152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Block(\n",
            "      (conv1): Conv2d(152, 368, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(368, 368, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=46, bias=False)\n",
            "      (bn2): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(368, 368, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(152, 368, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Block(\n",
            "      (conv1): Conv2d(368, 368, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(368, 368, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=46, bias=False)\n",
            "      (bn2): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(368, 368, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Block(\n",
            "      (conv1): Conv2d(368, 368, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(368, 368, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=46, bias=False)\n",
            "      (bn2): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(368, 368, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (3): Block(\n",
            "      (conv1): Conv2d(368, 368, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(368, 368, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=46, bias=False)\n",
            "      (bn2): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(368, 368, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (4): Block(\n",
            "      (conv1): Conv2d(368, 368, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(368, 368, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=46, bias=False)\n",
            "      (bn2): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(368, 368, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (5): Block(\n",
            "      (conv1): Conv2d(368, 368, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(368, 368, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=46, bias=False)\n",
            "      (bn2): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(368, 368, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (6): Block(\n",
            "      (conv1): Conv2d(368, 368, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(368, 368, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=46, bias=False)\n",
            "      (bn2): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(368, 368, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=368, out_features=10, bias=True)\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "            Conv2d-3           [-1, 24, 32, 32]           1,536\n",
            "       BatchNorm2d-4           [-1, 24, 32, 32]              48\n",
            "            Conv2d-5           [-1, 24, 32, 32]           1,728\n",
            "       BatchNorm2d-6           [-1, 24, 32, 32]              48\n",
            "            Conv2d-7           [-1, 24, 32, 32]             576\n",
            "       BatchNorm2d-8           [-1, 24, 32, 32]              48\n",
            "            Conv2d-9           [-1, 24, 32, 32]           1,536\n",
            "      BatchNorm2d-10           [-1, 24, 32, 32]              48\n",
            "            Block-11           [-1, 24, 32, 32]               0\n",
            "           Conv2d-12           [-1, 56, 32, 32]           1,344\n",
            "      BatchNorm2d-13           [-1, 56, 32, 32]             112\n",
            "           Conv2d-14           [-1, 56, 32, 32]           4,032\n",
            "      BatchNorm2d-15           [-1, 56, 32, 32]             112\n",
            "           Conv2d-16           [-1, 56, 32, 32]           3,136\n",
            "      BatchNorm2d-17           [-1, 56, 32, 32]             112\n",
            "           Conv2d-18           [-1, 56, 32, 32]           1,344\n",
            "      BatchNorm2d-19           [-1, 56, 32, 32]             112\n",
            "            Block-20           [-1, 56, 32, 32]               0\n",
            "           Conv2d-21          [-1, 152, 32, 32]           8,512\n",
            "      BatchNorm2d-22          [-1, 152, 32, 32]             304\n",
            "           Conv2d-23          [-1, 152, 16, 16]          10,944\n",
            "      BatchNorm2d-24          [-1, 152, 16, 16]             304\n",
            "           Conv2d-25          [-1, 152, 16, 16]          23,104\n",
            "      BatchNorm2d-26          [-1, 152, 16, 16]             304\n",
            "           Conv2d-27          [-1, 152, 16, 16]           8,512\n",
            "      BatchNorm2d-28          [-1, 152, 16, 16]             304\n",
            "            Block-29          [-1, 152, 16, 16]               0\n",
            "           Conv2d-30          [-1, 152, 16, 16]          23,104\n",
            "      BatchNorm2d-31          [-1, 152, 16, 16]             304\n",
            "           Conv2d-32          [-1, 152, 16, 16]          10,944\n",
            "      BatchNorm2d-33          [-1, 152, 16, 16]             304\n",
            "           Conv2d-34          [-1, 152, 16, 16]          23,104\n",
            "      BatchNorm2d-35          [-1, 152, 16, 16]             304\n",
            "            Block-36          [-1, 152, 16, 16]               0\n",
            "           Conv2d-37          [-1, 152, 16, 16]          23,104\n",
            "      BatchNorm2d-38          [-1, 152, 16, 16]             304\n",
            "           Conv2d-39          [-1, 152, 16, 16]          10,944\n",
            "      BatchNorm2d-40          [-1, 152, 16, 16]             304\n",
            "           Conv2d-41          [-1, 152, 16, 16]          23,104\n",
            "      BatchNorm2d-42          [-1, 152, 16, 16]             304\n",
            "            Block-43          [-1, 152, 16, 16]               0\n",
            "           Conv2d-44          [-1, 152, 16, 16]          23,104\n",
            "      BatchNorm2d-45          [-1, 152, 16, 16]             304\n",
            "           Conv2d-46          [-1, 152, 16, 16]          10,944\n",
            "      BatchNorm2d-47          [-1, 152, 16, 16]             304\n",
            "           Conv2d-48          [-1, 152, 16, 16]          23,104\n",
            "      BatchNorm2d-49          [-1, 152, 16, 16]             304\n",
            "            Block-50          [-1, 152, 16, 16]               0\n",
            "           Conv2d-51          [-1, 368, 16, 16]          55,936\n",
            "      BatchNorm2d-52          [-1, 368, 16, 16]             736\n",
            "           Conv2d-53            [-1, 368, 8, 8]          26,496\n",
            "      BatchNorm2d-54            [-1, 368, 8, 8]             736\n",
            "           Conv2d-55            [-1, 368, 8, 8]         135,424\n",
            "      BatchNorm2d-56            [-1, 368, 8, 8]             736\n",
            "           Conv2d-57            [-1, 368, 8, 8]          55,936\n",
            "      BatchNorm2d-58            [-1, 368, 8, 8]             736\n",
            "            Block-59            [-1, 368, 8, 8]               0\n",
            "           Conv2d-60            [-1, 368, 8, 8]         135,424\n",
            "      BatchNorm2d-61            [-1, 368, 8, 8]             736\n",
            "           Conv2d-62            [-1, 368, 8, 8]          26,496\n",
            "      BatchNorm2d-63            [-1, 368, 8, 8]             736\n",
            "           Conv2d-64            [-1, 368, 8, 8]         135,424\n",
            "      BatchNorm2d-65            [-1, 368, 8, 8]             736\n",
            "            Block-66            [-1, 368, 8, 8]               0\n",
            "           Conv2d-67            [-1, 368, 8, 8]         135,424\n",
            "      BatchNorm2d-68            [-1, 368, 8, 8]             736\n",
            "           Conv2d-69            [-1, 368, 8, 8]          26,496\n",
            "      BatchNorm2d-70            [-1, 368, 8, 8]             736\n",
            "           Conv2d-71            [-1, 368, 8, 8]         135,424\n",
            "      BatchNorm2d-72            [-1, 368, 8, 8]             736\n",
            "            Block-73            [-1, 368, 8, 8]               0\n",
            "           Conv2d-74            [-1, 368, 8, 8]         135,424\n",
            "      BatchNorm2d-75            [-1, 368, 8, 8]             736\n",
            "           Conv2d-76            [-1, 368, 8, 8]          26,496\n",
            "      BatchNorm2d-77            [-1, 368, 8, 8]             736\n",
            "           Conv2d-78            [-1, 368, 8, 8]         135,424\n",
            "      BatchNorm2d-79            [-1, 368, 8, 8]             736\n",
            "            Block-80            [-1, 368, 8, 8]               0\n",
            "           Conv2d-81            [-1, 368, 8, 8]         135,424\n",
            "      BatchNorm2d-82            [-1, 368, 8, 8]             736\n",
            "           Conv2d-83            [-1, 368, 8, 8]          26,496\n",
            "      BatchNorm2d-84            [-1, 368, 8, 8]             736\n",
            "           Conv2d-85            [-1, 368, 8, 8]         135,424\n",
            "      BatchNorm2d-86            [-1, 368, 8, 8]             736\n",
            "            Block-87            [-1, 368, 8, 8]               0\n",
            "           Conv2d-88            [-1, 368, 8, 8]         135,424\n",
            "      BatchNorm2d-89            [-1, 368, 8, 8]             736\n",
            "           Conv2d-90            [-1, 368, 8, 8]          26,496\n",
            "      BatchNorm2d-91            [-1, 368, 8, 8]             736\n",
            "           Conv2d-92            [-1, 368, 8, 8]         135,424\n",
            "      BatchNorm2d-93            [-1, 368, 8, 8]             736\n",
            "            Block-94            [-1, 368, 8, 8]               0\n",
            "           Conv2d-95            [-1, 368, 8, 8]         135,424\n",
            "      BatchNorm2d-96            [-1, 368, 8, 8]             736\n",
            "           Conv2d-97            [-1, 368, 8, 8]          26,496\n",
            "      BatchNorm2d-98            [-1, 368, 8, 8]             736\n",
            "           Conv2d-99            [-1, 368, 8, 8]         135,424\n",
            "     BatchNorm2d-100            [-1, 368, 8, 8]             736\n",
            "           Block-101            [-1, 368, 8, 8]               0\n",
            "          Linear-102                   [-1, 10]           3,690\n",
            "================================================================\n",
            "Total params: 2,321,946\n",
            "Trainable params: 2,321,946\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 27.55\n",
            "Params size (MB): 8.86\n",
            "Estimated Total Size (MB): 36.42\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimization paradigm\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=LR,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
      ],
      "metadata": {
        "id": "UD3QLgrZiZqy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if batch_idx%20==0:\n",
        "          print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                      % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    sum_loss = 0\n",
        "    sum_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            sum_loss+=(test_loss/(batch_idx+1))\n",
        "            sum_acc+=(100.*correct/total)\n",
        "    #test_losses.append(sum_loss/len(testloader))\n",
        "    test_accs.append(sum_acc/len(testloader))\n",
        "    print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                         % (sum_loss/len(testloader), sum_acc/len(testloader), correct, total))"
      ],
      "metadata": {
        "id": "Yz3MukEqyV49"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch=0\n",
        "avg_best_accs=[]\n",
        "for i in range(1, len(test_groups)):\n",
        "  trainloader, testloader=prepareData(i)\n",
        "  #model=RegNetX_200MF().to(device)\n",
        "  test_accs=[]\n",
        "  print('======= start test group:', i, '=======')\n",
        "  for epoch in range(start_epoch, start_epoch+40):\n",
        "      train(epoch)\n",
        "      test(epoch)\n",
        "      scheduler.step()\n",
        "  test_accs.sort(reverse=True)\n",
        "  avg_best_accs.append(sum(test_accs[:5])/5.00)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdWEHWljyZKZ",
        "outputId": "0a67c7b8-91a8-45f8-8e6e-d1996977b2e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "======= start test group: 1 =======\n",
            "\n",
            "Epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 391 Loss: 3.212 | Acc: 10.938% (14/128)\n",
            "20 391 Loss: 2.521 | Acc: 12.054% (324/2688)\n",
            "40 391 Loss: 2.390 | Acc: 15.053% (790/5248)\n",
            "60 391 Loss: 2.296 | Acc: 17.290% (1350/7808)\n",
            "80 391 Loss: 2.225 | Acc: 18.962% (1966/10368)\n",
            "100 391 Loss: 2.156 | Acc: 20.653% (2670/12928)\n",
            "120 391 Loss: 2.111 | Acc: 22.036% (3413/15488)\n",
            "140 391 Loss: 2.078 | Acc: 23.105% (4170/18048)\n",
            "160 391 Loss: 2.043 | Acc: 24.233% (4994/20608)\n",
            "180 391 Loss: 2.010 | Acc: 25.207% (5840/23168)\n",
            "200 391 Loss: 1.979 | Acc: 26.318% (6771/25728)\n",
            "220 391 Loss: 1.953 | Acc: 27.255% (7710/28288)\n",
            "240 391 Loss: 1.926 | Acc: 28.106% (8670/30848)\n",
            "260 391 Loss: 1.904 | Acc: 28.975% (9680/33408)\n",
            "280 391 Loss: 1.888 | Acc: 29.682% (10676/35968)\n",
            "300 391 Loss: 1.866 | Acc: 30.458% (11735/38528)\n",
            "320 391 Loss: 1.848 | Acc: 31.194% (12817/41088)\n",
            "340 391 Loss: 1.828 | Acc: 31.862% (13907/43648)\n",
            "360 391 Loss: 1.815 | Acc: 32.395% (14969/46208)\n",
            "380 391 Loss: 1.798 | Acc: 33.011% (16099/48768)\n",
            "99 100 Loss: 1.549 | Acc: 45.262% (4493/10000)\n",
            "\n",
            "Epoch: 1\n",
            "0 391 Loss: 1.449 | Acc: 43.750% (56/128)\n",
            "20 391 Loss: 1.485 | Acc: 44.866% (1206/2688)\n",
            "40 391 Loss: 1.463 | Acc: 46.265% (2428/5248)\n",
            "60 391 Loss: 1.455 | Acc: 46.619% (3640/7808)\n",
            "80 391 Loss: 1.447 | Acc: 47.000% (4873/10368)\n",
            "100 391 Loss: 1.446 | Acc: 46.991% (6075/12928)\n",
            "120 391 Loss: 1.439 | Acc: 47.166% (7305/15488)\n",
            "140 391 Loss: 1.427 | Acc: 47.717% (8612/18048)\n",
            "160 391 Loss: 1.422 | Acc: 48.015% (9895/20608)\n",
            "180 391 Loss: 1.421 | Acc: 47.997% (11120/23168)\n",
            "200 391 Loss: 1.424 | Acc: 48.072% (12368/25728)\n",
            "220 391 Loss: 1.420 | Acc: 48.307% (13665/28288)\n",
            "240 391 Loss: 1.412 | Acc: 48.697% (15022/30848)\n",
            "260 391 Loss: 1.402 | Acc: 49.171% (16427/33408)\n",
            "280 391 Loss: 1.390 | Acc: 49.641% (17855/35968)\n",
            "300 391 Loss: 1.382 | Acc: 49.961% (19249/38528)\n",
            "320 391 Loss: 1.371 | Acc: 50.377% (20699/41088)\n",
            "340 391 Loss: 1.367 | Acc: 50.442% (22017/43648)\n",
            "360 391 Loss: 1.358 | Acc: 50.773% (23461/46208)\n",
            "380 391 Loss: 1.350 | Acc: 51.159% (24949/48768)\n",
            "99 100 Loss: 1.277 | Acc: 55.605% (5510/10000)\n",
            "\n",
            "Epoch: 2\n",
            "0 391 Loss: 1.328 | Acc: 55.469% (71/128)\n",
            "20 391 Loss: 1.183 | Acc: 57.292% (1540/2688)\n",
            "40 391 Loss: 1.170 | Acc: 58.327% (3061/5248)\n",
            "60 391 Loss: 1.158 | Acc: 58.850% (4595/7808)\n",
            "80 391 Loss: 1.142 | Acc: 59.452% (6164/10368)\n",
            "100 391 Loss: 1.143 | Acc: 59.274% (7663/12928)\n",
            "120 391 Loss: 1.141 | Acc: 59.343% (9191/15488)\n",
            "140 391 Loss: 1.135 | Acc: 59.574% (10752/18048)\n",
            "160 391 Loss: 1.131 | Acc: 59.720% (12307/20608)\n",
            "180 391 Loss: 1.131 | Acc: 59.742% (13841/23168)\n",
            "200 391 Loss: 1.125 | Acc: 59.997% (15436/25728)\n",
            "220 391 Loss: 1.120 | Acc: 60.185% (17025/28288)\n",
            "240 391 Loss: 1.112 | Acc: 60.399% (18632/30848)\n",
            "260 391 Loss: 1.107 | Acc: 60.569% (20235/33408)\n",
            "280 391 Loss: 1.104 | Acc: 60.687% (21828/35968)\n",
            "300 391 Loss: 1.101 | Acc: 60.831% (23437/38528)\n",
            "320 391 Loss: 1.097 | Acc: 60.950% (25043/41088)\n",
            "340 391 Loss: 1.093 | Acc: 61.043% (26644/43648)\n",
            "360 391 Loss: 1.089 | Acc: 61.208% (28283/46208)\n",
            "380 391 Loss: 1.085 | Acc: 61.366% (29927/48768)\n",
            "99 100 Loss: 1.013 | Acc: 65.034% (6487/10000)\n",
            "\n",
            "Epoch: 3\n",
            "0 391 Loss: 1.018 | Acc: 64.844% (83/128)\n",
            "20 391 Loss: 1.004 | Acc: 65.699% (1766/2688)\n",
            "40 391 Loss: 0.980 | Acc: 65.758% (3451/5248)\n",
            "60 391 Loss: 0.971 | Acc: 65.727% (5132/7808)\n",
            "80 391 Loss: 0.959 | Acc: 66.117% (6855/10368)\n",
            "100 391 Loss: 0.957 | Acc: 66.197% (8558/12928)\n",
            "120 391 Loss: 0.957 | Acc: 66.206% (10254/15488)\n",
            "140 391 Loss: 0.949 | Acc: 66.412% (11986/18048)\n",
            "160 391 Loss: 0.947 | Acc: 66.377% (13679/20608)\n",
            "180 391 Loss: 0.943 | Acc: 66.540% (15416/23168)\n",
            "200 391 Loss: 0.944 | Acc: 66.488% (17106/25728)\n",
            "220 391 Loss: 0.944 | Acc: 66.498% (18811/28288)\n",
            "240 391 Loss: 0.940 | Acc: 66.562% (20533/30848)\n",
            "260 391 Loss: 0.942 | Acc: 66.529% (22226/33408)\n",
            "280 391 Loss: 0.942 | Acc: 66.584% (23949/35968)\n",
            "300 391 Loss: 0.938 | Acc: 66.736% (25712/38528)\n",
            "320 391 Loss: 0.936 | Acc: 66.813% (27452/41088)\n",
            "340 391 Loss: 0.934 | Acc: 66.876% (29190/43648)\n",
            "360 391 Loss: 0.932 | Acc: 66.993% (30956/46208)\n",
            "380 391 Loss: 0.930 | Acc: 67.050% (32699/48768)\n",
            "99 100 Loss: 0.916 | Acc: 67.999% (6767/10000)\n",
            "\n",
            "Epoch: 4\n",
            "0 391 Loss: 0.788 | Acc: 71.094% (91/128)\n",
            "20 391 Loss: 0.872 | Acc: 69.010% (1855/2688)\n",
            "40 391 Loss: 0.862 | Acc: 69.569% (3651/5248)\n",
            "60 391 Loss: 0.854 | Acc: 69.954% (5462/7808)\n",
            "80 391 Loss: 0.849 | Acc: 70.129% (7271/10368)\n",
            "100 391 Loss: 0.846 | Acc: 70.266% (9084/12928)\n",
            "120 391 Loss: 0.846 | Acc: 70.164% (10867/15488)\n",
            "140 391 Loss: 0.842 | Acc: 70.235% (12676/18048)\n",
            "160 391 Loss: 0.836 | Acc: 70.376% (14503/20608)\n",
            "180 391 Loss: 0.831 | Acc: 70.610% (16359/23168)\n",
            "200 391 Loss: 0.828 | Acc: 70.655% (18178/25728)\n",
            "220 391 Loss: 0.824 | Acc: 70.790% (20025/28288)\n",
            "240 391 Loss: 0.819 | Acc: 71.003% (21903/30848)\n",
            "260 391 Loss: 0.820 | Acc: 71.025% (23728/33408)\n",
            "280 391 Loss: 0.817 | Acc: 71.155% (25593/35968)\n",
            "300 391 Loss: 0.815 | Acc: 71.283% (27464/38528)\n",
            "320 391 Loss: 0.812 | Acc: 71.398% (29336/41088)\n",
            "340 391 Loss: 0.812 | Acc: 71.366% (31150/43648)\n",
            "360 391 Loss: 0.809 | Acc: 71.483% (33031/46208)\n",
            "380 391 Loss: 0.806 | Acc: 71.610% (34923/48768)\n",
            "99 100 Loss: 0.822 | Acc: 72.372% (7198/10000)\n",
            "\n",
            "Epoch: 5\n",
            "0 391 Loss: 0.726 | Acc: 72.656% (93/128)\n",
            "20 391 Loss: 0.728 | Acc: 74.442% (2001/2688)\n",
            "40 391 Loss: 0.735 | Acc: 74.447% (3907/5248)\n",
            "60 391 Loss: 0.727 | Acc: 74.872% (5846/7808)\n",
            "80 391 Loss: 0.729 | Acc: 74.518% (7726/10368)\n",
            "100 391 Loss: 0.728 | Acc: 74.536% (9636/12928)\n",
            "120 391 Loss: 0.726 | Acc: 74.600% (11554/15488)\n",
            "140 391 Loss: 0.724 | Acc: 74.695% (13481/18048)\n",
            "160 391 Loss: 0.721 | Acc: 74.854% (15426/20608)\n",
            "180 391 Loss: 0.725 | Acc: 74.741% (17316/23168)\n",
            "200 391 Loss: 0.725 | Acc: 74.639% (19203/25728)\n",
            "220 391 Loss: 0.722 | Acc: 74.806% (21161/28288)\n",
            "240 391 Loss: 0.719 | Acc: 74.890% (23102/30848)\n",
            "260 391 Loss: 0.716 | Acc: 74.988% (25052/33408)\n",
            "280 391 Loss: 0.715 | Acc: 75.083% (27006/35968)\n",
            "300 391 Loss: 0.712 | Acc: 75.153% (28955/38528)\n",
            "320 391 Loss: 0.707 | Acc: 75.294% (30937/41088)\n",
            "340 391 Loss: 0.706 | Acc: 75.328% (32879/43648)\n",
            "360 391 Loss: 0.706 | Acc: 75.331% (34809/46208)\n",
            "380 391 Loss: 0.702 | Acc: 75.398% (36770/48768)\n",
            "99 100 Loss: 0.819 | Acc: 73.376% (7283/10000)\n",
            "\n",
            "Epoch: 6\n",
            "0 391 Loss: 0.569 | Acc: 79.688% (102/128)\n",
            "20 391 Loss: 0.661 | Acc: 77.158% (2074/2688)\n",
            "40 391 Loss: 0.662 | Acc: 76.715% (4026/5248)\n",
            "60 391 Loss: 0.661 | Acc: 77.011% (6013/7808)\n",
            "80 391 Loss: 0.670 | Acc: 76.755% (7958/10368)\n",
            "100 391 Loss: 0.659 | Acc: 77.065% (9963/12928)\n",
            "120 391 Loss: 0.657 | Acc: 77.098% (11941/15488)\n",
            "140 391 Loss: 0.649 | Acc: 77.355% (13961/18048)\n",
            "160 391 Loss: 0.647 | Acc: 77.431% (15957/20608)\n",
            "180 391 Loss: 0.642 | Acc: 77.629% (17985/23168)\n",
            "200 391 Loss: 0.641 | Acc: 77.624% (19971/25728)\n",
            "220 391 Loss: 0.638 | Acc: 77.690% (21977/28288)\n",
            "240 391 Loss: 0.641 | Acc: 77.597% (23937/30848)\n",
            "260 391 Loss: 0.640 | Acc: 77.571% (25915/33408)\n",
            "280 391 Loss: 0.642 | Acc: 77.533% (27887/35968)\n",
            "300 391 Loss: 0.638 | Acc: 77.632% (29910/38528)\n",
            "320 391 Loss: 0.638 | Acc: 77.677% (31916/41088)\n",
            "340 391 Loss: 0.637 | Acc: 77.749% (33936/43648)\n",
            "360 391 Loss: 0.637 | Acc: 77.770% (35936/46208)\n",
            "380 391 Loss: 0.635 | Acc: 77.867% (37974/48768)\n",
            "99 100 Loss: 0.694 | Acc: 77.247% (7656/10000)\n",
            "\n",
            "Epoch: 7\n",
            "0 391 Loss: 0.508 | Acc: 82.031% (105/128)\n",
            "20 391 Loss: 0.596 | Acc: 79.092% (2126/2688)\n",
            "40 391 Loss: 0.613 | Acc: 78.582% (4124/5248)\n",
            "60 391 Loss: 0.606 | Acc: 78.945% (6164/7808)\n",
            "80 391 Loss: 0.601 | Acc: 78.868% (8177/10368)\n",
            "100 391 Loss: 0.603 | Acc: 79.076% (10223/12928)\n",
            "120 391 Loss: 0.603 | Acc: 79.197% (12266/15488)\n",
            "140 391 Loss: 0.594 | Acc: 79.538% (14355/18048)\n",
            "160 391 Loss: 0.589 | Acc: 79.721% (16429/20608)\n",
            "180 391 Loss: 0.588 | Acc: 79.623% (18447/23168)\n",
            "200 391 Loss: 0.584 | Acc: 79.761% (20521/25728)\n",
            "220 391 Loss: 0.580 | Acc: 79.875% (22595/28288)\n",
            "240 391 Loss: 0.581 | Acc: 79.817% (24622/30848)\n",
            "260 391 Loss: 0.579 | Acc: 79.894% (26691/33408)\n",
            "280 391 Loss: 0.578 | Acc: 79.902% (28739/35968)\n",
            "300 391 Loss: 0.577 | Acc: 79.960% (30807/38528)\n",
            "320 391 Loss: 0.579 | Acc: 79.865% (32815/41088)\n",
            "340 391 Loss: 0.580 | Acc: 79.882% (34867/43648)\n",
            "360 391 Loss: 0.578 | Acc: 79.943% (36940/46208)\n",
            "380 391 Loss: 0.577 | Acc: 80.005% (39017/48768)\n",
            "99 100 Loss: 0.636 | Acc: 78.502% (7821/10000)\n",
            "\n",
            "Epoch: 8\n",
            "0 391 Loss: 0.496 | Acc: 84.375% (108/128)\n",
            "20 391 Loss: 0.529 | Acc: 81.176% (2182/2688)\n",
            "40 391 Loss: 0.542 | Acc: 80.945% (4248/5248)\n",
            "60 391 Loss: 0.521 | Acc: 81.455% (6360/7808)\n",
            "80 391 Loss: 0.518 | Acc: 81.684% (8469/10368)\n",
            "100 391 Loss: 0.529 | Acc: 81.343% (10516/12928)\n",
            "120 391 Loss: 0.532 | Acc: 81.411% (12609/15488)\n",
            "140 391 Loss: 0.532 | Acc: 81.300% (14673/18048)\n",
            "160 391 Loss: 0.530 | Acc: 81.459% (16787/20608)\n",
            "180 391 Loss: 0.526 | Acc: 81.474% (18876/23168)\n",
            "200 391 Loss: 0.527 | Acc: 81.479% (20963/25728)\n",
            "220 391 Loss: 0.528 | Acc: 81.388% (23023/28288)\n",
            "240 391 Loss: 0.531 | Acc: 81.331% (25089/30848)\n",
            "260 391 Loss: 0.530 | Acc: 81.439% (27207/33408)\n",
            "280 391 Loss: 0.530 | Acc: 81.447% (29295/35968)\n",
            "300 391 Loss: 0.530 | Acc: 81.390% (31358/38528)\n",
            "320 391 Loss: 0.529 | Acc: 81.435% (33460/41088)\n",
            "340 391 Loss: 0.529 | Acc: 81.481% (35565/43648)\n",
            "360 391 Loss: 0.530 | Acc: 81.408% (37617/46208)\n",
            "380 391 Loss: 0.531 | Acc: 81.357% (39676/48768)\n",
            "99 100 Loss: 0.608 | Acc: 79.908% (7960/10000)\n",
            "\n",
            "Epoch: 9\n",
            "0 391 Loss: 0.398 | Acc: 87.500% (112/128)\n",
            "20 391 Loss: 0.509 | Acc: 81.882% (2201/2688)\n",
            "40 391 Loss: 0.502 | Acc: 82.336% (4321/5248)\n",
            "60 391 Loss: 0.488 | Acc: 82.812% (6466/7808)\n",
            "80 391 Loss: 0.491 | Acc: 82.716% (8576/10368)\n",
            "100 391 Loss: 0.493 | Acc: 82.650% (10685/12928)\n",
            "120 391 Loss: 0.488 | Acc: 82.909% (12841/15488)\n",
            "140 391 Loss: 0.488 | Acc: 82.890% (14960/18048)\n",
            "160 391 Loss: 0.488 | Acc: 82.905% (17085/20608)\n",
            "180 391 Loss: 0.488 | Acc: 82.938% (19215/23168)\n",
            "200 391 Loss: 0.485 | Acc: 82.972% (21347/25728)\n",
            "220 391 Loss: 0.486 | Acc: 82.968% (23470/28288)\n",
            "240 391 Loss: 0.488 | Acc: 82.861% (25561/30848)\n",
            "260 391 Loss: 0.491 | Acc: 82.753% (27646/33408)\n",
            "280 391 Loss: 0.488 | Acc: 82.874% (29808/35968)\n",
            "300 391 Loss: 0.488 | Acc: 82.911% (31944/38528)\n",
            "320 391 Loss: 0.486 | Acc: 82.951% (34083/41088)\n",
            "340 391 Loss: 0.485 | Acc: 82.998% (36227/43648)\n",
            "360 391 Loss: 0.486 | Acc: 82.962% (38335/46208)\n",
            "380 391 Loss: 0.487 | Acc: 82.952% (40454/48768)\n",
            "99 100 Loss: 0.663 | Acc: 77.932% (7777/10000)\n",
            "\n",
            "Epoch: 10\n",
            "0 391 Loss: 0.549 | Acc: 79.688% (102/128)\n",
            "20 391 Loss: 0.448 | Acc: 84.040% (2259/2688)\n",
            "40 391 Loss: 0.433 | Acc: 84.756% (4448/5248)\n",
            "60 391 Loss: 0.428 | Acc: 84.977% (6635/7808)\n",
            "80 391 Loss: 0.433 | Acc: 84.722% (8784/10368)\n",
            "100 391 Loss: 0.434 | Acc: 84.638% (10942/12928)\n",
            "120 391 Loss: 0.441 | Acc: 84.356% (13065/15488)\n",
            "140 391 Loss: 0.442 | Acc: 84.325% (15219/18048)\n",
            "160 391 Loss: 0.445 | Acc: 84.186% (17349/20608)\n",
            "180 391 Loss: 0.449 | Acc: 84.125% (19490/23168)\n",
            "200 391 Loss: 0.452 | Acc: 84.025% (21618/25728)\n",
            "220 391 Loss: 0.455 | Acc: 83.855% (23721/28288)\n",
            "240 391 Loss: 0.455 | Acc: 83.908% (25884/30848)\n",
            "260 391 Loss: 0.454 | Acc: 83.923% (28037/33408)\n",
            "280 391 Loss: 0.455 | Acc: 83.897% (30176/35968)\n",
            "300 391 Loss: 0.453 | Acc: 83.991% (32360/38528)\n",
            "320 391 Loss: 0.451 | Acc: 84.044% (34532/41088)\n",
            "340 391 Loss: 0.453 | Acc: 83.983% (36657/43648)\n",
            "360 391 Loss: 0.451 | Acc: 84.070% (38847/46208)\n",
            "380 391 Loss: 0.450 | Acc: 84.121% (41024/48768)\n",
            "99 100 Loss: 0.588 | Acc: 81.652% (8164/10000)\n",
            "\n",
            "Epoch: 11\n",
            "0 391 Loss: 0.467 | Acc: 83.594% (107/128)\n",
            "20 391 Loss: 0.433 | Acc: 84.859% (2281/2688)\n",
            "40 391 Loss: 0.427 | Acc: 85.080% (4465/5248)\n",
            "60 391 Loss: 0.420 | Acc: 85.476% (6674/7808)\n",
            "80 391 Loss: 0.423 | Acc: 85.195% (8833/10368)\n",
            "100 391 Loss: 0.426 | Acc: 85.218% (11017/12928)\n",
            "120 391 Loss: 0.429 | Acc: 85.021% (13168/15488)\n",
            "140 391 Loss: 0.430 | Acc: 84.940% (15330/18048)\n",
            "160 391 Loss: 0.431 | Acc: 84.938% (17504/20608)\n",
            "180 391 Loss: 0.429 | Acc: 84.975% (19687/23168)\n",
            "200 391 Loss: 0.430 | Acc: 85.009% (21871/25728)\n",
            "220 391 Loss: 0.429 | Acc: 85.061% (24062/28288)\n",
            "240 391 Loss: 0.425 | Acc: 85.137% (26263/30848)\n",
            "260 391 Loss: 0.423 | Acc: 85.228% (28473/33408)\n",
            "280 391 Loss: 0.423 | Acc: 85.254% (30664/35968)\n",
            "300 391 Loss: 0.422 | Acc: 85.276% (32855/38528)\n",
            "320 391 Loss: 0.423 | Acc: 85.244% (35025/41088)\n",
            "340 391 Loss: 0.422 | Acc: 85.301% (37232/43648)\n",
            "360 391 Loss: 0.423 | Acc: 85.269% (39401/46208)\n",
            "380 391 Loss: 0.424 | Acc: 85.232% (41566/48768)\n",
            "99 100 Loss: 0.594 | Acc: 81.351% (8104/10000)\n",
            "\n",
            "Epoch: 12\n",
            "0 391 Loss: 0.375 | Acc: 87.500% (112/128)\n",
            "20 391 Loss: 0.411 | Acc: 85.677% (2303/2688)\n",
            "40 391 Loss: 0.404 | Acc: 85.804% (4503/5248)\n",
            "60 391 Loss: 0.393 | Acc: 86.270% (6736/7808)\n",
            "80 391 Loss: 0.388 | Acc: 86.603% (8979/10368)\n",
            "100 391 Loss: 0.390 | Acc: 86.649% (11202/12928)\n",
            "120 391 Loss: 0.390 | Acc: 86.680% (13425/15488)\n",
            "140 391 Loss: 0.391 | Acc: 86.708% (15649/18048)\n",
            "160 391 Loss: 0.390 | Acc: 86.724% (17872/20608)\n",
            "180 391 Loss: 0.390 | Acc: 86.710% (20089/23168)\n",
            "200 391 Loss: 0.391 | Acc: 86.552% (22268/25728)\n",
            "220 391 Loss: 0.392 | Acc: 86.496% (24468/28288)\n",
            "240 391 Loss: 0.392 | Acc: 86.508% (26686/30848)\n",
            "260 391 Loss: 0.393 | Acc: 86.464% (28886/33408)\n",
            "280 391 Loss: 0.392 | Acc: 86.427% (31086/35968)\n",
            "300 391 Loss: 0.392 | Acc: 86.387% (33283/38528)\n",
            "320 391 Loss: 0.393 | Acc: 86.366% (35486/41088)\n",
            "340 391 Loss: 0.395 | Acc: 86.272% (37656/43648)\n",
            "360 391 Loss: 0.396 | Acc: 86.221% (39841/46208)\n",
            "380 391 Loss: 0.396 | Acc: 86.255% (42065/48768)\n",
            "99 100 Loss: 0.547 | Acc: 82.784% (8225/10000)\n",
            "\n",
            "Epoch: 13\n",
            "0 391 Loss: 0.553 | Acc: 81.250% (104/128)\n",
            "20 391 Loss: 0.388 | Acc: 86.644% (2329/2688)\n",
            "40 391 Loss: 0.365 | Acc: 87.481% (4591/5248)\n",
            "60 391 Loss: 0.350 | Acc: 87.871% (6861/7808)\n",
            "80 391 Loss: 0.356 | Acc: 87.693% (9092/10368)\n",
            "100 391 Loss: 0.362 | Acc: 87.392% (11298/12928)\n",
            "120 391 Loss: 0.363 | Acc: 87.377% (13533/15488)\n",
            "140 391 Loss: 0.365 | Acc: 87.273% (15751/18048)\n",
            "160 391 Loss: 0.365 | Acc: 87.286% (17988/20608)\n",
            "180 391 Loss: 0.367 | Acc: 87.185% (20199/23168)\n",
            "200 391 Loss: 0.367 | Acc: 87.177% (22429/25728)\n",
            "220 391 Loss: 0.372 | Acc: 86.966% (24601/28288)\n",
            "240 391 Loss: 0.373 | Acc: 86.949% (26822/30848)\n",
            "260 391 Loss: 0.372 | Acc: 86.937% (29044/33408)\n",
            "280 391 Loss: 0.371 | Acc: 87.022% (31300/35968)\n",
            "300 391 Loss: 0.373 | Acc: 86.968% (33507/38528)\n",
            "320 391 Loss: 0.374 | Acc: 86.923% (35715/41088)\n",
            "340 391 Loss: 0.374 | Acc: 86.962% (37957/43648)\n",
            "360 391 Loss: 0.373 | Acc: 86.994% (40198/46208)\n",
            "380 391 Loss: 0.373 | Acc: 87.041% (42448/48768)\n",
            "99 100 Loss: 0.578 | Acc: 81.873% (8166/10000)\n",
            "\n",
            "Epoch: 14\n",
            "0 391 Loss: 0.406 | Acc: 85.938% (110/128)\n",
            "20 391 Loss: 0.360 | Acc: 87.426% (2350/2688)\n",
            "40 391 Loss: 0.338 | Acc: 88.091% (4623/5248)\n",
            "60 391 Loss: 0.339 | Acc: 88.217% (6888/7808)\n",
            "80 391 Loss: 0.342 | Acc: 88.059% (9130/10368)\n",
            "100 391 Loss: 0.346 | Acc: 87.980% (11374/12928)\n",
            "120 391 Loss: 0.350 | Acc: 87.868% (13609/15488)\n",
            "140 391 Loss: 0.353 | Acc: 87.794% (15845/18048)\n",
            "160 391 Loss: 0.355 | Acc: 87.699% (18073/20608)\n",
            "180 391 Loss: 0.357 | Acc: 87.573% (20289/23168)\n",
            "200 391 Loss: 0.357 | Acc: 87.547% (22524/25728)\n",
            "220 391 Loss: 0.357 | Acc: 87.592% (24778/28288)\n",
            "240 391 Loss: 0.356 | Acc: 87.633% (27033/30848)\n",
            "260 391 Loss: 0.355 | Acc: 87.647% (29281/33408)\n",
            "280 391 Loss: 0.354 | Acc: 87.670% (31533/35968)\n",
            "300 391 Loss: 0.354 | Acc: 87.643% (33767/38528)\n",
            "320 391 Loss: 0.354 | Acc: 87.641% (36010/41088)\n",
            "340 391 Loss: 0.355 | Acc: 87.624% (38246/43648)\n",
            "360 391 Loss: 0.356 | Acc: 87.619% (40487/46208)\n",
            "380 391 Loss: 0.356 | Acc: 87.641% (42741/48768)\n",
            "99 100 Loss: 0.515 | Acc: 83.444% (8317/10000)\n",
            "\n",
            "Epoch: 15\n",
            "0 391 Loss: 0.377 | Acc: 88.281% (113/128)\n",
            "20 391 Loss: 0.332 | Acc: 88.170% (2370/2688)\n",
            "40 391 Loss: 0.335 | Acc: 88.262% (4632/5248)\n",
            "60 391 Loss: 0.330 | Acc: 88.358% (6899/7808)\n",
            "80 391 Loss: 0.331 | Acc: 88.243% (9149/10368)\n",
            "100 391 Loss: 0.328 | Acc: 88.351% (11422/12928)\n",
            "120 391 Loss: 0.329 | Acc: 88.372% (13687/15488)\n",
            "140 391 Loss: 0.328 | Acc: 88.436% (15961/18048)\n",
            "160 391 Loss: 0.331 | Acc: 88.373% (18212/20608)\n",
            "180 391 Loss: 0.331 | Acc: 88.389% (20478/23168)\n",
            "200 391 Loss: 0.332 | Acc: 88.398% (22743/25728)\n",
            "220 391 Loss: 0.330 | Acc: 88.412% (25010/28288)\n",
            "240 391 Loss: 0.328 | Acc: 88.388% (27266/30848)\n",
            "260 391 Loss: 0.327 | Acc: 88.446% (29548/33408)\n",
            "280 391 Loss: 0.330 | Acc: 88.337% (31773/35968)\n",
            "300 391 Loss: 0.330 | Acc: 88.367% (34046/38528)\n",
            "320 391 Loss: 0.332 | Acc: 88.345% (36299/41088)\n",
            "340 391 Loss: 0.331 | Acc: 88.384% (38578/43648)\n",
            "360 391 Loss: 0.330 | Acc: 88.472% (40881/46208)\n",
            "380 391 Loss: 0.331 | Acc: 88.462% (43141/48768)\n",
            "99 100 Loss: 0.528 | Acc: 83.897% (8386/10000)\n",
            "\n",
            "Epoch: 16\n",
            "0 391 Loss: 0.298 | Acc: 88.281% (113/128)\n",
            "20 391 Loss: 0.329 | Acc: 88.616% (2382/2688)\n",
            "40 391 Loss: 0.319 | Acc: 88.739% (4657/5248)\n",
            "60 391 Loss: 0.313 | Acc: 88.730% (6928/7808)\n",
            "80 391 Loss: 0.311 | Acc: 88.889% (9216/10368)\n",
            "100 391 Loss: 0.307 | Acc: 89.078% (11516/12928)\n",
            "120 391 Loss: 0.308 | Acc: 89.069% (13795/15488)\n",
            "140 391 Loss: 0.307 | Acc: 89.162% (16092/18048)\n",
            "160 391 Loss: 0.307 | Acc: 89.194% (18381/20608)\n",
            "180 391 Loss: 0.306 | Acc: 89.227% (20672/23168)\n",
            "200 391 Loss: 0.305 | Acc: 89.206% (22951/25728)\n",
            "220 391 Loss: 0.305 | Acc: 89.264% (25251/28288)\n",
            "240 391 Loss: 0.305 | Acc: 89.257% (27534/30848)\n",
            "260 391 Loss: 0.307 | Acc: 89.185% (29795/33408)\n",
            "280 391 Loss: 0.310 | Acc: 89.062% (32034/35968)\n",
            "300 391 Loss: 0.311 | Acc: 89.016% (34296/38528)\n",
            "320 391 Loss: 0.312 | Acc: 88.992% (36565/41088)\n",
            "340 391 Loss: 0.313 | Acc: 88.939% (38820/43648)\n",
            "360 391 Loss: 0.314 | Acc: 88.939% (41097/46208)\n",
            "380 391 Loss: 0.315 | Acc: 88.898% (43354/48768)\n",
            "99 100 Loss: 0.469 | Acc: 85.342% (8514/10000)\n",
            "\n",
            "Epoch: 17\n",
            "0 391 Loss: 0.242 | Acc: 92.188% (118/128)\n",
            "20 391 Loss: 0.278 | Acc: 90.588% (2435/2688)\n",
            "40 391 Loss: 0.269 | Acc: 90.796% (4765/5248)\n",
            "60 391 Loss: 0.276 | Acc: 90.804% (7090/7808)\n",
            "80 391 Loss: 0.276 | Acc: 90.808% (9415/10368)\n",
            "100 391 Loss: 0.280 | Acc: 90.656% (11720/12928)\n",
            "120 391 Loss: 0.285 | Acc: 90.283% (13983/15488)\n",
            "140 391 Loss: 0.286 | Acc: 90.187% (16277/18048)\n",
            "160 391 Loss: 0.288 | Acc: 90.038% (18555/20608)\n",
            "180 391 Loss: 0.290 | Acc: 89.952% (20840/23168)\n",
            "200 391 Loss: 0.290 | Acc: 89.960% (23145/25728)\n",
            "220 391 Loss: 0.289 | Acc: 89.957% (25447/28288)\n",
            "240 391 Loss: 0.291 | Acc: 89.883% (27727/30848)\n",
            "260 391 Loss: 0.293 | Acc: 89.820% (30007/33408)\n",
            "280 391 Loss: 0.292 | Acc: 89.822% (32307/35968)\n",
            "300 391 Loss: 0.290 | Acc: 89.929% (34648/38528)\n",
            "320 391 Loss: 0.291 | Acc: 89.902% (36939/41088)\n",
            "340 391 Loss: 0.292 | Acc: 89.864% (39224/43648)\n",
            "360 391 Loss: 0.293 | Acc: 89.816% (41502/46208)\n",
            "380 391 Loss: 0.295 | Acc: 89.723% (43756/48768)\n",
            "99 100 Loss: 0.492 | Acc: 84.476% (8442/10000)\n",
            "\n",
            "Epoch: 18\n",
            "0 391 Loss: 0.244 | Acc: 93.750% (120/128)\n",
            "20 391 Loss: 0.264 | Acc: 90.551% (2434/2688)\n",
            "40 391 Loss: 0.263 | Acc: 90.682% (4759/5248)\n",
            "60 391 Loss: 0.261 | Acc: 90.663% (7079/7808)\n",
            "80 391 Loss: 0.265 | Acc: 90.471% (9380/10368)\n",
            "100 391 Loss: 0.268 | Acc: 90.331% (11678/12928)\n",
            "120 391 Loss: 0.270 | Acc: 90.218% (13973/15488)\n",
            "140 391 Loss: 0.269 | Acc: 90.320% (16301/18048)\n",
            "160 391 Loss: 0.271 | Acc: 90.188% (18586/20608)\n",
            "180 391 Loss: 0.270 | Acc: 90.237% (20906/23168)\n",
            "200 391 Loss: 0.272 | Acc: 90.194% (23205/25728)\n",
            "220 391 Loss: 0.275 | Acc: 90.169% (25507/28288)\n",
            "240 391 Loss: 0.277 | Acc: 90.152% (27810/30848)\n",
            "260 391 Loss: 0.276 | Acc: 90.206% (30136/33408)\n",
            "280 391 Loss: 0.276 | Acc: 90.197% (32442/35968)\n",
            "300 391 Loss: 0.275 | Acc: 90.233% (34765/38528)\n",
            "320 391 Loss: 0.276 | Acc: 90.214% (37067/41088)\n",
            "340 391 Loss: 0.277 | Acc: 90.224% (39381/43648)\n",
            "360 391 Loss: 0.277 | Acc: 90.188% (41674/46208)\n",
            "380 391 Loss: 0.279 | Acc: 90.133% (43956/48768)\n",
            "99 100 Loss: 0.570 | Acc: 83.553% (8319/10000)\n",
            "\n",
            "Epoch: 19\n",
            "0 391 Loss: 0.313 | Acc: 88.281% (113/128)\n",
            "20 391 Loss: 0.256 | Acc: 90.513% (2433/2688)\n",
            "40 391 Loss: 0.260 | Acc: 90.701% (4760/5248)\n",
            "60 391 Loss: 0.255 | Acc: 90.971% (7103/7808)\n",
            "80 391 Loss: 0.256 | Acc: 90.837% (9418/10368)\n",
            "100 391 Loss: 0.258 | Acc: 90.842% (11744/12928)\n",
            "120 391 Loss: 0.258 | Acc: 90.819% (14066/15488)\n",
            "140 391 Loss: 0.260 | Acc: 90.697% (16369/18048)\n",
            "160 391 Loss: 0.264 | Acc: 90.596% (18670/20608)\n",
            "180 391 Loss: 0.264 | Acc: 90.664% (21005/23168)\n",
            "200 391 Loss: 0.261 | Acc: 90.784% (23357/25728)\n",
            "220 391 Loss: 0.262 | Acc: 90.756% (25673/28288)\n",
            "240 391 Loss: 0.262 | Acc: 90.683% (27974/30848)\n",
            "260 391 Loss: 0.263 | Acc: 90.676% (30293/33408)\n",
            "280 391 Loss: 0.265 | Acc: 90.622% (32595/35968)\n",
            "300 391 Loss: 0.264 | Acc: 90.664% (34931/38528)\n",
            "320 391 Loss: 0.264 | Acc: 90.679% (37258/41088)\n",
            "340 391 Loss: 0.265 | Acc: 90.632% (39559/43648)\n",
            "360 391 Loss: 0.264 | Acc: 90.642% (41884/46208)\n",
            "380 391 Loss: 0.265 | Acc: 90.592% (44180/48768)\n",
            "99 100 Loss: 0.533 | Acc: 83.525% (8332/10000)\n",
            "\n",
            "Epoch: 20\n",
            "0 391 Loss: 0.322 | Acc: 89.844% (115/128)\n",
            "20 391 Loss: 0.228 | Acc: 92.262% (2480/2688)\n",
            "40 391 Loss: 0.239 | Acc: 91.502% (4802/5248)\n",
            "60 391 Loss: 0.242 | Acc: 91.368% (7134/7808)\n",
            "80 391 Loss: 0.242 | Acc: 91.358% (9472/10368)\n",
            "100 391 Loss: 0.241 | Acc: 91.484% (11827/12928)\n",
            "120 391 Loss: 0.243 | Acc: 91.406% (14157/15488)\n",
            "140 391 Loss: 0.248 | Acc: 91.168% (16454/18048)\n",
            "160 391 Loss: 0.251 | Acc: 91.057% (18765/20608)\n",
            "180 391 Loss: 0.252 | Acc: 90.975% (21077/23168)\n",
            "200 391 Loss: 0.255 | Acc: 90.948% (23399/25728)\n",
            "220 391 Loss: 0.255 | Acc: 90.975% (25735/28288)\n",
            "240 391 Loss: 0.256 | Acc: 90.891% (28038/30848)\n",
            "260 391 Loss: 0.257 | Acc: 90.873% (30359/33408)\n",
            "280 391 Loss: 0.260 | Acc: 90.778% (32651/35968)\n",
            "300 391 Loss: 0.258 | Acc: 90.872% (35011/38528)\n",
            "320 391 Loss: 0.257 | Acc: 90.890% (37345/41088)\n",
            "340 391 Loss: 0.257 | Acc: 90.900% (39676/43648)\n",
            "360 391 Loss: 0.259 | Acc: 90.831% (41971/46208)\n",
            "380 391 Loss: 0.260 | Acc: 90.818% (44290/48768)\n",
            "99 100 Loss: 0.556 | Acc: 83.994% (8416/10000)\n",
            "\n",
            "Epoch: 21\n",
            "0 391 Loss: 0.173 | Acc: 92.969% (119/128)\n",
            "20 391 Loss: 0.233 | Acc: 91.667% (2464/2688)\n",
            "40 391 Loss: 0.242 | Acc: 91.444% (4799/5248)\n",
            "60 391 Loss: 0.234 | Acc: 91.803% (7168/7808)\n",
            "80 391 Loss: 0.237 | Acc: 91.657% (9503/10368)\n",
            "100 391 Loss: 0.230 | Acc: 91.847% (11874/12928)\n",
            "120 391 Loss: 0.227 | Acc: 91.942% (14240/15488)\n",
            "140 391 Loss: 0.230 | Acc: 91.850% (16577/18048)\n",
            "160 391 Loss: 0.233 | Acc: 91.838% (18926/20608)\n",
            "180 391 Loss: 0.233 | Acc: 91.821% (21273/23168)\n",
            "200 391 Loss: 0.232 | Acc: 91.845% (23630/25728)\n",
            "220 391 Loss: 0.231 | Acc: 91.894% (25995/28288)\n",
            "240 391 Loss: 0.232 | Acc: 91.795% (28317/30848)\n",
            "260 391 Loss: 0.233 | Acc: 91.780% (30662/33408)\n",
            "280 391 Loss: 0.234 | Acc: 91.804% (33020/35968)\n",
            "300 391 Loss: 0.234 | Acc: 91.811% (35373/38528)\n",
            "320 391 Loss: 0.235 | Acc: 91.764% (37704/41088)\n",
            "340 391 Loss: 0.236 | Acc: 91.679% (40016/43648)\n",
            "360 391 Loss: 0.237 | Acc: 91.633% (42342/46208)\n",
            "380 391 Loss: 0.238 | Acc: 91.589% (44666/48768)\n",
            "99 100 Loss: 0.478 | Acc: 85.834% (8551/10000)\n",
            "\n",
            "Epoch: 22\n",
            "0 391 Loss: 0.175 | Acc: 93.750% (120/128)\n",
            "20 391 Loss: 0.217 | Acc: 92.560% (2488/2688)\n",
            "40 391 Loss: 0.212 | Acc: 92.530% (4856/5248)\n",
            "60 391 Loss: 0.217 | Acc: 92.431% (7217/7808)\n",
            "80 391 Loss: 0.213 | Acc: 92.641% (9605/10368)\n",
            "100 391 Loss: 0.210 | Acc: 92.667% (11980/12928)\n",
            "120 391 Loss: 0.210 | Acc: 92.646% (14349/15488)\n",
            "140 391 Loss: 0.214 | Acc: 92.531% (16700/18048)\n",
            "160 391 Loss: 0.214 | Acc: 92.474% (19057/20608)\n",
            "180 391 Loss: 0.217 | Acc: 92.369% (21400/23168)\n",
            "200 391 Loss: 0.218 | Acc: 92.366% (23764/25728)\n",
            "220 391 Loss: 0.221 | Acc: 92.233% (26091/28288)\n",
            "240 391 Loss: 0.222 | Acc: 92.220% (28448/30848)\n",
            "260 391 Loss: 0.224 | Acc: 92.098% (30768/33408)\n",
            "280 391 Loss: 0.225 | Acc: 92.074% (33117/35968)\n",
            "300 391 Loss: 0.226 | Acc: 92.034% (35459/38528)\n",
            "320 391 Loss: 0.226 | Acc: 92.029% (37813/41088)\n",
            "340 391 Loss: 0.227 | Acc: 91.970% (40143/43648)\n",
            "360 391 Loss: 0.228 | Acc: 91.967% (42496/46208)\n",
            "380 391 Loss: 0.228 | Acc: 92.003% (44868/48768)\n",
            "99 100 Loss: 0.470 | Acc: 85.384% (8533/10000)\n",
            "\n",
            "Epoch: 23\n",
            "0 391 Loss: 0.216 | Acc: 91.406% (117/128)\n",
            "20 391 Loss: 0.208 | Acc: 92.932% (2498/2688)\n",
            "40 391 Loss: 0.209 | Acc: 92.912% (4876/5248)\n",
            "60 391 Loss: 0.208 | Acc: 92.905% (7254/7808)\n",
            "80 391 Loss: 0.212 | Acc: 92.747% (9616/10368)\n",
            "100 391 Loss: 0.210 | Acc: 92.775% (11994/12928)\n",
            "120 391 Loss: 0.210 | Acc: 92.801% (14373/15488)\n",
            "140 391 Loss: 0.212 | Acc: 92.703% (16731/18048)\n",
            "160 391 Loss: 0.215 | Acc: 92.585% (19080/20608)\n",
            "180 391 Loss: 0.214 | Acc: 92.567% (21446/23168)\n",
            "200 391 Loss: 0.213 | Acc: 92.603% (23825/25728)\n",
            "220 391 Loss: 0.214 | Acc: 92.534% (26176/28288)\n",
            "240 391 Loss: 0.213 | Acc: 92.593% (28563/30848)\n",
            "260 391 Loss: 0.214 | Acc: 92.526% (30911/33408)\n",
            "280 391 Loss: 0.214 | Acc: 92.538% (33284/35968)\n",
            "300 391 Loss: 0.216 | Acc: 92.442% (35616/38528)\n",
            "320 391 Loss: 0.217 | Acc: 92.387% (37960/41088)\n",
            "340 391 Loss: 0.218 | Acc: 92.316% (40294/43648)\n",
            "360 391 Loss: 0.218 | Acc: 92.300% (42650/46208)\n",
            "380 391 Loss: 0.218 | Acc: 92.280% (45003/48768)\n",
            "99 100 Loss: 0.426 | Acc: 86.883% (8657/10000)\n",
            "\n",
            "Epoch: 24\n",
            "0 391 Loss: 0.201 | Acc: 93.750% (120/128)\n",
            "20 391 Loss: 0.182 | Acc: 94.048% (2528/2688)\n",
            "40 391 Loss: 0.180 | Acc: 94.112% (4939/5248)\n",
            "60 391 Loss: 0.185 | Acc: 93.801% (7324/7808)\n",
            "80 391 Loss: 0.192 | Acc: 93.547% (9699/10368)\n",
            "100 391 Loss: 0.202 | Acc: 93.108% (12037/12928)\n",
            "120 391 Loss: 0.203 | Acc: 93.001% (14404/15488)\n",
            "140 391 Loss: 0.203 | Acc: 93.024% (16789/18048)\n",
            "160 391 Loss: 0.200 | Acc: 93.085% (19183/20608)\n",
            "180 391 Loss: 0.201 | Acc: 93.021% (21551/23168)\n",
            "200 391 Loss: 0.202 | Acc: 92.996% (23926/25728)\n",
            "220 391 Loss: 0.202 | Acc: 93.004% (26309/28288)\n",
            "240 391 Loss: 0.202 | Acc: 92.966% (28678/30848)\n",
            "260 391 Loss: 0.205 | Acc: 92.885% (31031/33408)\n",
            "280 391 Loss: 0.206 | Acc: 92.841% (33393/35968)\n",
            "300 391 Loss: 0.207 | Acc: 92.753% (35736/38528)\n",
            "320 391 Loss: 0.207 | Acc: 92.738% (38104/41088)\n",
            "340 391 Loss: 0.208 | Acc: 92.694% (40459/43648)\n",
            "360 391 Loss: 0.209 | Acc: 92.670% (42821/46208)\n",
            "380 391 Loss: 0.209 | Acc: 92.678% (45197/48768)\n",
            "99 100 Loss: 0.480 | Acc: 85.927% (8597/10000)\n",
            "\n",
            "Epoch: 25\n",
            "0 391 Loss: 0.152 | Acc: 94.531% (121/128)\n",
            "20 391 Loss: 0.197 | Acc: 92.411% (2484/2688)\n",
            "40 391 Loss: 0.198 | Acc: 92.550% (4857/5248)\n",
            "60 391 Loss: 0.193 | Acc: 92.994% (7261/7808)\n",
            "80 391 Loss: 0.184 | Acc: 93.345% (9678/10368)\n",
            "100 391 Loss: 0.183 | Acc: 93.603% (12101/12928)\n",
            "120 391 Loss: 0.183 | Acc: 93.653% (14505/15488)\n",
            "140 391 Loss: 0.187 | Acc: 93.611% (16895/18048)\n",
            "160 391 Loss: 0.186 | Acc: 93.614% (19292/20608)\n",
            "180 391 Loss: 0.186 | Acc: 93.646% (21696/23168)\n",
            "200 391 Loss: 0.187 | Acc: 93.583% (24077/25728)\n",
            "220 391 Loss: 0.189 | Acc: 93.442% (26433/28288)\n",
            "240 391 Loss: 0.192 | Acc: 93.355% (28798/30848)\n",
            "260 391 Loss: 0.192 | Acc: 93.334% (31181/33408)\n",
            "280 391 Loss: 0.194 | Acc: 93.247% (33539/35968)\n",
            "300 391 Loss: 0.197 | Acc: 93.130% (35881/38528)\n",
            "320 391 Loss: 0.199 | Acc: 93.071% (38241/41088)\n",
            "340 391 Loss: 0.200 | Acc: 92.989% (40588/43648)\n",
            "360 391 Loss: 0.200 | Acc: 92.984% (42966/46208)\n",
            "380 391 Loss: 0.199 | Acc: 93.043% (45375/48768)\n",
            "99 100 Loss: 0.532 | Acc: 85.216% (8498/10000)\n",
            "\n",
            "Epoch: 26\n",
            "0 391 Loss: 0.208 | Acc: 92.969% (119/128)\n",
            "20 391 Loss: 0.152 | Acc: 94.494% (2540/2688)\n",
            "40 391 Loss: 0.154 | Acc: 94.417% (4955/5248)\n",
            "60 391 Loss: 0.162 | Acc: 94.237% (7358/7808)\n",
            "80 391 Loss: 0.163 | Acc: 94.252% (9772/10368)\n",
            "100 391 Loss: 0.164 | Acc: 94.199% (12178/12928)\n",
            "120 391 Loss: 0.168 | Acc: 94.041% (14565/15488)\n",
            "140 391 Loss: 0.170 | Acc: 94.010% (16967/18048)\n",
            "160 391 Loss: 0.172 | Acc: 93.993% (19370/20608)\n",
            "180 391 Loss: 0.176 | Acc: 93.888% (21752/23168)\n",
            "200 391 Loss: 0.177 | Acc: 93.816% (24137/25728)\n",
            "220 391 Loss: 0.177 | Acc: 93.761% (26523/28288)\n",
            "240 391 Loss: 0.178 | Acc: 93.727% (28913/30848)\n",
            "260 391 Loss: 0.180 | Acc: 93.705% (31305/33408)\n",
            "280 391 Loss: 0.180 | Acc: 93.672% (33692/35968)\n",
            "300 391 Loss: 0.181 | Acc: 93.644% (36079/38528)\n",
            "320 391 Loss: 0.183 | Acc: 93.582% (38451/41088)\n",
            "340 391 Loss: 0.183 | Acc: 93.560% (40837/43648)\n",
            "360 391 Loss: 0.184 | Acc: 93.501% (43205/46208)\n",
            "380 391 Loss: 0.185 | Acc: 93.473% (45585/48768)\n",
            "99 100 Loss: 0.492 | Acc: 86.096% (8620/10000)\n",
            "\n",
            "Epoch: 27\n",
            "0 391 Loss: 0.092 | Acc: 97.656% (125/128)\n",
            "20 391 Loss: 0.184 | Acc: 93.713% (2519/2688)\n",
            "40 391 Loss: 0.177 | Acc: 93.960% (4931/5248)\n",
            "60 391 Loss: 0.173 | Acc: 94.185% (7354/7808)\n",
            "80 391 Loss: 0.169 | Acc: 94.203% (9767/10368)\n",
            "100 391 Loss: 0.177 | Acc: 93.858% (12134/12928)\n",
            "120 391 Loss: 0.177 | Acc: 93.724% (14516/15488)\n",
            "140 391 Loss: 0.175 | Acc: 93.772% (16924/18048)\n",
            "160 391 Loss: 0.174 | Acc: 93.823% (19335/20608)\n",
            "180 391 Loss: 0.174 | Acc: 93.854% (21744/23168)\n",
            "200 391 Loss: 0.174 | Acc: 93.863% (24149/25728)\n",
            "220 391 Loss: 0.176 | Acc: 93.842% (26546/28288)\n",
            "240 391 Loss: 0.175 | Acc: 93.880% (28960/30848)\n",
            "260 391 Loss: 0.175 | Acc: 93.879% (31363/33408)\n",
            "280 391 Loss: 0.178 | Acc: 93.786% (33733/35968)\n",
            "300 391 Loss: 0.178 | Acc: 93.766% (36126/38528)\n",
            "320 391 Loss: 0.178 | Acc: 93.811% (38545/41088)\n",
            "340 391 Loss: 0.176 | Acc: 93.855% (40966/43648)\n",
            "360 391 Loss: 0.176 | Acc: 93.828% (43356/46208)\n",
            "380 391 Loss: 0.178 | Acc: 93.787% (45738/48768)\n",
            "99 100 Loss: 0.684 | Acc: 82.116% (8207/10000)\n",
            "\n",
            "Epoch: 28\n",
            "0 391 Loss: 0.138 | Acc: 94.531% (121/128)\n",
            "20 391 Loss: 0.163 | Acc: 93.676% (2518/2688)\n",
            "40 391 Loss: 0.160 | Acc: 94.284% (4948/5248)\n",
            "60 391 Loss: 0.165 | Acc: 94.288% (7362/7808)\n",
            "80 391 Loss: 0.169 | Acc: 94.097% (9756/10368)\n",
            "100 391 Loss: 0.172 | Acc: 93.951% (12146/12928)\n",
            "120 391 Loss: 0.170 | Acc: 94.021% (14562/15488)\n",
            "140 391 Loss: 0.170 | Acc: 94.049% (16974/18048)\n",
            "160 391 Loss: 0.170 | Acc: 94.080% (19388/20608)\n",
            "180 391 Loss: 0.169 | Acc: 94.126% (21807/23168)\n",
            "200 391 Loss: 0.167 | Acc: 94.143% (24221/25728)\n",
            "220 391 Loss: 0.168 | Acc: 94.121% (26625/28288)\n",
            "240 391 Loss: 0.168 | Acc: 94.081% (29022/30848)\n",
            "260 391 Loss: 0.168 | Acc: 94.061% (31424/33408)\n",
            "280 391 Loss: 0.168 | Acc: 94.047% (33827/35968)\n",
            "300 391 Loss: 0.169 | Acc: 94.010% (36220/38528)\n",
            "320 391 Loss: 0.171 | Acc: 93.933% (38595/41088)\n",
            "340 391 Loss: 0.172 | Acc: 93.883% (40978/43648)\n",
            "360 391 Loss: 0.172 | Acc: 93.899% (43389/46208)\n",
            "380 391 Loss: 0.172 | Acc: 93.906% (45796/48768)\n",
            "99 100 Loss: 0.534 | Acc: 86.335% (8598/10000)\n",
            "\n",
            "Epoch: 29\n",
            "0 391 Loss: 0.174 | Acc: 91.406% (117/128)\n",
            "20 391 Loss: 0.151 | Acc: 94.494% (2540/2688)\n",
            "40 391 Loss: 0.152 | Acc: 94.665% (4968/5248)\n",
            "60 391 Loss: 0.151 | Acc: 94.723% (7396/7808)\n",
            "80 391 Loss: 0.151 | Acc: 94.686% (9817/10368)\n",
            "100 391 Loss: 0.149 | Acc: 94.810% (12257/12928)\n",
            "120 391 Loss: 0.151 | Acc: 94.731% (14672/15488)\n",
            "140 391 Loss: 0.149 | Acc: 94.803% (17110/18048)\n",
            "160 391 Loss: 0.150 | Acc: 94.730% (19522/20608)\n",
            "180 391 Loss: 0.152 | Acc: 94.557% (21907/23168)\n",
            "200 391 Loss: 0.155 | Acc: 94.465% (24304/25728)\n",
            "220 391 Loss: 0.158 | Acc: 94.386% (26700/28288)\n",
            "240 391 Loss: 0.159 | Acc: 94.372% (29112/30848)\n",
            "260 391 Loss: 0.161 | Acc: 94.292% (31501/33408)\n",
            "280 391 Loss: 0.161 | Acc: 94.306% (33920/35968)\n",
            "300 391 Loss: 0.162 | Acc: 94.256% (36315/38528)\n",
            "320 391 Loss: 0.163 | Acc: 94.234% (38719/41088)\n",
            "340 391 Loss: 0.163 | Acc: 94.259% (41142/43648)\n",
            "360 391 Loss: 0.164 | Acc: 94.222% (43538/46208)\n",
            "380 391 Loss: 0.164 | Acc: 94.218% (45948/48768)\n",
            "99 100 Loss: 0.558 | Acc: 85.762% (8572/10000)\n",
            "\n",
            "Epoch: 30\n",
            "0 391 Loss: 0.105 | Acc: 96.875% (124/128)\n",
            "20 391 Loss: 0.139 | Acc: 95.052% (2555/2688)\n",
            "40 391 Loss: 0.141 | Acc: 94.950% (4983/5248)\n",
            "60 391 Loss: 0.140 | Acc: 94.967% (7415/7808)\n",
            "80 391 Loss: 0.140 | Acc: 94.898% (9839/10368)\n",
            "100 391 Loss: 0.145 | Acc: 94.771% (12252/12928)\n",
            "120 391 Loss: 0.147 | Acc: 94.738% (14673/15488)\n",
            "140 391 Loss: 0.150 | Acc: 94.670% (17086/18048)\n",
            "160 391 Loss: 0.150 | Acc: 94.672% (19510/20608)\n",
            "180 391 Loss: 0.151 | Acc: 94.605% (21918/23168)\n",
            "200 391 Loss: 0.150 | Acc: 94.628% (24346/25728)\n",
            "220 391 Loss: 0.149 | Acc: 94.648% (26774/28288)\n",
            "240 391 Loss: 0.150 | Acc: 94.603% (29183/30848)\n",
            "260 391 Loss: 0.150 | Acc: 94.630% (31614/33408)\n",
            "280 391 Loss: 0.150 | Acc: 94.631% (34037/35968)\n",
            "300 391 Loss: 0.150 | Acc: 94.658% (36470/38528)\n",
            "320 391 Loss: 0.150 | Acc: 94.660% (38894/41088)\n",
            "340 391 Loss: 0.151 | Acc: 94.644% (41310/43648)\n",
            "360 391 Loss: 0.153 | Acc: 94.594% (43710/46208)\n",
            "380 391 Loss: 0.154 | Acc: 94.576% (46123/48768)\n",
            "99 100 Loss: 0.492 | Acc: 86.384% (8604/10000)\n",
            "\n",
            "Epoch: 31\n",
            "0 391 Loss: 0.105 | Acc: 96.094% (123/128)\n",
            "20 391 Loss: 0.130 | Acc: 95.238% (2560/2688)\n",
            "40 391 Loss: 0.135 | Acc: 95.046% (4988/5248)\n",
            "60 391 Loss: 0.130 | Acc: 95.210% (7434/7808)\n",
            "80 391 Loss: 0.139 | Acc: 94.878% (9837/10368)\n",
            "100 391 Loss: 0.139 | Acc: 94.872% (12265/12928)\n",
            "120 391 Loss: 0.139 | Acc: 94.919% (14701/15488)\n",
            "140 391 Loss: 0.139 | Acc: 94.941% (17135/18048)\n",
            "160 391 Loss: 0.137 | Acc: 95.026% (19583/20608)\n",
            "180 391 Loss: 0.136 | Acc: 95.092% (22031/23168)\n",
            "200 391 Loss: 0.138 | Acc: 95.064% (24458/25728)\n",
            "220 391 Loss: 0.138 | Acc: 95.058% (26890/28288)\n",
            "240 391 Loss: 0.139 | Acc: 95.050% (29321/30848)\n",
            "260 391 Loss: 0.142 | Acc: 94.977% (31730/33408)\n",
            "280 391 Loss: 0.142 | Acc: 94.998% (34169/35968)\n",
            "300 391 Loss: 0.143 | Acc: 94.978% (36593/38528)\n",
            "320 391 Loss: 0.144 | Acc: 94.930% (39005/41088)\n",
            "340 391 Loss: 0.145 | Acc: 94.873% (41410/43648)\n",
            "360 391 Loss: 0.146 | Acc: 94.830% (43819/46208)\n",
            "380 391 Loss: 0.146 | Acc: 94.820% (46242/48768)\n",
            "99 100 Loss: 0.445 | Acc: 87.771% (8776/10000)\n",
            "\n",
            "Epoch: 32\n",
            "0 391 Loss: 0.120 | Acc: 96.094% (123/128)\n",
            "20 391 Loss: 0.120 | Acc: 95.610% (2570/2688)\n",
            "40 391 Loss: 0.130 | Acc: 95.274% (5000/5248)\n",
            "60 391 Loss: 0.128 | Acc: 95.389% (7448/7808)\n",
            "80 391 Loss: 0.131 | Acc: 95.274% (9878/10368)\n",
            "100 391 Loss: 0.128 | Acc: 95.421% (12336/12928)\n",
            "120 391 Loss: 0.128 | Acc: 95.409% (14777/15488)\n",
            "140 391 Loss: 0.132 | Acc: 95.279% (17196/18048)\n",
            "160 391 Loss: 0.134 | Acc: 95.181% (19615/20608)\n",
            "180 391 Loss: 0.137 | Acc: 95.075% (22027/23168)\n",
            "200 391 Loss: 0.139 | Acc: 95.021% (24447/25728)\n",
            "220 391 Loss: 0.137 | Acc: 95.115% (26906/28288)\n",
            "240 391 Loss: 0.138 | Acc: 95.082% (29331/30848)\n",
            "260 391 Loss: 0.139 | Acc: 95.085% (31766/33408)\n",
            "280 391 Loss: 0.138 | Acc: 95.090% (34202/35968)\n",
            "300 391 Loss: 0.140 | Acc: 95.027% (36612/38528)\n",
            "320 391 Loss: 0.141 | Acc: 94.999% (39033/41088)\n",
            "340 391 Loss: 0.142 | Acc: 94.957% (41447/43648)\n",
            "360 391 Loss: 0.144 | Acc: 94.906% (43854/46208)\n",
            "380 391 Loss: 0.144 | Acc: 94.892% (46277/48768)\n",
            "99 100 Loss: 0.444 | Acc: 87.309% (8751/10000)\n",
            "\n",
            "Epoch: 33\n",
            "0 391 Loss: 0.156 | Acc: 95.312% (122/128)\n",
            "20 391 Loss: 0.118 | Acc: 96.057% (2582/2688)\n",
            "40 391 Loss: 0.126 | Acc: 95.541% (5014/5248)\n",
            "60 391 Loss: 0.121 | Acc: 95.825% (7482/7808)\n",
            "80 391 Loss: 0.124 | Acc: 95.795% (9932/10368)\n",
            "100 391 Loss: 0.126 | Acc: 95.684% (12370/12928)\n",
            "120 391 Loss: 0.128 | Acc: 95.590% (14805/15488)\n",
            "140 391 Loss: 0.128 | Acc: 95.562% (17247/18048)\n",
            "160 391 Loss: 0.130 | Acc: 95.419% (19664/20608)\n",
            "180 391 Loss: 0.133 | Acc: 95.338% (22088/23168)\n",
            "200 391 Loss: 0.133 | Acc: 95.312% (24522/25728)\n",
            "220 391 Loss: 0.135 | Acc: 95.249% (26944/28288)\n",
            "240 391 Loss: 0.135 | Acc: 95.193% (29365/30848)\n",
            "260 391 Loss: 0.134 | Acc: 95.262% (31825/33408)\n",
            "280 391 Loss: 0.135 | Acc: 95.215% (34247/35968)\n",
            "300 391 Loss: 0.134 | Acc: 95.250% (36698/38528)\n",
            "320 391 Loss: 0.134 | Acc: 95.276% (39147/41088)\n",
            "340 391 Loss: 0.135 | Acc: 95.251% (41575/43648)\n",
            "360 391 Loss: 0.135 | Acc: 95.252% (44014/46208)\n",
            "380 391 Loss: 0.136 | Acc: 95.230% (46442/48768)\n",
            "99 100 Loss: 0.472 | Acc: 86.959% (8712/10000)\n",
            "\n",
            "Epoch: 34\n",
            "0 391 Loss: 0.156 | Acc: 94.531% (121/128)\n",
            "20 391 Loss: 0.120 | Acc: 95.424% (2565/2688)\n",
            "40 391 Loss: 0.122 | Acc: 95.598% (5017/5248)\n",
            "60 391 Loss: 0.122 | Acc: 95.479% (7455/7808)\n",
            "80 391 Loss: 0.122 | Acc: 95.486% (9900/10368)\n",
            "100 391 Loss: 0.122 | Acc: 95.537% (12351/12928)\n",
            "120 391 Loss: 0.119 | Acc: 95.706% (14823/15488)\n",
            "140 391 Loss: 0.117 | Acc: 95.806% (17291/18048)\n",
            "160 391 Loss: 0.116 | Acc: 95.851% (19753/20608)\n",
            "180 391 Loss: 0.118 | Acc: 95.753% (22184/23168)\n",
            "200 391 Loss: 0.121 | Acc: 95.670% (24614/25728)\n",
            "220 391 Loss: 0.122 | Acc: 95.599% (27043/28288)\n",
            "240 391 Loss: 0.122 | Acc: 95.575% (29483/30848)\n",
            "260 391 Loss: 0.124 | Acc: 95.528% (31914/33408)\n",
            "280 391 Loss: 0.125 | Acc: 95.493% (34347/35968)\n",
            "300 391 Loss: 0.126 | Acc: 95.453% (36776/38528)\n",
            "320 391 Loss: 0.127 | Acc: 95.398% (39197/41088)\n",
            "340 391 Loss: 0.128 | Acc: 95.397% (41639/43648)\n",
            "360 391 Loss: 0.129 | Acc: 95.367% (44067/46208)\n",
            "380 391 Loss: 0.130 | Acc: 95.333% (46492/48768)\n",
            "99 100 Loss: 0.455 | Acc: 87.573% (8771/10000)\n",
            "\n",
            "Epoch: 35\n",
            "0 391 Loss: 0.090 | Acc: 98.438% (126/128)\n",
            "20 391 Loss: 0.113 | Acc: 96.168% (2585/2688)\n",
            "40 391 Loss: 0.122 | Acc: 95.903% (5033/5248)\n",
            "60 391 Loss: 0.117 | Acc: 96.004% (7496/7808)\n",
            "80 391 Loss: 0.118 | Acc: 95.930% (9946/10368)\n",
            "100 391 Loss: 0.120 | Acc: 95.823% (12388/12928)\n",
            "120 391 Loss: 0.121 | Acc: 95.848% (14845/15488)\n",
            "140 391 Loss: 0.122 | Acc: 95.772% (17285/18048)\n",
            "160 391 Loss: 0.122 | Acc: 95.817% (19746/20608)\n",
            "180 391 Loss: 0.123 | Acc: 95.757% (22185/23168)\n",
            "200 391 Loss: 0.125 | Acc: 95.682% (24617/25728)\n",
            "220 391 Loss: 0.127 | Acc: 95.592% (27041/28288)\n",
            "240 391 Loss: 0.129 | Acc: 95.539% (29472/30848)\n",
            "260 391 Loss: 0.129 | Acc: 95.519% (31911/33408)\n",
            "280 391 Loss: 0.129 | Acc: 95.532% (34361/35968)\n",
            "300 391 Loss: 0.129 | Acc: 95.525% (36804/38528)\n",
            "320 391 Loss: 0.129 | Acc: 95.517% (39246/41088)\n",
            "340 391 Loss: 0.129 | Acc: 95.526% (41695/43648)\n",
            "360 391 Loss: 0.129 | Acc: 95.512% (44134/46208)\n",
            "380 391 Loss: 0.129 | Acc: 95.509% (46578/48768)\n",
            "99 100 Loss: 0.429 | Acc: 87.846% (8816/10000)\n",
            "\n",
            "Epoch: 36\n",
            "0 391 Loss: 0.134 | Acc: 95.312% (122/128)\n",
            "20 391 Loss: 0.104 | Acc: 96.429% (2592/2688)\n",
            "40 391 Loss: 0.105 | Acc: 96.322% (5055/5248)\n",
            "60 391 Loss: 0.109 | Acc: 96.119% (7505/7808)\n",
            "80 391 Loss: 0.108 | Acc: 96.094% (9963/10368)\n",
            "100 391 Loss: 0.111 | Acc: 96.032% (12415/12928)\n",
            "120 391 Loss: 0.111 | Acc: 96.068% (14879/15488)\n",
            "140 391 Loss: 0.114 | Acc: 95.939% (17315/18048)\n",
            "160 391 Loss: 0.117 | Acc: 95.851% (19753/20608)\n",
            "180 391 Loss: 0.116 | Acc: 95.878% (22213/23168)\n",
            "200 391 Loss: 0.115 | Acc: 95.911% (24676/25728)\n",
            "220 391 Loss: 0.115 | Acc: 95.928% (27136/28288)\n",
            "240 391 Loss: 0.116 | Acc: 95.902% (29584/30848)\n",
            "260 391 Loss: 0.118 | Acc: 95.809% (32008/33408)\n",
            "280 391 Loss: 0.119 | Acc: 95.780% (34450/35968)\n",
            "300 391 Loss: 0.120 | Acc: 95.733% (36884/38528)\n",
            "320 391 Loss: 0.120 | Acc: 95.724% (39331/41088)\n",
            "340 391 Loss: 0.121 | Acc: 95.684% (41764/43648)\n",
            "360 391 Loss: 0.122 | Acc: 95.639% (44193/46208)\n",
            "380 391 Loss: 0.123 | Acc: 95.575% (46610/48768)\n",
            "99 100 Loss: 0.521 | Acc: 86.959% (8683/10000)\n",
            "\n",
            "Epoch: 37\n",
            "0 391 Loss: 0.115 | Acc: 95.312% (122/128)\n",
            "20 391 Loss: 0.112 | Acc: 96.354% (2590/2688)\n",
            "40 391 Loss: 0.117 | Acc: 96.208% (5049/5248)\n",
            "60 391 Loss: 0.114 | Acc: 96.209% (7512/7808)\n",
            "80 391 Loss: 0.111 | Acc: 96.431% (9998/10368)\n",
            "100 391 Loss: 0.108 | Acc: 96.496% (12475/12928)\n",
            "120 391 Loss: 0.110 | Acc: 96.346% (14922/15488)\n",
            "140 391 Loss: 0.111 | Acc: 96.299% (17380/18048)\n",
            "160 391 Loss: 0.113 | Acc: 96.230% (19831/20608)\n",
            "180 391 Loss: 0.112 | Acc: 96.219% (22292/23168)\n",
            "200 391 Loss: 0.113 | Acc: 96.179% (24745/25728)\n",
            "220 391 Loss: 0.112 | Acc: 96.161% (27202/28288)\n",
            "240 391 Loss: 0.111 | Acc: 96.165% (29665/30848)\n",
            "260 391 Loss: 0.111 | Acc: 96.139% (32118/33408)\n",
            "280 391 Loss: 0.111 | Acc: 96.138% (34579/35968)\n",
            "300 391 Loss: 0.112 | Acc: 96.047% (37005/38528)\n",
            "320 391 Loss: 0.113 | Acc: 96.021% (39453/41088)\n",
            "340 391 Loss: 0.113 | Acc: 96.032% (41916/43648)\n",
            "360 391 Loss: 0.113 | Acc: 96.037% (44377/46208)\n",
            "380 391 Loss: 0.114 | Acc: 95.995% (46815/48768)\n",
            "99 100 Loss: 0.489 | Acc: 87.103% (8772/10000)\n",
            "\n",
            "Epoch: 38\n",
            "0 391 Loss: 0.149 | Acc: 94.531% (121/128)\n",
            "20 391 Loss: 0.103 | Acc: 95.759% (2574/2688)\n",
            "40 391 Loss: 0.101 | Acc: 96.284% (5053/5248)\n",
            "60 391 Loss: 0.103 | Acc: 96.311% (7520/7808)\n",
            "80 391 Loss: 0.102 | Acc: 96.373% (9992/10368)\n",
            "100 391 Loss: 0.102 | Acc: 96.411% (12464/12928)\n",
            "120 391 Loss: 0.102 | Acc: 96.423% (14934/15488)\n",
            "140 391 Loss: 0.103 | Acc: 96.432% (17404/18048)\n",
            "160 391 Loss: 0.104 | Acc: 96.390% (19864/20608)\n",
            "180 391 Loss: 0.105 | Acc: 96.284% (22307/23168)\n",
            "200 391 Loss: 0.105 | Acc: 96.292% (24774/25728)\n",
            "220 391 Loss: 0.106 | Acc: 96.235% (27223/28288)\n",
            "240 391 Loss: 0.106 | Acc: 96.220% (29682/30848)\n",
            "260 391 Loss: 0.106 | Acc: 96.222% (32146/33408)\n",
            "280 391 Loss: 0.107 | Acc: 96.233% (34613/35968)\n",
            "300 391 Loss: 0.106 | Acc: 96.231% (37076/38528)\n",
            "320 391 Loss: 0.108 | Acc: 96.172% (39515/41088)\n",
            "340 391 Loss: 0.109 | Acc: 96.144% (41965/43648)\n",
            "360 391 Loss: 0.109 | Acc: 96.109% (44410/46208)\n",
            "380 391 Loss: 0.110 | Acc: 96.081% (46857/48768)\n",
            "99 100 Loss: 0.528 | Acc: 86.656% (8700/10000)\n",
            "\n",
            "Epoch: 39\n",
            "0 391 Loss: 0.106 | Acc: 95.312% (122/128)\n",
            "20 391 Loss: 0.103 | Acc: 96.057% (2582/2688)\n",
            "40 391 Loss: 0.106 | Acc: 96.151% (5046/5248)\n",
            "60 391 Loss: 0.105 | Acc: 96.414% (7528/7808)\n",
            "80 391 Loss: 0.104 | Acc: 96.402% (9995/10368)\n",
            "100 391 Loss: 0.102 | Acc: 96.542% (12481/12928)\n",
            "120 391 Loss: 0.099 | Acc: 96.649% (14969/15488)\n",
            "140 391 Loss: 0.098 | Acc: 96.664% (17446/18048)\n",
            "160 391 Loss: 0.098 | Acc: 96.652% (19918/20608)\n",
            "180 391 Loss: 0.097 | Acc: 96.655% (22393/23168)\n",
            "200 391 Loss: 0.097 | Acc: 96.642% (24864/25728)\n",
            "220 391 Loss: 0.098 | Acc: 96.585% (27322/28288)\n",
            "240 391 Loss: 0.100 | Acc: 96.496% (29767/30848)\n",
            "260 391 Loss: 0.100 | Acc: 96.471% (32229/33408)\n",
            "280 391 Loss: 0.100 | Acc: 96.486% (34704/35968)\n",
            "300 391 Loss: 0.101 | Acc: 96.483% (37173/38528)\n",
            "320 391 Loss: 0.103 | Acc: 96.400% (39609/41088)\n",
            "340 391 Loss: 0.103 | Acc: 96.378% (42067/43648)\n",
            "360 391 Loss: 0.103 | Acc: 96.390% (44540/46208)\n",
            "380 391 Loss: 0.103 | Acc: 96.401% (47013/48768)\n",
            "99 100 Loss: 0.458 | Acc: 88.510% (8857/10000)\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "======= start test group: 2 =======\n",
            "\n",
            "Epoch: 0\n",
            "0 391 Loss: 4.496 | Acc: 22.656% (29/128)\n",
            "20 391 Loss: 3.303 | Acc: 31.362% (843/2688)\n",
            "40 391 Loss: 2.726 | Acc: 31.936% (1676/5248)\n",
            "60 391 Loss: 2.403 | Acc: 34.375% (2684/7808)\n",
            "80 391 Loss: 2.224 | Acc: 35.793% (3711/10368)\n",
            "100 391 Loss: 2.100 | Acc: 37.121% (4799/12928)\n",
            "120 391 Loss: 2.012 | Acc: 38.294% (5931/15488)\n",
            "140 391 Loss: 1.943 | Acc: 39.301% (7093/18048)\n",
            "160 391 Loss: 1.884 | Acc: 40.392% (8324/20608)\n",
            "180 391 Loss: 1.835 | Acc: 41.272% (9562/23168)\n",
            "200 391 Loss: 1.796 | Acc: 42.051% (10819/25728)\n",
            "220 391 Loss: 1.762 | Acc: 42.718% (12084/28288)\n",
            "240 391 Loss: 1.736 | Acc: 43.154% (13312/30848)\n",
            "260 391 Loss: 1.712 | Acc: 43.684% (14594/33408)\n",
            "280 391 Loss: 1.686 | Acc: 44.303% (15935/35968)\n",
            "300 391 Loss: 1.666 | Acc: 44.739% (17237/38528)\n",
            "320 391 Loss: 1.647 | Acc: 45.239% (18588/41088)\n",
            "340 391 Loss: 1.631 | Acc: 45.590% (19899/43648)\n",
            "360 391 Loss: 1.616 | Acc: 45.942% (21229/46208)\n",
            "380 391 Loss: 1.603 | Acc: 46.192% (22527/48768)\n",
            "99 100 Loss: 1.118 | Acc: 61.784% (6188/10000)\n",
            "\n",
            "Epoch: 1\n",
            "0 391 Loss: 1.465 | Acc: 53.906% (69/128)\n",
            "20 391 Loss: 1.345 | Acc: 51.897% (1395/2688)\n",
            "40 391 Loss: 1.323 | Acc: 53.316% (2798/5248)\n",
            "60 391 Loss: 1.319 | Acc: 53.624% (4187/7808)\n",
            "80 391 Loss: 1.321 | Acc: 53.482% (5545/10368)\n",
            "100 391 Loss: 1.317 | Acc: 53.651% (6936/12928)\n",
            "120 391 Loss: 1.317 | Acc: 53.648% (8309/15488)\n",
            "140 391 Loss: 1.312 | Acc: 53.740% (9699/18048)\n",
            "160 391 Loss: 1.301 | Acc: 54.178% (11165/20608)\n",
            "180 391 Loss: 1.295 | Acc: 54.299% (12580/23168)\n",
            "200 391 Loss: 1.295 | Acc: 54.217% (13949/25728)\n",
            "220 391 Loss: 1.293 | Acc: 54.221% (15338/28288)\n",
            "240 391 Loss: 1.292 | Acc: 54.243% (16733/30848)\n",
            "260 391 Loss: 1.289 | Acc: 54.331% (18151/33408)\n",
            "280 391 Loss: 1.284 | Acc: 54.599% (19638/35968)\n",
            "300 391 Loss: 1.279 | Acc: 54.859% (21136/38528)\n",
            "320 391 Loss: 1.276 | Acc: 54.955% (22580/41088)\n",
            "340 391 Loss: 1.273 | Acc: 55.098% (24049/43648)\n",
            "360 391 Loss: 1.268 | Acc: 55.216% (25514/46208)\n",
            "380 391 Loss: 1.265 | Acc: 55.338% (26987/48768)\n",
            "99 100 Loss: 1.144 | Acc: 62.984% (6307/10000)\n",
            "\n",
            "Epoch: 2\n",
            "0 391 Loss: 1.198 | Acc: 60.156% (77/128)\n",
            "20 391 Loss: 1.187 | Acc: 58.966% (1585/2688)\n",
            "40 391 Loss: 1.182 | Acc: 58.518% (3071/5248)\n",
            "60 391 Loss: 1.189 | Acc: 58.120% (4538/7808)\n",
            "80 391 Loss: 1.187 | Acc: 58.218% (6036/10368)\n",
            "100 391 Loss: 1.192 | Acc: 58.168% (7520/12928)\n",
            "120 391 Loss: 1.196 | Acc: 58.122% (9002/15488)\n",
            "140 391 Loss: 1.196 | Acc: 58.084% (10483/18048)\n",
            "160 391 Loss: 1.196 | Acc: 58.065% (11966/20608)\n",
            "180 391 Loss: 1.195 | Acc: 58.149% (13472/23168)\n",
            "200 391 Loss: 1.190 | Acc: 58.302% (15000/25728)\n",
            "220 391 Loss: 1.189 | Acc: 58.325% (16499/28288)\n",
            "240 391 Loss: 1.191 | Acc: 58.214% (17958/30848)\n",
            "260 391 Loss: 1.190 | Acc: 58.315% (19482/33408)\n",
            "280 391 Loss: 1.189 | Acc: 58.299% (20969/35968)\n",
            "300 391 Loss: 1.188 | Acc: 58.368% (22488/38528)\n",
            "320 391 Loss: 1.185 | Acc: 58.492% (24033/41088)\n",
            "340 391 Loss: 1.183 | Acc: 58.543% (25553/43648)\n",
            "360 391 Loss: 1.182 | Acc: 58.464% (27015/46208)\n",
            "380 391 Loss: 1.178 | Acc: 58.670% (28612/48768)\n",
            "99 100 Loss: 0.965 | Acc: 66.450% (6649/10000)\n",
            "\n",
            "Epoch: 3\n",
            "0 391 Loss: 1.074 | Acc: 56.250% (72/128)\n",
            "20 391 Loss: 1.129 | Acc: 60.007% (1613/2688)\n",
            "40 391 Loss: 1.124 | Acc: 60.385% (3169/5248)\n",
            "60 391 Loss: 1.132 | Acc: 60.169% (4698/7808)\n",
            "80 391 Loss: 1.134 | Acc: 60.320% (6254/10368)\n",
            "100 391 Loss: 1.132 | Acc: 60.187% (7781/12928)\n",
            "120 391 Loss: 1.125 | Acc: 60.440% (9361/15488)\n",
            "140 391 Loss: 1.124 | Acc: 60.616% (10940/18048)\n",
            "160 391 Loss: 1.122 | Acc: 60.787% (12527/20608)\n",
            "180 391 Loss: 1.118 | Acc: 60.981% (14128/23168)\n",
            "200 391 Loss: 1.118 | Acc: 60.965% (15685/25728)\n",
            "220 391 Loss: 1.120 | Acc: 60.839% (17210/28288)\n",
            "240 391 Loss: 1.118 | Acc: 60.886% (18782/30848)\n",
            "260 391 Loss: 1.115 | Acc: 61.006% (20381/33408)\n",
            "280 391 Loss: 1.113 | Acc: 61.035% (21953/35968)\n",
            "300 391 Loss: 1.115 | Acc: 60.935% (23477/38528)\n",
            "320 391 Loss: 1.113 | Acc: 61.049% (25084/41088)\n",
            "340 391 Loss: 1.113 | Acc: 61.054% (26649/43648)\n",
            "360 391 Loss: 1.112 | Acc: 61.013% (28193/46208)\n",
            "380 391 Loss: 1.112 | Acc: 61.061% (29778/48768)\n",
            "99 100 Loss: 0.950 | Acc: 67.120% (6696/10000)\n",
            "\n",
            "Epoch: 4\n",
            "0 391 Loss: 1.037 | Acc: 70.312% (90/128)\n",
            "20 391 Loss: 1.065 | Acc: 62.872% (1690/2688)\n",
            "40 391 Loss: 1.063 | Acc: 62.919% (3302/5248)\n",
            "60 391 Loss: 1.060 | Acc: 63.025% (4921/7808)\n",
            "80 391 Loss: 1.067 | Acc: 63.011% (6533/10368)\n",
            "100 391 Loss: 1.067 | Acc: 62.995% (8144/12928)\n",
            "120 391 Loss: 1.068 | Acc: 62.952% (9750/15488)\n",
            "140 391 Loss: 1.073 | Acc: 62.716% (11319/18048)\n",
            "160 391 Loss: 1.073 | Acc: 62.762% (12934/20608)\n",
            "180 391 Loss: 1.072 | Acc: 62.711% (14529/23168)\n",
            "200 391 Loss: 1.073 | Acc: 62.617% (16110/25728)\n",
            "220 391 Loss: 1.071 | Acc: 62.670% (17728/28288)\n",
            "240 391 Loss: 1.070 | Acc: 62.737% (19353/30848)\n",
            "260 391 Loss: 1.069 | Acc: 62.793% (20978/33408)\n",
            "280 391 Loss: 1.068 | Acc: 62.795% (22586/35968)\n",
            "300 391 Loss: 1.066 | Acc: 62.798% (24195/38528)\n",
            "320 391 Loss: 1.065 | Acc: 62.850% (25824/41088)\n",
            "340 391 Loss: 1.066 | Acc: 62.818% (27419/43648)\n",
            "360 391 Loss: 1.066 | Acc: 62.848% (29041/46208)\n",
            "380 391 Loss: 1.066 | Acc: 62.787% (30620/48768)\n",
            "99 100 Loss: 1.000 | Acc: 66.164% (6657/10000)\n",
            "\n",
            "Epoch: 5\n",
            "0 391 Loss: 1.050 | Acc: 60.156% (77/128)\n",
            "20 391 Loss: 1.033 | Acc: 63.616% (1710/2688)\n",
            "40 391 Loss: 1.059 | Acc: 62.633% (3287/5248)\n",
            "60 391 Loss: 1.052 | Acc: 62.897% (4911/7808)\n",
            "80 391 Loss: 1.057 | Acc: 63.050% (6537/10368)\n",
            "100 391 Loss: 1.050 | Acc: 63.343% (8189/12928)\n",
            "120 391 Loss: 1.047 | Acc: 63.572% (9846/15488)\n",
            "140 391 Loss: 1.042 | Acc: 63.664% (11490/18048)\n",
            "160 391 Loss: 1.041 | Acc: 63.621% (13111/20608)\n",
            "180 391 Loss: 1.039 | Acc: 63.674% (14752/23168)\n",
            "200 391 Loss: 1.038 | Acc: 63.635% (16372/25728)\n",
            "220 391 Loss: 1.038 | Acc: 63.575% (17984/28288)\n",
            "240 391 Loss: 1.039 | Acc: 63.531% (19598/30848)\n",
            "260 391 Loss: 1.039 | Acc: 63.503% (21215/33408)\n",
            "280 391 Loss: 1.038 | Acc: 63.565% (22863/35968)\n",
            "300 391 Loss: 1.038 | Acc: 63.549% (24484/38528)\n",
            "320 391 Loss: 1.038 | Acc: 63.603% (26133/41088)\n",
            "340 391 Loss: 1.035 | Acc: 63.753% (27827/43648)\n",
            "360 391 Loss: 1.032 | Acc: 63.820% (29490/46208)\n",
            "380 391 Loss: 1.032 | Acc: 63.812% (31120/48768)\n",
            "99 100 Loss: 1.046 | Acc: 65.062% (6538/10000)\n",
            "\n",
            "Epoch: 6\n",
            "0 391 Loss: 1.090 | Acc: 62.500% (80/128)\n",
            "20 391 Loss: 1.013 | Acc: 63.690% (1712/2688)\n",
            "40 391 Loss: 1.022 | Acc: 63.929% (3355/5248)\n",
            "60 391 Loss: 1.013 | Acc: 64.793% (5059/7808)\n",
            "80 391 Loss: 1.012 | Acc: 65.046% (6744/10368)\n",
            "100 391 Loss: 1.001 | Acc: 65.393% (8454/12928)\n",
            "120 391 Loss: 1.004 | Acc: 65.134% (10088/15488)\n",
            "140 391 Loss: 0.999 | Acc: 65.182% (11764/18048)\n",
            "160 391 Loss: 1.001 | Acc: 65.213% (13439/20608)\n",
            "180 391 Loss: 1.001 | Acc: 65.241% (15115/23168)\n",
            "200 391 Loss: 1.000 | Acc: 65.229% (16782/25728)\n",
            "220 391 Loss: 0.997 | Acc: 65.247% (18457/28288)\n",
            "240 391 Loss: 0.997 | Acc: 65.200% (20113/30848)\n",
            "260 391 Loss: 0.995 | Acc: 65.233% (21793/33408)\n",
            "280 391 Loss: 0.991 | Acc: 65.400% (23523/35968)\n",
            "300 391 Loss: 0.989 | Acc: 65.480% (25228/38528)\n",
            "320 391 Loss: 0.989 | Acc: 65.474% (26902/41088)\n",
            "340 391 Loss: 0.986 | Acc: 65.554% (28613/43648)\n",
            "360 391 Loss: 0.986 | Acc: 65.580% (30303/46208)\n",
            "380 391 Loss: 0.986 | Acc: 65.529% (31957/48768)\n",
            "99 100 Loss: 0.969 | Acc: 66.511% (6695/10000)\n",
            "\n",
            "Epoch: 7\n",
            "0 391 Loss: 0.977 | Acc: 66.406% (85/128)\n",
            "20 391 Loss: 0.912 | Acc: 68.527% (1842/2688)\n",
            "40 391 Loss: 0.920 | Acc: 68.216% (3580/5248)\n",
            "60 391 Loss: 0.925 | Acc: 67.918% (5303/7808)\n",
            "80 391 Loss: 0.930 | Acc: 67.660% (7015/10368)\n",
            "100 391 Loss: 0.935 | Acc: 67.458% (8721/12928)\n",
            "120 391 Loss: 0.942 | Acc: 67.091% (10391/15488)\n",
            "140 391 Loss: 0.943 | Acc: 67.143% (12118/18048)\n",
            "160 391 Loss: 0.947 | Acc: 67.037% (13815/20608)\n",
            "180 391 Loss: 0.946 | Acc: 67.075% (15540/23168)\n",
            "200 391 Loss: 0.950 | Acc: 66.822% (17192/25728)\n",
            "220 391 Loss: 0.953 | Acc: 66.696% (18867/28288)\n",
            "240 391 Loss: 0.952 | Acc: 66.701% (20576/30848)\n",
            "260 391 Loss: 0.953 | Acc: 66.598% (22249/33408)\n",
            "280 391 Loss: 0.957 | Acc: 66.426% (23892/35968)\n",
            "300 391 Loss: 0.959 | Acc: 66.427% (25593/38528)\n",
            "320 391 Loss: 0.959 | Acc: 66.341% (27258/41088)\n",
            "340 391 Loss: 0.959 | Acc: 66.395% (28980/43648)\n",
            "360 391 Loss: 0.959 | Acc: 66.395% (30680/46208)\n",
            "380 391 Loss: 0.958 | Acc: 66.458% (32410/48768)\n",
            "99 100 Loss: 0.943 | Acc: 67.857% (6809/10000)\n",
            "\n",
            "Epoch: 8\n",
            "0 391 Loss: 0.992 | Acc: 69.531% (89/128)\n",
            "20 391 Loss: 0.963 | Acc: 66.964% (1800/2688)\n",
            "40 391 Loss: 0.958 | Acc: 66.902% (3511/5248)\n",
            "60 391 Loss: 0.944 | Acc: 66.829% (5218/7808)\n",
            "80 391 Loss: 0.937 | Acc: 67.004% (6947/10368)\n",
            "100 391 Loss: 0.944 | Acc: 66.754% (8630/12928)\n",
            "120 391 Loss: 0.941 | Acc: 67.084% (10390/15488)\n",
            "140 391 Loss: 0.943 | Acc: 67.066% (12104/18048)\n",
            "160 391 Loss: 0.943 | Acc: 66.984% (13804/20608)\n",
            "180 391 Loss: 0.944 | Acc: 66.972% (15516/23168)\n",
            "200 391 Loss: 0.945 | Acc: 66.966% (17229/25728)\n",
            "220 391 Loss: 0.939 | Acc: 67.149% (18995/28288)\n",
            "240 391 Loss: 0.937 | Acc: 67.223% (20737/30848)\n",
            "260 391 Loss: 0.938 | Acc: 67.149% (22433/33408)\n",
            "280 391 Loss: 0.939 | Acc: 67.048% (24116/35968)\n",
            "300 391 Loss: 0.941 | Acc: 66.962% (25799/38528)\n",
            "320 391 Loss: 0.943 | Acc: 66.895% (27486/41088)\n",
            "340 391 Loss: 0.943 | Acc: 66.901% (29201/43648)\n",
            "360 391 Loss: 0.944 | Acc: 66.878% (30903/46208)\n",
            "380 391 Loss: 0.943 | Acc: 66.913% (32632/48768)\n",
            "99 100 Loss: 0.943 | Acc: 68.173% (6852/10000)\n",
            "\n",
            "Epoch: 9\n",
            "0 391 Loss: 0.867 | Acc: 67.188% (86/128)\n",
            "20 391 Loss: 0.927 | Acc: 67.336% (1810/2688)\n",
            "40 391 Loss: 0.938 | Acc: 66.787% (3505/5248)\n",
            "60 391 Loss: 0.937 | Acc: 67.136% (5242/7808)\n",
            "80 391 Loss: 0.924 | Acc: 67.583% (7007/10368)\n",
            "100 391 Loss: 0.924 | Acc: 67.520% (8729/12928)\n",
            "120 391 Loss: 0.927 | Acc: 67.485% (10452/15488)\n",
            "140 391 Loss: 0.927 | Acc: 67.581% (12197/18048)\n",
            "160 391 Loss: 0.922 | Acc: 67.770% (13966/20608)\n",
            "180 391 Loss: 0.915 | Acc: 67.891% (15729/23168)\n",
            "200 391 Loss: 0.912 | Acc: 68.043% (17506/25728)\n",
            "220 391 Loss: 0.909 | Acc: 68.135% (19274/28288)\n",
            "240 391 Loss: 0.911 | Acc: 68.154% (21024/30848)\n",
            "260 391 Loss: 0.910 | Acc: 68.169% (22774/33408)\n",
            "280 391 Loss: 0.910 | Acc: 68.127% (24504/35968)\n",
            "300 391 Loss: 0.910 | Acc: 68.158% (26260/38528)\n",
            "320 391 Loss: 0.910 | Acc: 68.159% (28005/41088)\n",
            "340 391 Loss: 0.912 | Acc: 68.092% (29721/43648)\n",
            "360 391 Loss: 0.910 | Acc: 68.166% (31498/46208)\n",
            "380 391 Loss: 0.911 | Acc: 68.149% (33235/48768)\n",
            "99 100 Loss: 0.849 | Acc: 71.085% (7137/10000)\n",
            "\n",
            "Epoch: 10\n",
            "0 391 Loss: 0.867 | Acc: 75.000% (96/128)\n",
            "20 391 Loss: 0.872 | Acc: 69.345% (1864/2688)\n",
            "40 391 Loss: 0.843 | Acc: 70.217% (3685/5248)\n",
            "60 391 Loss: 0.865 | Acc: 69.518% (5428/7808)\n",
            "80 391 Loss: 0.858 | Acc: 69.830% (7240/10368)\n",
            "100 391 Loss: 0.860 | Acc: 69.895% (9036/12928)\n",
            "120 391 Loss: 0.868 | Acc: 69.628% (10784/15488)\n",
            "140 391 Loss: 0.874 | Acc: 69.420% (12529/18048)\n",
            "160 391 Loss: 0.879 | Acc: 69.274% (14276/20608)\n",
            "180 391 Loss: 0.881 | Acc: 69.143% (16019/23168)\n",
            "200 391 Loss: 0.882 | Acc: 69.139% (17788/25728)\n",
            "220 391 Loss: 0.883 | Acc: 69.139% (19558/28288)\n",
            "240 391 Loss: 0.883 | Acc: 69.149% (21331/30848)\n",
            "260 391 Loss: 0.883 | Acc: 69.157% (23104/33408)\n",
            "280 391 Loss: 0.883 | Acc: 69.111% (24858/35968)\n",
            "300 391 Loss: 0.885 | Acc: 69.028% (26595/38528)\n",
            "320 391 Loss: 0.884 | Acc: 69.076% (28382/41088)\n",
            "340 391 Loss: 0.888 | Acc: 68.894% (30071/43648)\n",
            "360 391 Loss: 0.886 | Acc: 69.036% (31900/46208)\n",
            "380 391 Loss: 0.887 | Acc: 69.000% (33650/48768)\n",
            "99 100 Loss: 0.957 | Acc: 68.441% (6854/10000)\n",
            "\n",
            "Epoch: 11\n",
            "0 391 Loss: 0.854 | Acc: 66.406% (85/128)\n",
            "20 391 Loss: 0.885 | Acc: 69.234% (1861/2688)\n",
            "40 391 Loss: 0.882 | Acc: 69.436% (3644/5248)\n",
            "60 391 Loss: 0.871 | Acc: 69.557% (5431/7808)\n",
            "80 391 Loss: 0.863 | Acc: 70.149% (7273/10368)\n",
            "100 391 Loss: 0.864 | Acc: 70.096% (9062/12928)\n",
            "120 391 Loss: 0.856 | Acc: 70.241% (10879/15488)\n",
            "140 391 Loss: 0.855 | Acc: 70.257% (12680/18048)\n",
            "160 391 Loss: 0.858 | Acc: 70.152% (14457/20608)\n",
            "180 391 Loss: 0.855 | Acc: 70.174% (16258/23168)\n",
            "200 391 Loss: 0.860 | Acc: 69.904% (17985/25728)\n",
            "220 391 Loss: 0.866 | Acc: 69.747% (19730/28288)\n",
            "240 391 Loss: 0.864 | Acc: 69.813% (21536/30848)\n",
            "260 391 Loss: 0.864 | Acc: 69.822% (23326/33408)\n",
            "280 391 Loss: 0.867 | Acc: 69.709% (25073/35968)\n",
            "300 391 Loss: 0.868 | Acc: 69.643% (26832/38528)\n",
            "320 391 Loss: 0.871 | Acc: 69.585% (28591/41088)\n",
            "340 391 Loss: 0.872 | Acc: 69.559% (30361/43648)\n",
            "360 391 Loss: 0.871 | Acc: 69.546% (32136/46208)\n",
            "380 391 Loss: 0.873 | Acc: 69.523% (33905/48768)\n",
            "99 100 Loss: 0.929 | Acc: 68.249% (6837/10000)\n",
            "\n",
            "Epoch: 12\n",
            "0 391 Loss: 0.693 | Acc: 76.562% (98/128)\n",
            "20 391 Loss: 0.852 | Acc: 69.420% (1866/2688)\n",
            "40 391 Loss: 0.840 | Acc: 69.950% (3671/5248)\n",
            "60 391 Loss: 0.850 | Acc: 69.762% (5447/7808)\n",
            "80 391 Loss: 0.853 | Acc: 69.878% (7245/10368)\n",
            "100 391 Loss: 0.849 | Acc: 70.011% (9051/12928)\n",
            "120 391 Loss: 0.851 | Acc: 69.983% (10839/15488)\n",
            "140 391 Loss: 0.847 | Acc: 70.146% (12660/18048)\n",
            "160 391 Loss: 0.848 | Acc: 70.012% (14428/20608)\n",
            "180 391 Loss: 0.851 | Acc: 69.941% (16204/23168)\n",
            "200 391 Loss: 0.849 | Acc: 70.021% (18015/25728)\n",
            "220 391 Loss: 0.846 | Acc: 70.143% (19842/28288)\n",
            "240 391 Loss: 0.850 | Acc: 70.066% (21614/30848)\n",
            "260 391 Loss: 0.854 | Acc: 69.977% (23378/33408)\n",
            "280 391 Loss: 0.855 | Acc: 69.920% (25149/35968)\n",
            "300 391 Loss: 0.856 | Acc: 69.874% (26921/38528)\n",
            "320 391 Loss: 0.854 | Acc: 69.945% (28739/41088)\n",
            "340 391 Loss: 0.853 | Acc: 70.040% (30571/43648)\n",
            "360 391 Loss: 0.852 | Acc: 70.064% (32375/46208)\n",
            "380 391 Loss: 0.853 | Acc: 70.101% (34187/48768)\n",
            "99 100 Loss: 0.840 | Acc: 71.291% (7147/10000)\n",
            "\n",
            "Epoch: 13\n",
            "0 391 Loss: 0.769 | Acc: 75.781% (97/128)\n",
            "20 391 Loss: 0.815 | Acc: 71.689% (1927/2688)\n",
            "40 391 Loss: 0.824 | Acc: 71.056% (3729/5248)\n",
            "60 391 Loss: 0.826 | Acc: 70.850% (5532/7808)\n",
            "80 391 Loss: 0.825 | Acc: 70.978% (7359/10368)\n",
            "100 391 Loss: 0.827 | Acc: 70.823% (9156/12928)\n",
            "120 391 Loss: 0.832 | Acc: 70.823% (10969/15488)\n",
            "140 391 Loss: 0.837 | Acc: 70.645% (12750/18048)\n",
            "160 391 Loss: 0.838 | Acc: 70.555% (14540/20608)\n",
            "180 391 Loss: 0.841 | Acc: 70.399% (16310/23168)\n",
            "200 391 Loss: 0.840 | Acc: 70.379% (18107/25728)\n",
            "220 391 Loss: 0.841 | Acc: 70.309% (19889/28288)\n",
            "240 391 Loss: 0.839 | Acc: 70.494% (21746/30848)\n",
            "260 391 Loss: 0.837 | Acc: 70.570% (23576/33408)\n",
            "280 391 Loss: 0.836 | Acc: 70.607% (25396/35968)\n",
            "300 391 Loss: 0.836 | Acc: 70.621% (27209/38528)\n",
            "320 391 Loss: 0.837 | Acc: 70.622% (29017/41088)\n",
            "340 391 Loss: 0.837 | Acc: 70.668% (30845/43648)\n",
            "360 391 Loss: 0.837 | Acc: 70.654% (32648/46208)\n",
            "380 391 Loss: 0.837 | Acc: 70.673% (34466/48768)\n",
            "99 100 Loss: 0.916 | Acc: 69.041% (6941/10000)\n",
            "\n",
            "Epoch: 14\n",
            "0 391 Loss: 0.758 | Acc: 73.438% (94/128)\n",
            "20 391 Loss: 0.775 | Acc: 72.842% (1958/2688)\n",
            "40 391 Loss: 0.776 | Acc: 72.504% (3805/5248)\n",
            "60 391 Loss: 0.793 | Acc: 72.157% (5634/7808)\n",
            "80 391 Loss: 0.795 | Acc: 71.962% (7461/10368)\n",
            "100 391 Loss: 0.800 | Acc: 71.782% (9280/12928)\n",
            "120 391 Loss: 0.808 | Acc: 71.752% (11113/15488)\n",
            "140 391 Loss: 0.812 | Acc: 71.648% (12931/18048)\n",
            "160 391 Loss: 0.812 | Acc: 71.647% (14765/20608)\n",
            "180 391 Loss: 0.810 | Acc: 71.750% (16623/23168)\n",
            "200 391 Loss: 0.808 | Acc: 71.836% (18482/25728)\n",
            "220 391 Loss: 0.807 | Acc: 71.847% (20324/28288)\n",
            "240 391 Loss: 0.807 | Acc: 71.833% (22159/30848)\n",
            "260 391 Loss: 0.808 | Acc: 71.767% (23976/33408)\n",
            "280 391 Loss: 0.813 | Acc: 71.622% (25761/35968)\n",
            "300 391 Loss: 0.814 | Acc: 71.626% (27596/38528)\n",
            "320 391 Loss: 0.815 | Acc: 71.556% (29401/41088)\n",
            "340 391 Loss: 0.815 | Acc: 71.547% (31229/43648)\n",
            "360 391 Loss: 0.815 | Acc: 71.579% (33075/46208)\n",
            "380 391 Loss: 0.816 | Acc: 71.543% (34890/48768)\n",
            "99 100 Loss: 0.969 | Acc: 66.768% (6673/10000)\n",
            "\n",
            "Epoch: 15\n",
            "0 391 Loss: 0.848 | Acc: 67.969% (87/128)\n",
            "20 391 Loss: 0.813 | Acc: 72.061% (1937/2688)\n",
            "40 391 Loss: 0.812 | Acc: 72.123% (3785/5248)\n",
            "60 391 Loss: 0.808 | Acc: 71.926% (5616/7808)\n",
            "80 391 Loss: 0.802 | Acc: 72.174% (7483/10368)\n",
            "100 391 Loss: 0.804 | Acc: 72.215% (9336/12928)\n",
            "120 391 Loss: 0.801 | Acc: 72.450% (11221/15488)\n",
            "140 391 Loss: 0.801 | Acc: 72.484% (13082/18048)\n",
            "160 391 Loss: 0.802 | Acc: 72.414% (14923/20608)\n",
            "180 391 Loss: 0.793 | Acc: 72.803% (16867/23168)\n",
            "200 391 Loss: 0.792 | Acc: 72.796% (18729/25728)\n",
            "220 391 Loss: 0.792 | Acc: 72.738% (20576/28288)\n",
            "240 391 Loss: 0.793 | Acc: 72.682% (22421/30848)\n",
            "260 391 Loss: 0.793 | Acc: 72.731% (24298/33408)\n",
            "280 391 Loss: 0.794 | Acc: 72.690% (26145/35968)\n",
            "300 391 Loss: 0.796 | Acc: 72.602% (27972/38528)\n",
            "320 391 Loss: 0.798 | Acc: 72.510% (29793/41088)\n",
            "340 391 Loss: 0.796 | Acc: 72.526% (31656/43648)\n",
            "360 391 Loss: 0.798 | Acc: 72.444% (33475/46208)\n",
            "380 391 Loss: 0.798 | Acc: 72.429% (35322/48768)\n",
            "99 100 Loss: 0.872 | Acc: 69.736% (7013/10000)\n",
            "\n",
            "Epoch: 16\n",
            "0 391 Loss: 0.934 | Acc: 67.969% (87/128)\n",
            "20 391 Loss: 0.778 | Acc: 72.842% (1958/2688)\n",
            "40 391 Loss: 0.784 | Acc: 73.037% (3833/5248)\n",
            "60 391 Loss: 0.775 | Acc: 72.989% (5699/7808)\n",
            "80 391 Loss: 0.772 | Acc: 73.129% (7582/10368)\n",
            "100 391 Loss: 0.772 | Acc: 73.190% (9462/12928)\n",
            "120 391 Loss: 0.769 | Acc: 73.250% (11345/15488)\n",
            "140 391 Loss: 0.771 | Acc: 73.205% (13212/18048)\n",
            "160 391 Loss: 0.773 | Acc: 73.132% (15071/20608)\n",
            "180 391 Loss: 0.777 | Acc: 73.027% (16919/23168)\n",
            "200 391 Loss: 0.777 | Acc: 73.053% (18795/25728)\n",
            "220 391 Loss: 0.776 | Acc: 73.088% (20675/28288)\n",
            "240 391 Loss: 0.781 | Acc: 72.945% (22502/30848)\n",
            "260 391 Loss: 0.785 | Acc: 72.800% (24321/33408)\n",
            "280 391 Loss: 0.790 | Acc: 72.553% (26096/35968)\n",
            "300 391 Loss: 0.789 | Acc: 72.589% (27967/38528)\n",
            "320 391 Loss: 0.787 | Acc: 72.666% (29857/41088)\n",
            "340 391 Loss: 0.786 | Acc: 72.716% (31739/43648)\n",
            "360 391 Loss: 0.785 | Acc: 72.760% (33621/46208)\n",
            "380 391 Loss: 0.787 | Acc: 72.736% (35472/48768)\n",
            "99 100 Loss: 0.971 | Acc: 68.640% (6875/10000)\n",
            "\n",
            "Epoch: 17\n",
            "0 391 Loss: 0.608 | Acc: 80.469% (103/128)\n",
            "20 391 Loss: 0.731 | Acc: 74.777% (2010/2688)\n",
            "40 391 Loss: 0.756 | Acc: 73.819% (3874/5248)\n",
            "60 391 Loss: 0.763 | Acc: 73.386% (5730/7808)\n",
            "80 391 Loss: 0.766 | Acc: 73.524% (7623/10368)\n",
            "100 391 Loss: 0.768 | Acc: 73.244% (9469/12928)\n",
            "120 391 Loss: 0.770 | Acc: 73.044% (11313/15488)\n",
            "140 391 Loss: 0.774 | Acc: 72.922% (13161/18048)\n",
            "160 391 Loss: 0.778 | Acc: 72.816% (15006/20608)\n",
            "180 391 Loss: 0.776 | Acc: 72.941% (16899/23168)\n",
            "200 391 Loss: 0.780 | Acc: 72.827% (18737/25728)\n",
            "220 391 Loss: 0.779 | Acc: 72.794% (20592/28288)\n",
            "240 391 Loss: 0.781 | Acc: 72.692% (22424/30848)\n",
            "260 391 Loss: 0.779 | Acc: 72.776% (24313/33408)\n",
            "280 391 Loss: 0.780 | Acc: 72.795% (26183/35968)\n",
            "300 391 Loss: 0.778 | Acc: 72.752% (28030/38528)\n",
            "320 391 Loss: 0.776 | Acc: 72.849% (29932/41088)\n",
            "340 391 Loss: 0.775 | Acc: 72.856% (31800/43648)\n",
            "360 391 Loss: 0.774 | Acc: 72.855% (33665/46208)\n",
            "380 391 Loss: 0.774 | Acc: 72.847% (35526/48768)\n",
            "99 100 Loss: 0.824 | Acc: 71.842% (7198/10000)\n",
            "\n",
            "Epoch: 18\n",
            "0 391 Loss: 0.775 | Acc: 75.781% (97/128)\n",
            "20 391 Loss: 0.731 | Acc: 75.149% (2020/2688)\n",
            "40 391 Loss: 0.727 | Acc: 75.076% (3940/5248)\n",
            "60 391 Loss: 0.728 | Acc: 74.859% (5845/7808)\n",
            "80 391 Loss: 0.733 | Acc: 74.817% (7757/10368)\n",
            "100 391 Loss: 0.734 | Acc: 74.691% (9656/12928)\n",
            "120 391 Loss: 0.739 | Acc: 74.496% (11538/15488)\n",
            "140 391 Loss: 0.750 | Acc: 74.086% (13371/18048)\n",
            "160 391 Loss: 0.751 | Acc: 74.136% (15278/20608)\n",
            "180 391 Loss: 0.754 | Acc: 74.020% (17149/23168)\n",
            "200 391 Loss: 0.755 | Acc: 73.943% (19024/25728)\n",
            "220 391 Loss: 0.753 | Acc: 73.865% (20895/28288)\n",
            "240 391 Loss: 0.758 | Acc: 73.674% (22727/30848)\n",
            "260 391 Loss: 0.759 | Acc: 73.566% (24577/33408)\n",
            "280 391 Loss: 0.760 | Acc: 73.501% (26437/35968)\n",
            "300 391 Loss: 0.760 | Acc: 73.518% (28325/38528)\n",
            "320 391 Loss: 0.760 | Acc: 73.552% (30221/41088)\n",
            "340 391 Loss: 0.761 | Acc: 73.458% (32063/43648)\n",
            "360 391 Loss: 0.762 | Acc: 73.383% (33909/46208)\n",
            "380 391 Loss: 0.762 | Acc: 73.355% (35774/48768)\n",
            "99 100 Loss: 0.888 | Acc: 69.673% (6967/10000)\n",
            "\n",
            "Epoch: 19\n",
            "0 391 Loss: 0.833 | Acc: 72.656% (93/128)\n",
            "20 391 Loss: 0.744 | Acc: 74.591% (2005/2688)\n",
            "40 391 Loss: 0.721 | Acc: 75.324% (3953/5248)\n",
            "60 391 Loss: 0.729 | Acc: 74.667% (5830/7808)\n",
            "80 391 Loss: 0.723 | Acc: 74.672% (7742/10368)\n",
            "100 391 Loss: 0.732 | Acc: 74.281% (9603/12928)\n",
            "120 391 Loss: 0.738 | Acc: 74.167% (11487/15488)\n",
            "140 391 Loss: 0.738 | Acc: 74.147% (13382/18048)\n",
            "160 391 Loss: 0.736 | Acc: 74.316% (15315/20608)\n",
            "180 391 Loss: 0.741 | Acc: 74.137% (17176/23168)\n",
            "200 391 Loss: 0.743 | Acc: 74.052% (19052/25728)\n",
            "220 391 Loss: 0.747 | Acc: 73.865% (20895/28288)\n",
            "240 391 Loss: 0.748 | Acc: 73.859% (22784/30848)\n",
            "260 391 Loss: 0.748 | Acc: 73.884% (24683/33408)\n",
            "280 391 Loss: 0.751 | Acc: 73.810% (26548/35968)\n",
            "300 391 Loss: 0.751 | Acc: 73.829% (28445/38528)\n",
            "320 391 Loss: 0.748 | Acc: 73.851% (30344/41088)\n",
            "340 391 Loss: 0.749 | Acc: 73.777% (32202/43648)\n",
            "360 391 Loss: 0.747 | Acc: 73.836% (34118/46208)\n",
            "380 391 Loss: 0.749 | Acc: 73.800% (35991/48768)\n",
            "99 100 Loss: 0.835 | Acc: 71.645% (7153/10000)\n",
            "\n",
            "Epoch: 20\n",
            "0 391 Loss: 0.729 | Acc: 73.438% (94/128)\n",
            "20 391 Loss: 0.727 | Acc: 74.554% (2004/2688)\n",
            "40 391 Loss: 0.726 | Acc: 74.314% (3900/5248)\n",
            "60 391 Loss: 0.714 | Acc: 75.013% (5857/7808)\n",
            "80 391 Loss: 0.720 | Acc: 74.817% (7757/10368)\n",
            "100 391 Loss: 0.723 | Acc: 74.752% (9664/12928)\n",
            "120 391 Loss: 0.726 | Acc: 74.593% (11553/15488)\n",
            "140 391 Loss: 0.730 | Acc: 74.512% (13448/18048)\n",
            "160 391 Loss: 0.730 | Acc: 74.515% (15356/20608)\n",
            "180 391 Loss: 0.726 | Acc: 74.577% (17278/23168)\n",
            "200 391 Loss: 0.728 | Acc: 74.549% (19180/25728)\n",
            "220 391 Loss: 0.729 | Acc: 74.537% (21085/28288)\n",
            "240 391 Loss: 0.731 | Acc: 74.413% (22955/30848)\n",
            "260 391 Loss: 0.731 | Acc: 74.395% (24854/33408)\n",
            "280 391 Loss: 0.730 | Acc: 74.436% (26773/35968)\n",
            "300 391 Loss: 0.730 | Acc: 74.463% (28689/38528)\n",
            "320 391 Loss: 0.730 | Acc: 74.433% (30583/41088)\n",
            "340 391 Loss: 0.730 | Acc: 74.446% (32494/43648)\n",
            "360 391 Loss: 0.730 | Acc: 74.457% (34405/46208)\n",
            "380 391 Loss: 0.732 | Acc: 74.375% (36271/48768)\n",
            "99 100 Loss: 0.865 | Acc: 70.760% (7065/10000)\n",
            "\n",
            "Epoch: 21\n",
            "0 391 Loss: 0.789 | Acc: 72.656% (93/128)\n",
            "20 391 Loss: 0.743 | Acc: 74.144% (1993/2688)\n",
            "40 391 Loss: 0.739 | Acc: 74.047% (3886/5248)\n",
            "60 391 Loss: 0.738 | Acc: 74.091% (5785/7808)\n",
            "80 391 Loss: 0.733 | Acc: 74.479% (7722/10368)\n",
            "100 391 Loss: 0.728 | Acc: 74.729% (9661/12928)\n",
            "120 391 Loss: 0.730 | Acc: 74.626% (11558/15488)\n",
            "140 391 Loss: 0.728 | Acc: 74.656% (13474/18048)\n",
            "160 391 Loss: 0.727 | Acc: 74.714% (15397/20608)\n",
            "180 391 Loss: 0.723 | Acc: 74.806% (17331/23168)\n",
            "200 391 Loss: 0.721 | Acc: 74.903% (19271/25728)\n",
            "220 391 Loss: 0.721 | Acc: 74.954% (21203/28288)\n",
            "240 391 Loss: 0.721 | Acc: 74.942% (23118/30848)\n",
            "260 391 Loss: 0.723 | Acc: 74.856% (25008/33408)\n",
            "280 391 Loss: 0.723 | Acc: 74.842% (26919/35968)\n",
            "300 391 Loss: 0.723 | Acc: 74.811% (28823/38528)\n",
            "320 391 Loss: 0.722 | Acc: 74.832% (30747/41088)\n",
            "340 391 Loss: 0.721 | Acc: 74.851% (32671/43648)\n",
            "360 391 Loss: 0.723 | Acc: 74.825% (34575/46208)\n",
            "380 391 Loss: 0.724 | Acc: 74.795% (36476/48768)\n",
            "99 100 Loss: 0.887 | Acc: 69.602% (7007/10000)\n",
            "\n",
            "Epoch: 22\n",
            "0 391 Loss: 0.770 | Acc: 72.656% (93/128)\n",
            "20 391 Loss: 0.690 | Acc: 76.153% (2047/2688)\n",
            "40 391 Loss: 0.692 | Acc: 75.629% (3969/5248)\n",
            "60 391 Loss: 0.696 | Acc: 75.679% (5909/7808)\n",
            "80 391 Loss: 0.700 | Acc: 75.299% (7807/10368)\n",
            "100 391 Loss: 0.692 | Acc: 75.511% (9762/12928)\n",
            "120 391 Loss: 0.697 | Acc: 75.465% (11688/15488)\n",
            "140 391 Loss: 0.699 | Acc: 75.493% (13625/18048)\n",
            "160 391 Loss: 0.700 | Acc: 75.446% (15548/20608)\n",
            "180 391 Loss: 0.700 | Acc: 75.548% (17503/23168)\n",
            "200 391 Loss: 0.699 | Acc: 75.482% (19420/25728)\n",
            "220 391 Loss: 0.701 | Acc: 75.509% (21360/28288)\n",
            "240 391 Loss: 0.704 | Acc: 75.464% (23279/30848)\n",
            "260 391 Loss: 0.707 | Acc: 75.335% (25168/33408)\n",
            "280 391 Loss: 0.706 | Acc: 75.373% (27110/35968)\n",
            "300 391 Loss: 0.705 | Acc: 75.358% (29034/38528)\n",
            "320 391 Loss: 0.707 | Acc: 75.270% (30927/41088)\n",
            "340 391 Loss: 0.708 | Acc: 75.160% (32806/43648)\n",
            "360 391 Loss: 0.708 | Acc: 75.167% (34733/46208)\n",
            "380 391 Loss: 0.708 | Acc: 75.221% (36684/48768)\n",
            "99 100 Loss: 0.945 | Acc: 68.169% (6827/10000)\n",
            "\n",
            "Epoch: 23\n",
            "0 391 Loss: 0.631 | Acc: 80.469% (103/128)\n",
            "20 391 Loss: 0.690 | Acc: 75.409% (2027/2688)\n",
            "40 391 Loss: 0.676 | Acc: 75.896% (3983/5248)\n",
            "60 391 Loss: 0.688 | Acc: 75.871% (5924/7808)\n",
            "80 391 Loss: 0.690 | Acc: 75.646% (7843/10368)\n",
            "100 391 Loss: 0.689 | Acc: 75.897% (9812/12928)\n",
            "120 391 Loss: 0.692 | Acc: 75.826% (11744/15488)\n",
            "140 391 Loss: 0.694 | Acc: 75.731% (13668/18048)\n",
            "160 391 Loss: 0.695 | Acc: 75.670% (15594/20608)\n",
            "180 391 Loss: 0.695 | Acc: 75.704% (17539/23168)\n",
            "200 391 Loss: 0.693 | Acc: 75.762% (19492/25728)\n",
            "220 391 Loss: 0.694 | Acc: 75.689% (21411/28288)\n",
            "240 391 Loss: 0.697 | Acc: 75.635% (23332/30848)\n",
            "260 391 Loss: 0.697 | Acc: 75.662% (25277/33408)\n",
            "280 391 Loss: 0.697 | Acc: 75.664% (27215/35968)\n",
            "300 391 Loss: 0.699 | Acc: 75.592% (29124/38528)\n",
            "320 391 Loss: 0.698 | Acc: 75.577% (31053/41088)\n",
            "340 391 Loss: 0.698 | Acc: 75.596% (32996/43648)\n",
            "360 391 Loss: 0.699 | Acc: 75.576% (34922/46208)\n",
            "380 391 Loss: 0.700 | Acc: 75.566% (36852/48768)\n",
            "99 100 Loss: 0.830 | Acc: 72.233% (7241/10000)\n",
            "\n",
            "Epoch: 24\n",
            "0 391 Loss: 0.680 | Acc: 75.781% (97/128)\n",
            "20 391 Loss: 0.702 | Acc: 75.149% (2020/2688)\n",
            "40 391 Loss: 0.690 | Acc: 75.743% (3975/5248)\n",
            "60 391 Loss: 0.676 | Acc: 75.973% (5932/7808)\n",
            "80 391 Loss: 0.678 | Acc: 76.128% (7893/10368)\n",
            "100 391 Loss: 0.677 | Acc: 76.191% (9850/12928)\n",
            "120 391 Loss: 0.679 | Acc: 76.046% (11778/15488)\n",
            "140 391 Loss: 0.683 | Acc: 75.975% (13712/18048)\n",
            "160 391 Loss: 0.678 | Acc: 76.145% (15692/20608)\n",
            "180 391 Loss: 0.680 | Acc: 76.187% (17651/23168)\n",
            "200 391 Loss: 0.685 | Acc: 76.046% (19565/25728)\n",
            "220 391 Loss: 0.688 | Acc: 75.933% (21480/28288)\n",
            "240 391 Loss: 0.689 | Acc: 75.917% (23419/30848)\n",
            "260 391 Loss: 0.688 | Acc: 75.922% (25364/33408)\n",
            "280 391 Loss: 0.691 | Acc: 75.842% (27279/35968)\n",
            "300 391 Loss: 0.689 | Acc: 75.963% (29267/38528)\n",
            "320 391 Loss: 0.689 | Acc: 75.983% (31220/41088)\n",
            "340 391 Loss: 0.689 | Acc: 75.994% (33170/43648)\n",
            "360 391 Loss: 0.689 | Acc: 75.991% (35114/46208)\n",
            "380 391 Loss: 0.687 | Acc: 76.021% (37074/48768)\n",
            "99 100 Loss: 0.857 | Acc: 71.109% (7168/10000)\n",
            "\n",
            "Epoch: 25\n",
            "0 391 Loss: 0.762 | Acc: 75.000% (96/128)\n",
            "20 391 Loss: 0.667 | Acc: 76.414% (2054/2688)\n",
            "40 391 Loss: 0.675 | Acc: 76.124% (3995/5248)\n",
            "60 391 Loss: 0.654 | Acc: 76.883% (6003/7808)\n",
            "80 391 Loss: 0.656 | Acc: 76.833% (7966/10368)\n",
            "100 391 Loss: 0.661 | Acc: 76.640% (9908/12928)\n",
            "120 391 Loss: 0.663 | Acc: 76.646% (11871/15488)\n",
            "140 391 Loss: 0.666 | Acc: 76.540% (13814/18048)\n",
            "160 391 Loss: 0.666 | Acc: 76.548% (15775/20608)\n",
            "180 391 Loss: 0.671 | Acc: 76.424% (17706/23168)\n",
            "200 391 Loss: 0.673 | Acc: 76.329% (19638/25728)\n",
            "220 391 Loss: 0.677 | Acc: 76.213% (21559/28288)\n",
            "240 391 Loss: 0.677 | Acc: 76.161% (23494/30848)\n",
            "260 391 Loss: 0.677 | Acc: 76.158% (25443/33408)\n",
            "280 391 Loss: 0.678 | Acc: 76.140% (27386/35968)\n",
            "300 391 Loss: 0.678 | Acc: 76.152% (29340/38528)\n",
            "320 391 Loss: 0.678 | Acc: 76.212% (31314/41088)\n",
            "340 391 Loss: 0.675 | Acc: 76.310% (33308/43648)\n",
            "360 391 Loss: 0.675 | Acc: 76.340% (35275/46208)\n",
            "380 391 Loss: 0.676 | Acc: 76.265% (37193/48768)\n",
            "99 100 Loss: 0.851 | Acc: 70.734% (7130/10000)\n",
            "\n",
            "Epoch: 26\n",
            "0 391 Loss: 0.700 | Acc: 75.781% (97/128)\n",
            "20 391 Loss: 0.668 | Acc: 76.302% (2051/2688)\n",
            "40 391 Loss: 0.669 | Acc: 76.277% (4003/5248)\n",
            "60 391 Loss: 0.660 | Acc: 76.678% (5987/7808)\n",
            "80 391 Loss: 0.660 | Acc: 76.698% (7952/10368)\n",
            "100 391 Loss: 0.662 | Acc: 76.725% (9919/12928)\n",
            "120 391 Loss: 0.665 | Acc: 76.601% (11864/15488)\n",
            "140 391 Loss: 0.665 | Acc: 76.574% (13820/18048)\n",
            "160 391 Loss: 0.664 | Acc: 76.655% (15797/20608)\n",
            "180 391 Loss: 0.669 | Acc: 76.511% (17726/23168)\n",
            "200 391 Loss: 0.673 | Acc: 76.356% (19645/25728)\n",
            "220 391 Loss: 0.672 | Acc: 76.368% (21603/28288)\n",
            "240 391 Loss: 0.673 | Acc: 76.358% (23555/30848)\n",
            "260 391 Loss: 0.672 | Acc: 76.455% (25542/33408)\n",
            "280 391 Loss: 0.671 | Acc: 76.546% (27532/35968)\n",
            "300 391 Loss: 0.670 | Acc: 76.446% (29453/38528)\n",
            "320 391 Loss: 0.670 | Acc: 76.465% (31418/41088)\n",
            "340 391 Loss: 0.671 | Acc: 76.382% (33339/43648)\n",
            "360 391 Loss: 0.672 | Acc: 76.372% (35290/46208)\n",
            "380 391 Loss: 0.671 | Acc: 76.425% (37271/48768)\n",
            "99 100 Loss: 0.844 | Acc: 70.707% (7150/10000)\n",
            "\n",
            "Epoch: 27\n",
            "0 391 Loss: 0.545 | Acc: 79.688% (102/128)\n",
            "20 391 Loss: 0.645 | Acc: 77.083% (2072/2688)\n",
            "40 391 Loss: 0.640 | Acc: 77.229% (4053/5248)\n",
            "60 391 Loss: 0.641 | Acc: 77.472% (6049/7808)\n",
            "80 391 Loss: 0.656 | Acc: 76.948% (7978/10368)\n",
            "100 391 Loss: 0.651 | Acc: 77.166% (9976/12928)\n",
            "120 391 Loss: 0.649 | Acc: 77.150% (11949/15488)\n",
            "140 391 Loss: 0.648 | Acc: 77.288% (13949/18048)\n",
            "160 391 Loss: 0.653 | Acc: 77.164% (15902/20608)\n",
            "180 391 Loss: 0.655 | Acc: 77.106% (17864/23168)\n",
            "200 391 Loss: 0.653 | Acc: 77.181% (19857/25728)\n",
            "220 391 Loss: 0.651 | Acc: 77.259% (21855/28288)\n",
            "240 391 Loss: 0.651 | Acc: 77.195% (23813/30848)\n",
            "260 391 Loss: 0.655 | Acc: 77.065% (25746/33408)\n",
            "280 391 Loss: 0.655 | Acc: 77.066% (27719/35968)\n",
            "300 391 Loss: 0.656 | Acc: 77.025% (29676/38528)\n",
            "320 391 Loss: 0.657 | Acc: 76.991% (31634/41088)\n",
            "340 391 Loss: 0.657 | Acc: 77.000% (33609/43648)\n",
            "360 391 Loss: 0.657 | Acc: 77.054% (35605/46208)\n",
            "380 391 Loss: 0.657 | Acc: 77.026% (37564/48768)\n",
            "99 100 Loss: 0.896 | Acc: 69.565% (7020/10000)\n",
            "\n",
            "Epoch: 28\n",
            "0 391 Loss: 0.717 | Acc: 74.219% (95/128)\n",
            "20 391 Loss: 0.656 | Acc: 78.162% (2101/2688)\n",
            "40 391 Loss: 0.644 | Acc: 78.049% (4096/5248)\n",
            "60 391 Loss: 0.643 | Acc: 77.779% (6073/7808)\n",
            "80 391 Loss: 0.640 | Acc: 77.710% (8057/10368)\n",
            "100 391 Loss: 0.638 | Acc: 77.862% (10066/12928)\n",
            "120 391 Loss: 0.636 | Acc: 77.970% (12076/15488)\n",
            "140 391 Loss: 0.640 | Acc: 77.854% (14051/18048)\n",
            "160 391 Loss: 0.639 | Acc: 77.863% (16046/20608)\n",
            "180 391 Loss: 0.640 | Acc: 77.767% (18017/23168)\n",
            "200 391 Loss: 0.641 | Acc: 77.748% (20003/25728)\n",
            "220 391 Loss: 0.638 | Acc: 77.920% (22042/28288)\n",
            "240 391 Loss: 0.641 | Acc: 77.759% (23987/30848)\n",
            "260 391 Loss: 0.642 | Acc: 77.733% (25969/33408)\n",
            "280 391 Loss: 0.643 | Acc: 77.655% (27931/35968)\n",
            "300 391 Loss: 0.646 | Acc: 77.510% (29863/38528)\n",
            "320 391 Loss: 0.647 | Acc: 77.446% (31821/41088)\n",
            "340 391 Loss: 0.648 | Acc: 77.408% (33787/43648)\n",
            "360 391 Loss: 0.648 | Acc: 77.407% (35768/46208)\n",
            "380 391 Loss: 0.648 | Acc: 77.442% (37767/48768)\n",
            "99 100 Loss: 0.791 | Acc: 72.595% (7264/10000)\n",
            "\n",
            "Epoch: 29\n",
            "0 391 Loss: 0.531 | Acc: 79.688% (102/128)\n",
            "20 391 Loss: 0.601 | Acc: 78.571% (2112/2688)\n",
            "40 391 Loss: 0.618 | Acc: 78.182% (4103/5248)\n",
            "60 391 Loss: 0.634 | Acc: 77.741% (6070/7808)\n",
            "80 391 Loss: 0.634 | Acc: 77.826% (8069/10368)\n",
            "100 391 Loss: 0.637 | Acc: 77.839% (10063/12928)\n",
            "120 391 Loss: 0.633 | Acc: 77.996% (12080/15488)\n",
            "140 391 Loss: 0.635 | Acc: 77.892% (14058/18048)\n",
            "160 391 Loss: 0.636 | Acc: 77.848% (16043/20608)\n",
            "180 391 Loss: 0.638 | Acc: 77.758% (18015/23168)\n",
            "200 391 Loss: 0.639 | Acc: 77.674% (19984/25728)\n",
            "220 391 Loss: 0.638 | Acc: 77.644% (21964/28288)\n",
            "240 391 Loss: 0.640 | Acc: 77.593% (23936/30848)\n",
            "260 391 Loss: 0.644 | Acc: 77.487% (25887/33408)\n",
            "280 391 Loss: 0.643 | Acc: 77.519% (27882/35968)\n",
            "300 391 Loss: 0.641 | Acc: 77.551% (29879/38528)\n",
            "320 391 Loss: 0.642 | Acc: 77.480% (31835/41088)\n",
            "340 391 Loss: 0.641 | Acc: 77.559% (33853/43648)\n",
            "360 391 Loss: 0.641 | Acc: 77.532% (35826/46208)\n",
            "380 391 Loss: 0.641 | Acc: 77.563% (37826/48768)\n",
            "99 100 Loss: 0.839 | Acc: 71.511% (7185/10000)\n",
            "\n",
            "Epoch: 30\n",
            "0 391 Loss: 0.555 | Acc: 77.344% (99/128)\n",
            "20 391 Loss: 0.620 | Acc: 78.237% (2103/2688)\n",
            "40 391 Loss: 0.632 | Acc: 78.011% (4094/5248)\n",
            "60 391 Loss: 0.617 | Acc: 78.535% (6132/7808)\n",
            "80 391 Loss: 0.620 | Acc: 78.607% (8150/10368)\n",
            "100 391 Loss: 0.621 | Acc: 78.442% (10141/12928)\n",
            "120 391 Loss: 0.622 | Acc: 78.461% (12152/15488)\n",
            "140 391 Loss: 0.621 | Acc: 78.446% (14158/18048)\n",
            "160 391 Loss: 0.625 | Acc: 78.217% (16119/20608)\n",
            "180 391 Loss: 0.625 | Acc: 78.242% (18127/23168)\n",
            "200 391 Loss: 0.629 | Acc: 78.117% (20098/25728)\n",
            "220 391 Loss: 0.632 | Acc: 77.980% (22059/28288)\n",
            "240 391 Loss: 0.632 | Acc: 77.901% (24031/30848)\n",
            "260 391 Loss: 0.634 | Acc: 77.924% (26033/33408)\n",
            "280 391 Loss: 0.632 | Acc: 77.986% (28050/35968)\n",
            "300 391 Loss: 0.635 | Acc: 77.907% (30016/38528)\n",
            "320 391 Loss: 0.635 | Acc: 77.886% (32002/41088)\n",
            "340 391 Loss: 0.634 | Acc: 77.903% (34003/43648)\n",
            "360 391 Loss: 0.634 | Acc: 77.900% (35996/46208)\n",
            "380 391 Loss: 0.633 | Acc: 77.893% (37987/48768)\n",
            "99 100 Loss: 0.844 | Acc: 71.265% (7204/10000)\n",
            "\n",
            "Epoch: 31\n",
            "0 391 Loss: 0.527 | Acc: 80.469% (103/128)\n",
            "20 391 Loss: 0.622 | Acc: 78.237% (2103/2688)\n",
            "40 391 Loss: 0.614 | Acc: 78.449% (4117/5248)\n",
            "60 391 Loss: 0.614 | Acc: 78.509% (6130/7808)\n",
            "80 391 Loss: 0.615 | Acc: 78.328% (8121/10368)\n",
            "100 391 Loss: 0.615 | Acc: 78.311% (10124/12928)\n",
            "120 391 Loss: 0.616 | Acc: 78.364% (12137/15488)\n",
            "140 391 Loss: 0.614 | Acc: 78.596% (14185/18048)\n",
            "160 391 Loss: 0.614 | Acc: 78.664% (16211/20608)\n",
            "180 391 Loss: 0.617 | Acc: 78.574% (18204/23168)\n",
            "200 391 Loss: 0.619 | Acc: 78.514% (20200/25728)\n",
            "220 391 Loss: 0.620 | Acc: 78.563% (22224/28288)\n",
            "240 391 Loss: 0.620 | Acc: 78.410% (24188/30848)\n",
            "260 391 Loss: 0.621 | Acc: 78.358% (26178/33408)\n",
            "280 391 Loss: 0.622 | Acc: 78.275% (28154/35968)\n",
            "300 391 Loss: 0.624 | Acc: 78.255% (30150/38528)\n",
            "320 391 Loss: 0.623 | Acc: 78.278% (32163/41088)\n",
            "340 391 Loss: 0.625 | Acc: 78.267% (34162/43648)\n",
            "360 391 Loss: 0.627 | Acc: 78.188% (36129/46208)\n",
            "380 391 Loss: 0.628 | Acc: 78.203% (38138/48768)\n",
            "99 100 Loss: 0.791 | Acc: 72.858% (7313/10000)\n",
            "\n",
            "Epoch: 32\n",
            "0 391 Loss: 0.553 | Acc: 80.469% (103/128)\n",
            "20 391 Loss: 0.582 | Acc: 79.464% (2136/2688)\n",
            "40 391 Loss: 0.591 | Acc: 79.268% (4160/5248)\n",
            "60 391 Loss: 0.587 | Acc: 79.457% (6204/7808)\n",
            "80 391 Loss: 0.595 | Acc: 78.858% (8176/10368)\n",
            "100 391 Loss: 0.600 | Acc: 78.690% (10173/12928)\n",
            "120 391 Loss: 0.606 | Acc: 78.525% (12162/15488)\n",
            "140 391 Loss: 0.608 | Acc: 78.502% (14168/18048)\n",
            "160 391 Loss: 0.608 | Acc: 78.537% (16185/20608)\n",
            "180 391 Loss: 0.609 | Acc: 78.496% (18186/23168)\n",
            "200 391 Loss: 0.608 | Acc: 78.564% (20213/25728)\n",
            "220 391 Loss: 0.608 | Acc: 78.627% (22242/28288)\n",
            "240 391 Loss: 0.609 | Acc: 78.579% (24240/30848)\n",
            "260 391 Loss: 0.607 | Acc: 78.637% (26271/33408)\n",
            "280 391 Loss: 0.608 | Acc: 78.592% (28268/35968)\n",
            "300 391 Loss: 0.610 | Acc: 78.488% (30240/38528)\n",
            "320 391 Loss: 0.609 | Acc: 78.553% (32276/41088)\n",
            "340 391 Loss: 0.610 | Acc: 78.521% (34273/43648)\n",
            "360 391 Loss: 0.611 | Acc: 78.541% (36292/46208)\n",
            "380 391 Loss: 0.611 | Acc: 78.535% (38300/48768)\n",
            "99 100 Loss: 0.826 | Acc: 71.772% (7225/10000)\n",
            "\n",
            "Epoch: 33\n",
            "0 391 Loss: 0.600 | Acc: 82.812% (106/128)\n",
            "20 391 Loss: 0.568 | Acc: 80.841% (2173/2688)\n",
            "40 391 Loss: 0.579 | Acc: 80.107% (4204/5248)\n",
            "60 391 Loss: 0.584 | Acc: 79.918% (6240/7808)\n",
            "80 391 Loss: 0.586 | Acc: 79.716% (8265/10368)\n",
            "100 391 Loss: 0.585 | Acc: 79.610% (10292/12928)\n",
            "120 391 Loss: 0.585 | Acc: 79.526% (12317/15488)\n",
            "140 391 Loss: 0.588 | Acc: 79.444% (14338/18048)\n",
            "160 391 Loss: 0.588 | Acc: 79.387% (16360/20608)\n",
            "180 391 Loss: 0.589 | Acc: 79.329% (18379/23168)\n",
            "200 391 Loss: 0.593 | Acc: 79.194% (20375/25728)\n",
            "220 391 Loss: 0.593 | Acc: 79.239% (22415/28288)\n",
            "240 391 Loss: 0.595 | Acc: 79.091% (24398/30848)\n",
            "260 391 Loss: 0.595 | Acc: 79.065% (26414/33408)\n",
            "280 391 Loss: 0.597 | Acc: 79.034% (28427/35968)\n",
            "300 391 Loss: 0.596 | Acc: 79.046% (30455/38528)\n",
            "320 391 Loss: 0.598 | Acc: 79.013% (32465/41088)\n",
            "340 391 Loss: 0.597 | Acc: 78.991% (34478/43648)\n",
            "360 391 Loss: 0.598 | Acc: 78.898% (36457/46208)\n",
            "380 391 Loss: 0.599 | Acc: 78.855% (38456/48768)\n",
            "99 100 Loss: 0.871 | Acc: 71.536% (7175/10000)\n",
            "\n",
            "Epoch: 34\n",
            "0 391 Loss: 0.594 | Acc: 78.906% (101/128)\n",
            "20 391 Loss: 0.552 | Acc: 80.134% (2154/2688)\n",
            "40 391 Loss: 0.550 | Acc: 80.088% (4203/5248)\n",
            "60 391 Loss: 0.570 | Acc: 79.406% (6200/7808)\n",
            "80 391 Loss: 0.570 | Acc: 79.427% (8235/10368)\n",
            "100 391 Loss: 0.573 | Acc: 79.533% (10282/12928)\n",
            "120 391 Loss: 0.572 | Acc: 79.616% (12331/15488)\n",
            "140 391 Loss: 0.574 | Acc: 79.593% (14365/18048)\n",
            "160 391 Loss: 0.581 | Acc: 79.425% (16368/20608)\n",
            "180 391 Loss: 0.581 | Acc: 79.424% (18401/23168)\n",
            "200 391 Loss: 0.582 | Acc: 79.404% (20429/25728)\n",
            "220 391 Loss: 0.585 | Acc: 79.387% (22457/28288)\n",
            "240 391 Loss: 0.588 | Acc: 79.328% (24471/30848)\n",
            "260 391 Loss: 0.590 | Acc: 79.244% (26474/33408)\n",
            "280 391 Loss: 0.594 | Acc: 79.159% (28472/35968)\n",
            "300 391 Loss: 0.596 | Acc: 79.059% (30460/38528)\n",
            "320 391 Loss: 0.595 | Acc: 79.091% (32497/41088)\n",
            "340 391 Loss: 0.595 | Acc: 79.117% (34533/43648)\n",
            "360 391 Loss: 0.598 | Acc: 78.988% (36499/46208)\n",
            "380 391 Loss: 0.599 | Acc: 78.958% (38506/48768)\n",
            "99 100 Loss: 0.803 | Acc: 72.762% (7301/10000)\n",
            "\n",
            "Epoch: 35\n",
            "0 391 Loss: 0.570 | Acc: 79.688% (102/128)\n",
            "20 391 Loss: 0.553 | Acc: 79.874% (2147/2688)\n",
            "40 391 Loss: 0.557 | Acc: 79.783% (4187/5248)\n",
            "60 391 Loss: 0.574 | Acc: 79.572% (6213/7808)\n",
            "80 391 Loss: 0.578 | Acc: 79.466% (8239/10368)\n",
            "100 391 Loss: 0.580 | Acc: 79.316% (10254/12928)\n",
            "120 391 Loss: 0.582 | Acc: 79.487% (12311/15488)\n",
            "140 391 Loss: 0.583 | Acc: 79.538% (14355/18048)\n",
            "160 391 Loss: 0.579 | Acc: 79.702% (16425/20608)\n",
            "180 391 Loss: 0.578 | Acc: 79.731% (18472/23168)\n",
            "200 391 Loss: 0.582 | Acc: 79.555% (20468/25728)\n",
            "220 391 Loss: 0.583 | Acc: 79.518% (22494/28288)\n",
            "240 391 Loss: 0.585 | Acc: 79.509% (24527/30848)\n",
            "260 391 Loss: 0.585 | Acc: 79.517% (26565/33408)\n",
            "280 391 Loss: 0.584 | Acc: 79.518% (28601/35968)\n",
            "300 391 Loss: 0.585 | Acc: 79.553% (30650/38528)\n",
            "320 391 Loss: 0.586 | Acc: 79.505% (32667/41088)\n",
            "340 391 Loss: 0.587 | Acc: 79.495% (34698/43648)\n",
            "360 391 Loss: 0.586 | Acc: 79.530% (36749/46208)\n",
            "380 391 Loss: 0.587 | Acc: 79.487% (38764/48768)\n",
            "99 100 Loss: 0.809 | Acc: 73.352% (7284/10000)\n",
            "\n",
            "Epoch: 36\n",
            "0 391 Loss: 0.591 | Acc: 78.125% (100/128)\n",
            "20 391 Loss: 0.566 | Acc: 80.022% (2151/2688)\n",
            "40 391 Loss: 0.578 | Acc: 79.573% (4176/5248)\n",
            "60 391 Loss: 0.574 | Acc: 80.059% (6251/7808)\n",
            "80 391 Loss: 0.566 | Acc: 80.218% (8317/10368)\n",
            "100 391 Loss: 0.573 | Acc: 79.881% (10327/12928)\n",
            "120 391 Loss: 0.574 | Acc: 79.765% (12354/15488)\n",
            "140 391 Loss: 0.575 | Acc: 79.588% (14364/18048)\n",
            "160 391 Loss: 0.575 | Acc: 79.634% (16411/20608)\n",
            "180 391 Loss: 0.576 | Acc: 79.580% (18437/23168)\n",
            "200 391 Loss: 0.573 | Acc: 79.808% (20533/25728)\n",
            "220 391 Loss: 0.571 | Acc: 79.804% (22575/28288)\n",
            "240 391 Loss: 0.573 | Acc: 79.733% (24596/30848)\n",
            "260 391 Loss: 0.573 | Acc: 79.735% (26638/33408)\n",
            "280 391 Loss: 0.577 | Acc: 79.615% (28636/35968)\n",
            "300 391 Loss: 0.579 | Acc: 79.573% (30658/38528)\n",
            "320 391 Loss: 0.580 | Acc: 79.554% (32687/41088)\n",
            "340 391 Loss: 0.582 | Acc: 79.486% (34694/43648)\n",
            "360 391 Loss: 0.583 | Acc: 79.473% (36723/46208)\n",
            "380 391 Loss: 0.584 | Acc: 79.470% (38756/48768)\n",
            "99 100 Loss: 0.802 | Acc: 72.307% (7267/10000)\n",
            "\n",
            "Epoch: 37\n",
            "0 391 Loss: 0.390 | Acc: 87.500% (112/128)\n",
            "20 391 Loss: 0.542 | Acc: 79.650% (2141/2688)\n",
            "40 391 Loss: 0.556 | Acc: 79.821% (4189/5248)\n",
            "60 391 Loss: 0.555 | Acc: 80.187% (6261/7808)\n",
            "80 391 Loss: 0.555 | Acc: 80.334% (8329/10368)\n",
            "100 391 Loss: 0.559 | Acc: 80.136% (10360/12928)\n",
            "120 391 Loss: 0.564 | Acc: 79.978% (12387/15488)\n",
            "140 391 Loss: 0.568 | Acc: 79.843% (14410/18048)\n",
            "160 391 Loss: 0.566 | Acc: 79.833% (16452/20608)\n",
            "180 391 Loss: 0.566 | Acc: 79.830% (18495/23168)\n",
            "200 391 Loss: 0.568 | Acc: 79.835% (20540/25728)\n",
            "220 391 Loss: 0.568 | Acc: 79.910% (22605/28288)\n",
            "240 391 Loss: 0.571 | Acc: 79.856% (24634/30848)\n",
            "260 391 Loss: 0.573 | Acc: 79.762% (26647/33408)\n",
            "280 391 Loss: 0.572 | Acc: 79.804% (28704/35968)\n",
            "300 391 Loss: 0.572 | Acc: 79.817% (30752/38528)\n",
            "320 391 Loss: 0.572 | Acc: 79.812% (32793/41088)\n",
            "340 391 Loss: 0.571 | Acc: 79.864% (34859/43648)\n",
            "360 391 Loss: 0.573 | Acc: 79.802% (36875/46208)\n",
            "380 391 Loss: 0.573 | Acc: 79.794% (38914/48768)\n",
            "99 100 Loss: 0.811 | Acc: 72.359% (7277/10000)\n",
            "\n",
            "Epoch: 38\n",
            "0 391 Loss: 0.518 | Acc: 82.031% (105/128)\n",
            "20 391 Loss: 0.521 | Acc: 81.436% (2189/2688)\n",
            "40 391 Loss: 0.536 | Acc: 80.869% (4244/5248)\n",
            "60 391 Loss: 0.533 | Acc: 81.007% (6325/7808)\n",
            "80 391 Loss: 0.538 | Acc: 80.758% (8373/10368)\n",
            "100 391 Loss: 0.544 | Acc: 80.623% (10423/12928)\n",
            "120 391 Loss: 0.548 | Acc: 80.527% (12472/15488)\n",
            "140 391 Loss: 0.552 | Acc: 80.413% (14513/18048)\n",
            "160 391 Loss: 0.554 | Acc: 80.289% (16546/20608)\n",
            "180 391 Loss: 0.557 | Acc: 80.201% (18581/23168)\n",
            "200 391 Loss: 0.558 | Acc: 80.201% (20634/25728)\n",
            "220 391 Loss: 0.557 | Acc: 80.267% (22706/28288)\n",
            "240 391 Loss: 0.558 | Acc: 80.255% (24757/30848)\n",
            "260 391 Loss: 0.560 | Acc: 80.217% (26799/33408)\n",
            "280 391 Loss: 0.559 | Acc: 80.269% (28871/35968)\n",
            "300 391 Loss: 0.560 | Acc: 80.313% (30943/38528)\n",
            "320 391 Loss: 0.562 | Acc: 80.235% (32967/41088)\n",
            "340 391 Loss: 0.563 | Acc: 80.272% (35037/43648)\n",
            "360 391 Loss: 0.561 | Acc: 80.311% (37110/46208)\n",
            "380 391 Loss: 0.563 | Acc: 80.255% (39139/48768)\n",
            "99 100 Loss: 0.839 | Acc: 71.558% (7121/10000)\n",
            "\n",
            "Epoch: 39\n",
            "0 391 Loss: 0.676 | Acc: 78.906% (101/128)\n",
            "20 391 Loss: 0.554 | Acc: 81.176% (2182/2688)\n",
            "40 391 Loss: 0.544 | Acc: 81.479% (4276/5248)\n",
            "60 391 Loss: 0.535 | Acc: 81.711% (6380/7808)\n",
            "80 391 Loss: 0.534 | Acc: 81.665% (8467/10368)\n",
            "100 391 Loss: 0.538 | Acc: 81.621% (10552/12928)\n",
            "120 391 Loss: 0.542 | Acc: 81.399% (12607/15488)\n",
            "140 391 Loss: 0.544 | Acc: 81.211% (14657/18048)\n",
            "160 391 Loss: 0.545 | Acc: 81.124% (16718/20608)\n",
            "180 391 Loss: 0.545 | Acc: 81.095% (18788/23168)\n",
            "200 391 Loss: 0.548 | Acc: 80.931% (20822/25728)\n",
            "220 391 Loss: 0.547 | Acc: 80.978% (22907/28288)\n",
            "240 391 Loss: 0.549 | Acc: 80.877% (24949/30848)\n",
            "260 391 Loss: 0.553 | Acc: 80.708% (26963/33408)\n",
            "280 391 Loss: 0.554 | Acc: 80.686% (29021/35968)\n",
            "300 391 Loss: 0.554 | Acc: 80.682% (31085/38528)\n",
            "320 391 Loss: 0.554 | Acc: 80.666% (33144/41088)\n",
            "340 391 Loss: 0.556 | Acc: 80.613% (35186/43648)\n",
            "360 391 Loss: 0.557 | Acc: 80.501% (37198/46208)\n",
            "380 391 Loss: 0.557 | Acc: 80.559% (39287/48768)\n",
            "99 100 Loss: 0.785 | Acc: 74.397% (7438/10000)\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "======= start test group: 3 =======\n",
            "\n",
            "Epoch: 0\n",
            "0 391 Loss: 3.548 | Acc: 10.156% (13/128)\n",
            "20 391 Loss: 3.163 | Acc: 9.561% (257/2688)\n",
            "40 391 Loss: 2.857 | Acc: 9.642% (506/5248)\n",
            "60 391 Loss: 2.708 | Acc: 9.849% (769/7808)\n",
            "80 391 Loss: 2.636 | Acc: 9.635% (999/10368)\n",
            "100 391 Loss: 2.581 | Acc: 9.924% (1283/12928)\n",
            "120 391 Loss: 2.545 | Acc: 10.079% (1561/15488)\n",
            "140 391 Loss: 2.518 | Acc: 9.951% (1796/18048)\n",
            "160 391 Loss: 2.499 | Acc: 9.991% (2059/20608)\n",
            "180 391 Loss: 2.481 | Acc: 9.919% (2298/23168)\n",
            "200 391 Loss: 2.467 | Acc: 9.985% (2569/25728)\n",
            "220 391 Loss: 2.454 | Acc: 10.082% (2852/28288)\n",
            "240 391 Loss: 2.444 | Acc: 10.114% (3120/30848)\n",
            "260 391 Loss: 2.436 | Acc: 10.081% (3368/33408)\n",
            "280 391 Loss: 2.429 | Acc: 10.123% (3641/35968)\n",
            "300 391 Loss: 2.423 | Acc: 10.076% (3882/38528)\n",
            "320 391 Loss: 2.417 | Acc: 10.105% (4152/41088)\n",
            "340 391 Loss: 2.412 | Acc: 10.053% (4388/43648)\n",
            "360 391 Loss: 2.408 | Acc: 10.100% (4667/46208)\n",
            "380 391 Loss: 2.404 | Acc: 10.115% (4933/48768)\n",
            "99 100 Loss: 5.286 | Acc: 9.879% (1000/10000)\n",
            "\n",
            "Epoch: 1\n",
            "0 391 Loss: 2.328 | Acc: 4.688% (6/128)\n",
            "20 391 Loss: 2.326 | Acc: 10.454% (281/2688)\n",
            "40 391 Loss: 2.327 | Acc: 11.109% (583/5248)\n",
            "60 391 Loss: 2.330 | Acc: 10.758% (840/7808)\n",
            "80 391 Loss: 2.329 | Acc: 10.696% (1109/10368)\n",
            "100 391 Loss: 2.327 | Acc: 10.891% (1408/12928)\n",
            "120 391 Loss: 2.328 | Acc: 10.692% (1656/15488)\n",
            "140 391 Loss: 2.329 | Acc: 10.516% (1898/18048)\n",
            "160 391 Loss: 2.329 | Acc: 10.530% (2170/20608)\n",
            "180 391 Loss: 2.330 | Acc: 10.445% (2420/23168)\n",
            "200 391 Loss: 2.329 | Acc: 10.506% (2703/25728)\n",
            "220 391 Loss: 2.329 | Acc: 10.489% (2967/28288)\n",
            "240 391 Loss: 2.329 | Acc: 10.383% (3203/30848)\n",
            "260 391 Loss: 2.330 | Acc: 10.369% (3464/33408)\n",
            "280 391 Loss: 2.329 | Acc: 10.379% (3733/35968)\n",
            "300 391 Loss: 2.329 | Acc: 10.320% (3976/38528)\n",
            "320 391 Loss: 2.329 | Acc: 10.356% (4255/41088)\n",
            "340 391 Loss: 2.328 | Acc: 10.314% (4502/43648)\n",
            "360 391 Loss: 2.329 | Acc: 10.254% (4738/46208)\n",
            "380 391 Loss: 2.329 | Acc: 10.292% (5019/48768)\n",
            "99 100 Loss: 4.126 | Acc: 9.879% (1000/10000)\n",
            "\n",
            "Epoch: 2\n",
            "0 391 Loss: 2.307 | Acc: 12.500% (16/128)\n",
            "20 391 Loss: 2.324 | Acc: 10.342% (278/2688)\n",
            "40 391 Loss: 2.325 | Acc: 10.709% (562/5248)\n",
            "60 391 Loss: 2.325 | Acc: 10.681% (834/7808)\n",
            "80 391 Loss: 2.326 | Acc: 10.774% (1117/10368)\n",
            "100 391 Loss: 2.326 | Acc: 10.450% (1351/12928)\n",
            "120 391 Loss: 2.326 | Acc: 10.434% (1616/15488)\n",
            "140 391 Loss: 2.325 | Acc: 10.489% (1893/18048)\n",
            "160 391 Loss: 2.324 | Acc: 10.413% (2146/20608)\n",
            "180 391 Loss: 2.324 | Acc: 10.350% (2398/23168)\n",
            "200 391 Loss: 2.325 | Acc: 10.296% (2649/25728)\n",
            "220 391 Loss: 2.325 | Acc: 10.259% (2902/28288)\n",
            "240 391 Loss: 2.325 | Acc: 10.351% (3193/30848)\n",
            "260 391 Loss: 2.325 | Acc: 10.357% (3460/33408)\n",
            "280 391 Loss: 2.324 | Acc: 10.345% (3721/35968)\n",
            "300 391 Loss: 2.324 | Acc: 10.328% (3979/38528)\n",
            "320 391 Loss: 2.324 | Acc: 10.305% (4234/41088)\n",
            "340 391 Loss: 2.324 | Acc: 10.298% (4495/43648)\n",
            "360 391 Loss: 2.323 | Acc: 10.323% (4770/46208)\n",
            "380 391 Loss: 2.323 | Acc: 10.335% (5040/48768)\n",
            "99 100 Loss: 3.720 | Acc: 9.879% (1000/10000)\n",
            "\n",
            "Epoch: 3\n",
            "0 391 Loss: 2.329 | Acc: 10.938% (14/128)\n",
            "20 391 Loss: 2.317 | Acc: 11.533% (310/2688)\n",
            "40 391 Loss: 2.317 | Acc: 10.842% (569/5248)\n",
            "60 391 Loss: 2.317 | Acc: 10.361% (809/7808)\n",
            "80 391 Loss: 2.318 | Acc: 9.896% (1026/10368)\n",
            "100 391 Loss: 2.318 | Acc: 10.133% (1310/12928)\n",
            "120 391 Loss: 2.317 | Acc: 10.221% (1583/15488)\n",
            "140 391 Loss: 2.318 | Acc: 10.217% (1844/18048)\n",
            "160 391 Loss: 2.318 | Acc: 10.229% (2108/20608)\n",
            "180 391 Loss: 2.318 | Acc: 10.290% (2384/23168)\n",
            "200 391 Loss: 2.318 | Acc: 10.296% (2649/25728)\n",
            "220 391 Loss: 2.318 | Acc: 10.206% (2887/28288)\n",
            "240 391 Loss: 2.317 | Acc: 10.260% (3165/30848)\n",
            "260 391 Loss: 2.318 | Acc: 10.285% (3436/33408)\n",
            "280 391 Loss: 2.318 | Acc: 10.281% (3698/35968)\n",
            "300 391 Loss: 2.318 | Acc: 10.302% (3969/38528)\n",
            "320 391 Loss: 2.318 | Acc: 10.346% (4251/41088)\n",
            "340 391 Loss: 2.318 | Acc: 10.333% (4510/43648)\n",
            "360 391 Loss: 2.317 | Acc: 10.364% (4789/46208)\n",
            "380 391 Loss: 2.318 | Acc: 10.314% (5030/48768)\n",
            "99 100 Loss: 3.983 | Acc: 9.879% (1000/10000)\n",
            "\n",
            "Epoch: 4\n",
            "0 391 Loss: 2.333 | Acc: 5.469% (7/128)\n",
            "20 391 Loss: 2.313 | Acc: 10.342% (278/2688)\n",
            "40 391 Loss: 2.311 | Acc: 10.728% (563/5248)\n",
            "60 391 Loss: 2.314 | Acc: 10.899% (851/7808)\n",
            "80 391 Loss: 2.315 | Acc: 10.600% (1099/10368)\n",
            "100 391 Loss: 2.315 | Acc: 10.450% (1351/12928)\n",
            "120 391 Loss: 2.315 | Acc: 10.460% (1620/15488)\n",
            "140 391 Loss: 2.314 | Acc: 10.522% (1899/18048)\n",
            "160 391 Loss: 2.314 | Acc: 10.496% (2163/20608)\n",
            "180 391 Loss: 2.315 | Acc: 10.497% (2432/23168)\n",
            "200 391 Loss: 2.315 | Acc: 10.471% (2694/25728)\n",
            "220 391 Loss: 2.315 | Acc: 10.538% (2981/28288)\n",
            "240 391 Loss: 2.315 | Acc: 10.529% (3248/30848)\n",
            "260 391 Loss: 2.315 | Acc: 10.506% (3510/33408)\n",
            "280 391 Loss: 2.315 | Acc: 10.579% (3805/35968)\n",
            "300 391 Loss: 2.315 | Acc: 10.553% (4066/38528)\n",
            "320 391 Loss: 2.315 | Acc: 10.555% (4337/41088)\n",
            "340 391 Loss: 2.315 | Acc: 10.610% (4631/43648)\n",
            "360 391 Loss: 2.315 | Acc: 10.591% (4894/46208)\n",
            "380 391 Loss: 2.315 | Acc: 10.589% (5164/48768)\n",
            "99 100 Loss: 3.968 | Acc: 9.879% (1000/10000)\n",
            "\n",
            "Epoch: 5\n",
            "0 391 Loss: 2.332 | Acc: 7.031% (9/128)\n",
            "20 391 Loss: 2.323 | Acc: 10.975% (295/2688)\n",
            "40 391 Loss: 2.320 | Acc: 10.118% (531/5248)\n",
            "60 391 Loss: 2.317 | Acc: 10.079% (787/7808)\n",
            "80 391 Loss: 2.315 | Acc: 10.301% (1068/10368)\n",
            "100 391 Loss: 2.315 | Acc: 10.373% (1341/12928)\n",
            "120 391 Loss: 2.315 | Acc: 10.518% (1629/15488)\n",
            "140 391 Loss: 2.315 | Acc: 10.461% (1888/18048)\n",
            "160 391 Loss: 2.314 | Acc: 10.520% (2168/20608)\n",
            "180 391 Loss: 2.314 | Acc: 10.614% (2459/23168)\n",
            "200 391 Loss: 2.315 | Acc: 10.576% (2721/25728)\n",
            "220 391 Loss: 2.315 | Acc: 10.609% (3001/28288)\n",
            "240 391 Loss: 2.315 | Acc: 10.594% (3268/30848)\n",
            "260 391 Loss: 2.315 | Acc: 10.569% (3531/33408)\n",
            "280 391 Loss: 2.314 | Acc: 10.590% (3809/35968)\n",
            "300 391 Loss: 2.314 | Acc: 10.649% (4103/38528)\n",
            "320 391 Loss: 2.314 | Acc: 10.650% (4376/41088)\n",
            "340 391 Loss: 2.314 | Acc: 10.660% (4653/43648)\n",
            "360 391 Loss: 2.315 | Acc: 10.622% (4908/46208)\n",
            "380 391 Loss: 2.315 | Acc: 10.655% (5196/48768)\n",
            "99 100 Loss: 3.694 | Acc: 9.879% (1000/10000)\n",
            "\n",
            "Epoch: 6\n",
            "0 391 Loss: 2.284 | Acc: 12.500% (16/128)\n",
            "20 391 Loss: 2.312 | Acc: 10.677% (287/2688)\n",
            "40 391 Loss: 2.310 | Acc: 10.575% (555/5248)\n",
            "60 391 Loss: 2.311 | Acc: 10.476% (818/7808)\n",
            "80 391 Loss: 2.314 | Acc: 10.532% (1092/10368)\n",
            "100 391 Loss: 2.313 | Acc: 10.721% (1386/12928)\n",
            "120 391 Loss: 2.313 | Acc: 10.834% (1678/15488)\n",
            "140 391 Loss: 2.313 | Acc: 10.744% (1939/18048)\n",
            "160 391 Loss: 2.313 | Acc: 10.743% (2214/20608)\n",
            "180 391 Loss: 2.313 | Acc: 10.752% (2491/23168)\n",
            "200 391 Loss: 2.313 | Acc: 10.708% (2755/25728)\n",
            "220 391 Loss: 2.312 | Acc: 10.764% (3045/28288)\n",
            "240 391 Loss: 2.313 | Acc: 10.685% (3296/30848)\n",
            "260 391 Loss: 2.313 | Acc: 10.665% (3563/33408)\n",
            "280 391 Loss: 2.313 | Acc: 10.676% (3840/35968)\n",
            "300 391 Loss: 2.313 | Acc: 10.683% (4116/38528)\n",
            "320 391 Loss: 2.312 | Acc: 10.740% (4413/41088)\n",
            "340 391 Loss: 2.312 | Acc: 10.782% (4706/43648)\n",
            "360 391 Loss: 2.312 | Acc: 10.764% (4974/46208)\n",
            "380 391 Loss: 2.312 | Acc: 10.784% (5259/48768)\n",
            "99 100 Loss: 3.370 | Acc: 9.879% (1000/10000)\n",
            "\n",
            "Epoch: 7\n",
            "0 391 Loss: 2.291 | Acc: 13.281% (17/128)\n",
            "20 391 Loss: 2.313 | Acc: 10.528% (283/2688)\n",
            "40 391 Loss: 2.312 | Acc: 10.004% (525/5248)\n",
            "60 391 Loss: 2.311 | Acc: 9.939% (776/7808)\n",
            "80 391 Loss: 2.313 | Acc: 9.761% (1012/10368)\n",
            "100 391 Loss: 2.311 | Acc: 10.257% (1326/12928)\n",
            "120 391 Loss: 2.311 | Acc: 10.169% (1575/15488)\n",
            "140 391 Loss: 2.311 | Acc: 10.372% (1872/18048)\n",
            "160 391 Loss: 2.311 | Acc: 10.413% (2146/20608)\n",
            "180 391 Loss: 2.311 | Acc: 10.363% (2401/23168)\n",
            "200 391 Loss: 2.311 | Acc: 10.382% (2671/25728)\n",
            "220 391 Loss: 2.312 | Acc: 10.382% (2937/28288)\n",
            "240 391 Loss: 2.312 | Acc: 10.422% (3215/30848)\n",
            "260 391 Loss: 2.311 | Acc: 10.491% (3505/33408)\n",
            "280 391 Loss: 2.311 | Acc: 10.504% (3778/35968)\n",
            "300 391 Loss: 2.311 | Acc: 10.517% (4052/38528)\n",
            "320 391 Loss: 2.311 | Acc: 10.453% (4295/41088)\n",
            "340 391 Loss: 2.311 | Acc: 10.484% (4576/43648)\n",
            "360 391 Loss: 2.311 | Acc: 10.466% (4836/46208)\n",
            "380 391 Loss: 2.312 | Acc: 10.509% (5125/48768)\n",
            "99 100 Loss: 3.167 | Acc: 9.879% (1000/10000)\n",
            "\n",
            "Epoch: 8\n",
            "0 391 Loss: 2.313 | Acc: 7.031% (9/128)\n",
            "20 391 Loss: 2.312 | Acc: 10.938% (294/2688)\n",
            "40 391 Loss: 2.309 | Acc: 10.690% (561/5248)\n",
            "60 391 Loss: 2.310 | Acc: 10.605% (828/7808)\n",
            "80 391 Loss: 2.309 | Acc: 10.610% (1100/10368)\n",
            "100 391 Loss: 2.310 | Acc: 10.690% (1382/12928)\n",
            "120 391 Loss: 2.309 | Acc: 10.557% (1635/15488)\n",
            "140 391 Loss: 2.310 | Acc: 10.588% (1911/18048)\n",
            "160 391 Loss: 2.310 | Acc: 10.598% (2184/20608)\n",
            "180 391 Loss: 2.310 | Acc: 10.597% (2455/23168)\n",
            "200 391 Loss: 2.310 | Acc: 10.634% (2736/25728)\n",
            "220 391 Loss: 2.310 | Acc: 10.690% (3024/28288)\n",
            "240 391 Loss: 2.311 | Acc: 10.691% (3298/30848)\n",
            "260 391 Loss: 2.311 | Acc: 10.626% (3550/33408)\n",
            "280 391 Loss: 2.311 | Acc: 10.618% (3819/35968)\n",
            "300 391 Loss: 2.311 | Acc: 10.608% (4087/38528)\n",
            "320 391 Loss: 2.311 | Acc: 10.592% (4352/41088)\n",
            "340 391 Loss: 2.311 | Acc: 10.612% (4632/43648)\n",
            "360 391 Loss: 2.311 | Acc: 10.632% (4913/46208)\n",
            "380 391 Loss: 2.310 | Acc: 10.663% (5200/48768)\n",
            "99 100 Loss: 3.083 | Acc: 9.879% (1000/10000)\n",
            "\n",
            "Epoch: 9\n",
            "0 391 Loss: 2.326 | Acc: 9.375% (12/128)\n",
            "20 391 Loss: 2.311 | Acc: 11.533% (310/2688)\n",
            "40 391 Loss: 2.313 | Acc: 11.090% (582/5248)\n",
            "60 391 Loss: 2.312 | Acc: 10.784% (842/7808)\n",
            "80 391 Loss: 2.312 | Acc: 10.745% (1114/10368)\n",
            "100 391 Loss: 2.311 | Acc: 10.938% (1414/12928)\n",
            "120 391 Loss: 2.312 | Acc: 10.873% (1684/15488)\n",
            "140 391 Loss: 2.310 | Acc: 11.010% (1987/18048)\n",
            "160 391 Loss: 2.310 | Acc: 10.933% (2253/20608)\n",
            "180 391 Loss: 2.309 | Acc: 10.886% (2522/23168)\n",
            "200 391 Loss: 2.310 | Acc: 10.868% (2796/25728)\n",
            "220 391 Loss: 2.310 | Acc: 10.909% (3086/28288)\n",
            "240 391 Loss: 2.310 | Acc: 10.996% (3392/30848)\n",
            "260 391 Loss: 2.310 | Acc: 10.985% (3670/33408)\n",
            "280 391 Loss: 2.309 | Acc: 11.146% (4009/35968)\n",
            "300 391 Loss: 2.309 | Acc: 11.166% (4302/38528)\n",
            "320 391 Loss: 2.310 | Acc: 11.176% (4592/41088)\n",
            "340 391 Loss: 2.310 | Acc: 11.194% (4886/43648)\n",
            "360 391 Loss: 2.309 | Acc: 11.158% (5156/46208)\n",
            "380 391 Loss: 2.309 | Acc: 11.114% (5420/48768)\n",
            "99 100 Loss: 2.953 | Acc: 9.879% (1000/10000)\n",
            "\n",
            "Epoch: 10\n",
            "0 391 Loss: 2.301 | Acc: 12.500% (16/128)\n",
            "20 391 Loss: 2.310 | Acc: 9.673% (260/2688)\n",
            "40 391 Loss: 2.308 | Acc: 10.347% (543/5248)\n",
            "60 391 Loss: 2.309 | Acc: 10.438% (815/7808)\n",
            "80 391 Loss: 2.307 | Acc: 10.494% (1088/10368)\n",
            "100 391 Loss: 2.309 | Acc: 10.373% (1341/12928)\n",
            "120 391 Loss: 2.308 | Acc: 10.318% (1598/15488)\n",
            "140 391 Loss: 2.309 | Acc: 10.422% (1881/18048)\n",
            "160 391 Loss: 2.310 | Acc: 10.345% (2132/20608)\n",
            "180 391 Loss: 2.309 | Acc: 10.299% (2386/23168)\n",
            "200 391 Loss: 2.309 | Acc: 10.424% (2682/25728)\n",
            "220 391 Loss: 2.309 | Acc: 10.425% (2949/28288)\n",
            "240 391 Loss: 2.309 | Acc: 10.442% (3221/30848)\n",
            "260 391 Loss: 2.309 | Acc: 10.489% (3504/33408)\n",
            "280 391 Loss: 2.310 | Acc: 10.498% (3776/35968)\n",
            "300 391 Loss: 2.310 | Acc: 10.569% (4072/38528)\n",
            "320 391 Loss: 2.309 | Acc: 10.653% (4377/41088)\n",
            "340 391 Loss: 2.309 | Acc: 10.644% (4646/43648)\n",
            "360 391 Loss: 2.309 | Acc: 10.667% (4929/46208)\n",
            "380 391 Loss: 2.309 | Acc: 10.655% (5196/48768)\n",
            "99 100 Loss: 2.841 | Acc: 9.879% (1000/10000)\n",
            "\n",
            "Epoch: 11\n",
            "0 391 Loss: 2.310 | Acc: 12.500% (16/128)\n",
            "20 391 Loss: 2.303 | Acc: 11.272% (303/2688)\n",
            "40 391 Loss: 2.307 | Acc: 11.090% (582/5248)\n",
            "60 391 Loss: 2.309 | Acc: 11.002% (859/7808)\n",
            "80 391 Loss: 2.310 | Acc: 10.909% (1131/10368)\n",
            "100 391 Loss: 2.310 | Acc: 11.077% (1432/12928)\n",
            "120 391 Loss: 2.311 | Acc: 10.873% (1684/15488)\n",
            "140 391 Loss: 2.310 | Acc: 11.010% (1987/18048)\n",
            "160 391 Loss: 2.310 | Acc: 10.996% (2266/20608)\n",
            "180 391 Loss: 2.311 | Acc: 10.821% (2507/23168)\n",
            "200 391 Loss: 2.310 | Acc: 10.798% (2778/25728)\n",
            "220 391 Loss: 2.310 | Acc: 10.842% (3067/28288)\n",
            "240 391 Loss: 2.309 | Acc: 10.879% (3356/30848)\n",
            "260 391 Loss: 2.309 | Acc: 10.833% (3619/33408)\n",
            "280 391 Loss: 2.309 | Acc: 10.796% (3883/35968)\n",
            "300 391 Loss: 2.309 | Acc: 10.800% (4161/38528)\n",
            "320 391 Loss: 2.310 | Acc: 10.748% (4416/41088)\n",
            "340 391 Loss: 2.310 | Acc: 10.745% (4690/43648)\n",
            "360 391 Loss: 2.309 | Acc: 10.790% (4986/46208)\n",
            "380 391 Loss: 2.309 | Acc: 10.829% (5281/48768)\n",
            "99 100 Loss: 2.800 | Acc: 9.879% (1000/10000)\n",
            "\n",
            "Epoch: 12\n",
            "0 391 Loss: 2.323 | Acc: 7.812% (10/128)\n",
            "20 391 Loss: 2.305 | Acc: 10.268% (276/2688)\n",
            "40 391 Loss: 2.308 | Acc: 10.175% (534/5248)\n",
            "60 391 Loss: 2.308 | Acc: 10.195% (796/7808)\n",
            "80 391 Loss: 2.309 | Acc: 10.002% (1037/10368)\n",
            "100 391 Loss: 2.308 | Acc: 10.110% (1307/12928)\n",
            "120 391 Loss: 2.308 | Acc: 10.227% (1584/15488)\n",
            "140 391 Loss: 2.308 | Acc: 10.300% (1859/18048)\n",
            "160 391 Loss: 2.308 | Acc: 10.389% (2141/20608)\n",
            "180 391 Loss: 2.308 | Acc: 10.454% (2422/23168)\n",
            "200 391 Loss: 2.307 | Acc: 10.522% (2707/25728)\n",
            "220 391 Loss: 2.307 | Acc: 10.605% (3000/28288)\n",
            "240 391 Loss: 2.307 | Acc: 10.711% (3304/30848)\n",
            "260 391 Loss: 2.307 | Acc: 10.707% (3577/33408)\n",
            "280 391 Loss: 2.307 | Acc: 10.729% (3859/35968)\n",
            "300 391 Loss: 2.307 | Acc: 10.673% (4112/38528)\n",
            "320 391 Loss: 2.307 | Acc: 10.701% (4397/41088)\n",
            "340 391 Loss: 2.307 | Acc: 10.720% (4679/43648)\n",
            "360 391 Loss: 2.307 | Acc: 10.760% (4972/46208)\n",
            "380 391 Loss: 2.307 | Acc: 10.782% (5258/48768)\n",
            "99 100 Loss: 2.820 | Acc: 9.879% (1000/10000)\n",
            "\n",
            "Epoch: 13\n",
            "0 391 Loss: 2.302 | Acc: 14.062% (18/128)\n",
            "20 391 Loss: 2.315 | Acc: 8.929% (240/2688)\n",
            "40 391 Loss: 2.310 | Acc: 10.004% (525/5248)\n",
            "60 391 Loss: 2.310 | Acc: 10.361% (809/7808)\n",
            "80 391 Loss: 2.308 | Acc: 10.822% (1122/10368)\n",
            "100 391 Loss: 2.308 | Acc: 10.907% (1410/12928)\n",
            "120 391 Loss: 2.309 | Acc: 10.718% (1660/15488)\n",
            "140 391 Loss: 2.308 | Acc: 10.805% (1950/18048)\n",
            "160 391 Loss: 2.308 | Acc: 10.729% (2211/20608)\n",
            "180 391 Loss: 2.308 | Acc: 10.717% (2483/23168)\n",
            "200 391 Loss: 2.307 | Acc: 10.770% (2771/25728)\n",
            "220 391 Loss: 2.307 | Acc: 10.789% (3052/28288)\n",
            "240 391 Loss: 2.307 | Acc: 10.805% (3333/30848)\n",
            "260 391 Loss: 2.307 | Acc: 10.788% (3604/33408)\n",
            "280 391 Loss: 2.307 | Acc: 10.729% (3859/35968)\n",
            "300 391 Loss: 2.307 | Acc: 10.699% (4122/38528)\n",
            "320 391 Loss: 2.307 | Acc: 10.716% (4403/41088)\n",
            "340 391 Loss: 2.307 | Acc: 10.674% (4659/43648)\n",
            "360 391 Loss: 2.307 | Acc: 10.673% (4932/46208)\n",
            "380 391 Loss: 2.307 | Acc: 10.737% (5236/48768)\n",
            "99 100 Loss: 2.746 | Acc: 9.879% (1000/10000)\n",
            "\n",
            "Epoch: 14\n",
            "0 391 Loss: 2.295 | Acc: 8.594% (11/128)\n",
            "20 391 Loss: 2.303 | Acc: 10.379% (279/2688)\n",
            "40 391 Loss: 2.305 | Acc: 11.014% (578/5248)\n",
            "60 391 Loss: 2.304 | Acc: 10.950% (855/7808)\n",
            "80 391 Loss: 2.305 | Acc: 11.044% (1145/10368)\n",
            "100 391 Loss: 2.304 | Acc: 11.100% (1435/12928)\n",
            "120 391 Loss: 2.303 | Acc: 11.299% (1750/15488)\n",
            "140 391 Loss: 2.304 | Acc: 11.253% (2031/18048)\n",
            "160 391 Loss: 2.304 | Acc: 11.195% (2307/20608)\n",
            "180 391 Loss: 2.304 | Acc: 11.175% (2589/23168)\n",
            "200 391 Loss: 2.304 | Acc: 11.124% (2862/25728)\n",
            "220 391 Loss: 2.304 | Acc: 11.132% (3149/28288)\n",
            "240 391 Loss: 2.305 | Acc: 11.096% (3423/30848)\n",
            "260 391 Loss: 2.305 | Acc: 11.150% (3725/33408)\n",
            "280 391 Loss: 2.305 | Acc: 11.090% (3989/35968)\n",
            "300 391 Loss: 2.305 | Acc: 11.140% (4292/38528)\n",
            "320 391 Loss: 2.305 | Acc: 11.186% (4596/41088)\n",
            "340 391 Loss: 2.305 | Acc: 11.151% (4867/43648)\n",
            "360 391 Loss: 2.305 | Acc: 11.145% (5150/46208)\n",
            "380 391 Loss: 2.305 | Acc: 11.161% (5443/48768)\n",
            "99 100 Loss: 2.671 | Acc: 9.232% (930/10000)\n",
            "\n",
            "Epoch: 15\n",
            "0 391 Loss: 2.276 | Acc: 17.188% (22/128)\n",
            "20 391 Loss: 2.307 | Acc: 11.533% (310/2688)\n",
            "40 391 Loss: 2.305 | Acc: 11.147% (585/5248)\n",
            "60 391 Loss: 2.305 | Acc: 10.938% (854/7808)\n",
            "80 391 Loss: 2.307 | Acc: 10.812% (1121/10368)\n",
            "100 391 Loss: 2.308 | Acc: 10.837% (1401/12928)\n",
            "120 391 Loss: 2.307 | Acc: 10.802% (1673/15488)\n",
            "140 391 Loss: 2.307 | Acc: 10.915% (1970/18048)\n",
            "160 391 Loss: 2.306 | Acc: 10.928% (2252/20608)\n",
            "180 391 Loss: 2.306 | Acc: 10.868% (2518/23168)\n",
            "200 391 Loss: 2.306 | Acc: 10.934% (2813/25728)\n",
            "220 391 Loss: 2.306 | Acc: 10.874% (3076/28288)\n",
            "240 391 Loss: 2.306 | Acc: 10.873% (3354/30848)\n",
            "260 391 Loss: 2.306 | Acc: 10.881% (3635/33408)\n",
            "280 391 Loss: 2.306 | Acc: 10.896% (3919/35968)\n",
            "300 391 Loss: 2.306 | Acc: 10.943% (4216/38528)\n",
            "320 391 Loss: 2.306 | Acc: 10.935% (4493/41088)\n",
            "340 391 Loss: 2.306 | Acc: 10.896% (4756/43648)\n",
            "360 391 Loss: 2.305 | Acc: 10.961% (5065/46208)\n",
            "380 391 Loss: 2.305 | Acc: 10.985% (5357/48768)\n",
            "99 100 Loss: 2.730 | Acc: 9.958% (1000/10000)\n",
            "\n",
            "Epoch: 16\n",
            "0 391 Loss: 2.313 | Acc: 11.719% (15/128)\n",
            "20 391 Loss: 2.303 | Acc: 11.310% (304/2688)\n",
            "40 391 Loss: 2.305 | Acc: 11.071% (581/5248)\n",
            "60 391 Loss: 2.305 | Acc: 11.078% (865/7808)\n",
            "80 391 Loss: 2.305 | Acc: 10.986% (1139/10368)\n",
            "100 391 Loss: 2.305 | Acc: 11.084% (1433/12928)\n",
            "120 391 Loss: 2.305 | Acc: 11.176% (1731/15488)\n",
            "140 391 Loss: 2.305 | Acc: 11.248% (2030/18048)\n",
            "160 391 Loss: 2.305 | Acc: 11.049% (2277/20608)\n",
            "180 391 Loss: 2.306 | Acc: 11.028% (2555/23168)\n",
            "200 391 Loss: 2.305 | Acc: 11.023% (2836/25728)\n",
            "220 391 Loss: 2.305 | Acc: 11.033% (3121/28288)\n",
            "240 391 Loss: 2.305 | Acc: 11.002% (3394/30848)\n",
            "260 391 Loss: 2.305 | Acc: 10.988% (3671/33408)\n",
            "280 391 Loss: 2.305 | Acc: 10.988% (3952/35968)\n",
            "300 391 Loss: 2.305 | Acc: 11.078% (4268/38528)\n",
            "320 391 Loss: 2.305 | Acc: 11.093% (4558/41088)\n",
            "340 391 Loss: 2.305 | Acc: 11.135% (4860/43648)\n",
            "360 391 Loss: 2.305 | Acc: 11.063% (5112/46208)\n",
            "380 391 Loss: 2.305 | Acc: 11.091% (5409/48768)\n",
            "99 100 Loss: 2.623 | Acc: 9.958% (1000/10000)\n",
            "\n",
            "Epoch: 17\n",
            "0 391 Loss: 2.282 | Acc: 12.500% (16/128)\n",
            "20 391 Loss: 2.305 | Acc: 10.305% (277/2688)\n",
            "40 391 Loss: 2.304 | Acc: 10.899% (572/5248)\n",
            "60 391 Loss: 2.304 | Acc: 11.219% (876/7808)\n",
            "80 391 Loss: 2.304 | Acc: 11.092% (1150/10368)\n",
            "100 391 Loss: 2.304 | Acc: 11.332% (1465/12928)\n",
            "120 391 Loss: 2.304 | Acc: 11.228% (1739/15488)\n",
            "140 391 Loss: 2.304 | Acc: 11.181% (2018/18048)\n",
            "160 391 Loss: 2.304 | Acc: 11.064% (2280/20608)\n",
            "180 391 Loss: 2.304 | Acc: 11.050% (2560/23168)\n",
            "200 391 Loss: 2.303 | Acc: 11.178% (2876/25728)\n",
            "220 391 Loss: 2.303 | Acc: 11.157% (3156/28288)\n",
            "240 391 Loss: 2.303 | Acc: 11.164% (3444/30848)\n",
            "260 391 Loss: 2.303 | Acc: 11.129% (3718/33408)\n",
            "280 391 Loss: 2.304 | Acc: 11.143% (4008/35968)\n",
            "300 391 Loss: 2.303 | Acc: 11.161% (4300/38528)\n",
            "320 391 Loss: 2.303 | Acc: 11.152% (4582/41088)\n",
            "340 391 Loss: 2.303 | Acc: 11.173% (4877/43648)\n",
            "360 391 Loss: 2.303 | Acc: 11.180% (5166/46208)\n",
            "380 391 Loss: 2.303 | Acc: 11.167% (5446/48768)\n",
            "99 100 Loss: 2.626 | Acc: 9.958% (1000/10000)\n",
            "\n",
            "Epoch: 18\n",
            "0 391 Loss: 2.323 | Acc: 9.375% (12/128)\n",
            "20 391 Loss: 2.307 | Acc: 12.016% (323/2688)\n",
            "40 391 Loss: 2.305 | Acc: 11.338% (595/5248)\n",
            "60 391 Loss: 2.306 | Acc: 11.283% (881/7808)\n",
            "80 391 Loss: 2.305 | Acc: 11.101% (1151/10368)\n",
            "100 391 Loss: 2.304 | Acc: 11.123% (1438/12928)\n",
            "120 391 Loss: 2.305 | Acc: 10.963% (1698/15488)\n",
            "140 391 Loss: 2.305 | Acc: 10.976% (1981/18048)\n",
            "160 391 Loss: 2.305 | Acc: 10.913% (2249/20608)\n",
            "180 391 Loss: 2.305 | Acc: 10.994% (2547/23168)\n",
            "200 391 Loss: 2.305 | Acc: 10.899% (2804/25728)\n",
            "220 391 Loss: 2.305 | Acc: 10.856% (3071/28288)\n",
            "240 391 Loss: 2.305 | Acc: 10.853% (3348/30848)\n",
            "260 391 Loss: 2.305 | Acc: 10.958% (3661/33408)\n",
            "280 391 Loss: 2.304 | Acc: 10.979% (3949/35968)\n",
            "300 391 Loss: 2.304 | Acc: 10.966% (4225/38528)\n",
            "320 391 Loss: 2.304 | Acc: 11.037% (4535/41088)\n",
            "340 391 Loss: 2.303 | Acc: 11.054% (4825/43648)\n",
            "360 391 Loss: 2.303 | Acc: 11.072% (5116/46208)\n",
            "380 391 Loss: 2.303 | Acc: 11.097% (5412/48768)\n",
            "99 100 Loss: 2.649 | Acc: 9.958% (1000/10000)\n",
            "\n",
            "Epoch: 19\n",
            "0 391 Loss: 2.295 | Acc: 14.062% (18/128)\n",
            "20 391 Loss: 2.309 | Acc: 10.082% (271/2688)\n",
            "40 391 Loss: 2.308 | Acc: 10.480% (550/5248)\n",
            "60 391 Loss: 2.307 | Acc: 11.066% (864/7808)\n",
            "80 391 Loss: 2.306 | Acc: 11.217% (1163/10368)\n",
            "100 391 Loss: 2.305 | Acc: 11.154% (1442/12928)\n",
            "120 391 Loss: 2.305 | Acc: 11.067% (1714/15488)\n",
            "140 391 Loss: 2.305 | Acc: 11.043% (1993/18048)\n",
            "160 391 Loss: 2.305 | Acc: 11.035% (2274/20608)\n",
            "180 391 Loss: 2.305 | Acc: 11.041% (2558/23168)\n",
            "200 391 Loss: 2.304 | Acc: 11.050% (2843/25728)\n",
            "220 391 Loss: 2.304 | Acc: 11.100% (3140/28288)\n",
            "240 391 Loss: 2.304 | Acc: 11.028% (3402/30848)\n",
            "260 391 Loss: 2.304 | Acc: 10.985% (3670/33408)\n",
            "280 391 Loss: 2.304 | Acc: 11.024% (3965/35968)\n",
            "300 391 Loss: 2.304 | Acc: 11.000% (4238/38528)\n",
            "320 391 Loss: 2.304 | Acc: 10.991% (4516/41088)\n",
            "340 391 Loss: 2.303 | Acc: 11.009% (4805/43648)\n",
            "360 391 Loss: 2.303 | Acc: 11.048% (5105/46208)\n",
            "380 391 Loss: 2.304 | Acc: 11.058% (5393/48768)\n",
            "99 100 Loss: 2.598 | Acc: 9.958% (1000/10000)\n",
            "\n",
            "Epoch: 20\n",
            "0 391 Loss: 2.314 | Acc: 8.594% (11/128)\n",
            "20 391 Loss: 2.304 | Acc: 10.379% (279/2688)\n",
            "40 391 Loss: 2.304 | Acc: 10.671% (560/5248)\n",
            "60 391 Loss: 2.304 | Acc: 10.848% (847/7808)\n",
            "80 391 Loss: 2.304 | Acc: 10.860% (1126/10368)\n",
            "100 391 Loss: 2.304 | Acc: 10.961% (1417/12928)\n",
            "120 391 Loss: 2.303 | Acc: 10.931% (1693/15488)\n",
            "140 391 Loss: 2.303 | Acc: 11.048% (1994/18048)\n",
            "160 391 Loss: 2.303 | Acc: 11.107% (2289/20608)\n",
            "180 391 Loss: 2.303 | Acc: 11.007% (2550/23168)\n",
            "200 391 Loss: 2.303 | Acc: 11.015% (2834/25728)\n",
            "220 391 Loss: 2.303 | Acc: 11.040% (3123/28288)\n",
            "240 391 Loss: 2.303 | Acc: 11.080% (3418/30848)\n",
            "260 391 Loss: 2.303 | Acc: 11.048% (3691/33408)\n",
            "280 391 Loss: 2.302 | Acc: 11.082% (3986/35968)\n",
            "300 391 Loss: 2.303 | Acc: 11.137% (4291/38528)\n",
            "320 391 Loss: 2.303 | Acc: 11.103% (4562/41088)\n",
            "340 391 Loss: 2.303 | Acc: 11.089% (4840/43648)\n",
            "360 391 Loss: 2.303 | Acc: 11.028% (5096/46208)\n",
            "380 391 Loss: 2.303 | Acc: 11.032% (5380/48768)\n",
            "99 100 Loss: 2.568 | Acc: 9.958% (1000/10000)\n",
            "\n",
            "Epoch: 21\n",
            "0 391 Loss: 2.318 | Acc: 8.594% (11/128)\n",
            "20 391 Loss: 2.305 | Acc: 10.714% (288/2688)\n",
            "40 391 Loss: 2.303 | Acc: 11.109% (583/5248)\n",
            "60 391 Loss: 2.303 | Acc: 11.322% (884/7808)\n",
            "80 391 Loss: 2.304 | Acc: 11.256% (1167/10368)\n",
            "100 391 Loss: 2.303 | Acc: 11.293% (1460/12928)\n",
            "120 391 Loss: 2.304 | Acc: 11.163% (1729/15488)\n",
            "140 391 Loss: 2.303 | Acc: 11.275% (2035/18048)\n",
            "160 391 Loss: 2.303 | Acc: 11.287% (2326/20608)\n",
            "180 391 Loss: 2.302 | Acc: 11.296% (2617/23168)\n",
            "200 391 Loss: 2.303 | Acc: 11.283% (2903/25728)\n",
            "220 391 Loss: 2.303 | Acc: 11.249% (3182/28288)\n",
            "240 391 Loss: 2.303 | Acc: 11.268% (3476/30848)\n",
            "260 391 Loss: 2.303 | Acc: 11.300% (3775/33408)\n",
            "280 391 Loss: 2.303 | Acc: 11.318% (4071/35968)\n",
            "300 391 Loss: 2.303 | Acc: 11.290% (4350/38528)\n",
            "320 391 Loss: 2.303 | Acc: 11.261% (4627/41088)\n",
            "340 391 Loss: 2.303 | Acc: 11.288% (4927/43648)\n",
            "360 391 Loss: 2.303 | Acc: 11.331% (5236/46208)\n",
            "380 391 Loss: 2.303 | Acc: 11.344% (5532/48768)\n",
            "99 100 Loss: 2.527 | Acc: 9.958% (1000/10000)\n",
            "\n",
            "Epoch: 22\n",
            "0 391 Loss: 2.308 | Acc: 10.156% (13/128)\n",
            "20 391 Loss: 2.303 | Acc: 11.570% (311/2688)\n",
            "40 391 Loss: 2.303 | Acc: 11.338% (595/5248)\n",
            "60 391 Loss: 2.304 | Acc: 11.091% (866/7808)\n",
            "80 391 Loss: 2.303 | Acc: 11.159% (1157/10368)\n",
            "100 391 Loss: 2.302 | Acc: 11.402% (1474/12928)\n",
            "120 391 Loss: 2.303 | Acc: 11.454% (1774/15488)\n",
            "140 391 Loss: 2.303 | Acc: 11.447% (2066/18048)\n",
            "160 391 Loss: 2.303 | Acc: 11.316% (2332/20608)\n",
            "180 391 Loss: 2.303 | Acc: 11.304% (2619/23168)\n",
            "200 391 Loss: 2.303 | Acc: 11.268% (2899/25728)\n",
            "220 391 Loss: 2.303 | Acc: 11.316% (3201/28288)\n",
            "240 391 Loss: 2.303 | Acc: 11.359% (3504/30848)\n",
            "260 391 Loss: 2.303 | Acc: 11.315% (3780/33408)\n",
            "280 391 Loss: 2.303 | Acc: 11.335% (4077/35968)\n",
            "300 391 Loss: 2.303 | Acc: 11.213% (4320/38528)\n",
            "320 391 Loss: 2.303 | Acc: 11.232% (4615/41088)\n",
            "340 391 Loss: 2.303 | Acc: 11.212% (4894/43648)\n",
            "360 391 Loss: 2.303 | Acc: 11.212% (5181/46208)\n",
            "380 391 Loss: 2.303 | Acc: 11.214% (5469/48768)\n",
            "99 100 Loss: 2.529 | Acc: 9.958% (1000/10000)\n",
            "\n",
            "Epoch: 23\n",
            "0 391 Loss: 2.327 | Acc: 7.812% (10/128)\n",
            "20 391 Loss: 2.303 | Acc: 11.012% (296/2688)\n",
            "40 391 Loss: 2.302 | Acc: 10.842% (569/5248)\n",
            "60 391 Loss: 2.301 | Acc: 10.886% (850/7808)\n",
            "80 391 Loss: 2.301 | Acc: 10.841% (1124/10368)\n",
            "100 391 Loss: 2.301 | Acc: 11.054% (1429/12928)\n",
            "120 391 Loss: 2.301 | Acc: 10.976% (1700/15488)\n",
            "140 391 Loss: 2.301 | Acc: 10.877% (1963/18048)\n",
            "160 391 Loss: 2.301 | Acc: 10.991% (2265/20608)\n",
            "180 391 Loss: 2.301 | Acc: 10.985% (2545/23168)\n",
            "200 391 Loss: 2.301 | Acc: 11.089% (2853/25728)\n",
            "220 391 Loss: 2.301 | Acc: 11.072% (3132/28288)\n",
            "240 391 Loss: 2.301 | Acc: 11.113% (3428/30848)\n",
            "260 391 Loss: 2.301 | Acc: 11.111% (3712/33408)\n",
            "280 391 Loss: 2.301 | Acc: 11.096% (3991/35968)\n",
            "300 391 Loss: 2.301 | Acc: 11.119% (4284/38528)\n",
            "320 391 Loss: 2.301 | Acc: 11.142% (4578/41088)\n",
            "340 391 Loss: 2.301 | Acc: 11.180% (4880/43648)\n",
            "360 391 Loss: 2.302 | Acc: 11.160% (5157/46208)\n",
            "380 391 Loss: 2.302 | Acc: 11.136% (5431/48768)\n",
            "99 100 Loss: 2.483 | Acc: 9.958% (1000/10000)\n",
            "\n",
            "Epoch: 24\n",
            "0 391 Loss: 2.306 | Acc: 10.938% (14/128)\n",
            "20 391 Loss: 2.303 | Acc: 10.900% (293/2688)\n",
            "40 391 Loss: 2.302 | Acc: 11.471% (602/5248)\n",
            "60 391 Loss: 2.302 | Acc: 11.181% (873/7808)\n",
            "80 391 Loss: 2.304 | Acc: 10.774% (1117/10368)\n",
            "100 391 Loss: 2.304 | Acc: 10.713% (1385/12928)\n",
            "120 391 Loss: 2.304 | Acc: 10.828% (1677/15488)\n",
            "140 391 Loss: 2.304 | Acc: 10.849% (1958/18048)\n",
            "160 391 Loss: 2.303 | Acc: 10.816% (2229/20608)\n",
            "180 391 Loss: 2.303 | Acc: 10.903% (2526/23168)\n",
            "200 391 Loss: 2.302 | Acc: 10.996% (2829/25728)\n",
            "220 391 Loss: 2.302 | Acc: 11.008% (3114/28288)\n",
            "240 391 Loss: 2.302 | Acc: 11.012% (3397/30848)\n",
            "260 391 Loss: 2.302 | Acc: 10.952% (3659/33408)\n",
            "280 391 Loss: 2.302 | Acc: 10.901% (3921/35968)\n",
            "300 391 Loss: 2.303 | Acc: 10.875% (4190/38528)\n",
            "320 391 Loss: 2.303 | Acc: 10.804% (4439/41088)\n",
            "340 391 Loss: 2.303 | Acc: 10.821% (4723/43648)\n",
            "360 391 Loss: 2.302 | Acc: 10.875% (5025/46208)\n",
            "380 391 Loss: 2.302 | Acc: 10.909% (5320/48768)\n",
            "99 100 Loss: 2.475 | Acc: 9.958% (1000/10000)\n",
            "\n",
            "Epoch: 25\n",
            "0 391 Loss: 2.312 | Acc: 7.812% (10/128)\n",
            "20 391 Loss: 2.304 | Acc: 10.975% (295/2688)\n",
            "40 391 Loss: 2.302 | Acc: 11.452% (601/5248)\n",
            "60 391 Loss: 2.302 | Acc: 11.270% (880/7808)\n",
            "80 391 Loss: 2.303 | Acc: 11.159% (1157/10368)\n",
            "100 391 Loss: 2.302 | Acc: 11.177% (1445/12928)\n",
            "120 391 Loss: 2.303 | Acc: 11.118% (1722/15488)\n",
            "140 391 Loss: 2.303 | Acc: 11.209% (2023/18048)\n",
            "160 391 Loss: 2.302 | Acc: 11.263% (2321/20608)\n",
            "180 391 Loss: 2.302 | Acc: 11.378% (2636/23168)\n",
            "200 391 Loss: 2.302 | Acc: 11.338% (2917/25728)\n",
            "220 391 Loss: 2.302 | Acc: 11.277% (3190/28288)\n",
            "240 391 Loss: 2.302 | Acc: 11.323% (3493/30848)\n",
            "260 391 Loss: 2.303 | Acc: 11.279% (3768/33408)\n",
            "280 391 Loss: 2.303 | Acc: 11.193% (4026/35968)\n",
            "300 391 Loss: 2.303 | Acc: 11.202% (4316/38528)\n",
            "320 391 Loss: 2.302 | Acc: 11.208% (4605/41088)\n",
            "340 391 Loss: 2.302 | Acc: 11.190% (4884/43648)\n",
            "360 391 Loss: 2.302 | Acc: 11.113% (5135/46208)\n",
            "380 391 Loss: 2.302 | Acc: 11.108% (5417/48768)\n",
            "99 100 Loss: 2.485 | Acc: 11.246% (1100/10000)\n",
            "\n",
            "Epoch: 26\n",
            "0 391 Loss: 2.322 | Acc: 13.281% (17/128)\n",
            "20 391 Loss: 2.304 | Acc: 10.454% (281/2688)\n",
            "40 391 Loss: 2.304 | Acc: 10.633% (558/5248)\n",
            "60 391 Loss: 2.303 | Acc: 11.014% (860/7808)\n",
            "80 391 Loss: 2.302 | Acc: 11.159% (1157/10368)\n",
            "100 391 Loss: 2.301 | Acc: 11.270% (1457/12928)\n",
            "120 391 Loss: 2.302 | Acc: 11.402% (1766/15488)\n",
            "140 391 Loss: 2.302 | Acc: 11.375% (2053/18048)\n",
            "160 391 Loss: 2.302 | Acc: 11.238% (2316/20608)\n",
            "180 391 Loss: 2.302 | Acc: 11.205% (2596/23168)\n",
            "200 391 Loss: 2.302 | Acc: 11.225% (2888/25728)\n",
            "220 391 Loss: 2.301 | Acc: 11.319% (3202/28288)\n",
            "240 391 Loss: 2.301 | Acc: 11.284% (3481/30848)\n",
            "260 391 Loss: 2.301 | Acc: 11.333% (3786/33408)\n",
            "280 391 Loss: 2.301 | Acc: 11.455% (4120/35968)\n",
            "300 391 Loss: 2.301 | Acc: 11.446% (4410/38528)\n",
            "320 391 Loss: 2.301 | Acc: 11.427% (4695/41088)\n",
            "340 391 Loss: 2.301 | Acc: 11.432% (4990/43648)\n",
            "360 391 Loss: 2.301 | Acc: 11.372% (5255/46208)\n",
            "380 391 Loss: 2.301 | Acc: 11.376% (5548/48768)\n",
            "99 100 Loss: 2.472 | Acc: 9.958% (1000/10000)\n",
            "\n",
            "Epoch: 27\n",
            "0 391 Loss: 2.309 | Acc: 10.938% (14/128)\n",
            "20 391 Loss: 2.307 | Acc: 11.905% (320/2688)\n",
            "40 391 Loss: 2.302 | Acc: 11.643% (611/5248)\n",
            "60 391 Loss: 2.303 | Acc: 11.194% (874/7808)\n",
            "80 391 Loss: 2.302 | Acc: 11.478% (1190/10368)\n",
            "100 391 Loss: 2.302 | Acc: 11.587% (1498/12928)\n",
            "120 391 Loss: 2.302 | Acc: 11.499% (1781/15488)\n",
            "140 391 Loss: 2.302 | Acc: 11.364% (2051/18048)\n",
            "160 391 Loss: 2.301 | Acc: 11.297% (2328/20608)\n",
            "180 391 Loss: 2.301 | Acc: 11.330% (2625/23168)\n",
            "200 391 Loss: 2.301 | Acc: 11.350% (2920/25728)\n",
            "220 391 Loss: 2.300 | Acc: 11.404% (3226/28288)\n",
            "240 391 Loss: 2.300 | Acc: 11.391% (3514/30848)\n",
            "260 391 Loss: 2.300 | Acc: 11.419% (3815/33408)\n",
            "280 391 Loss: 2.300 | Acc: 11.477% (4128/35968)\n",
            "300 391 Loss: 2.300 | Acc: 11.441% (4408/38528)\n",
            "320 391 Loss: 2.300 | Acc: 11.429% (4696/41088)\n",
            "340 391 Loss: 2.300 | Acc: 11.480% (5011/43648)\n",
            "360 391 Loss: 2.300 | Acc: 11.461% (5296/46208)\n",
            "380 391 Loss: 2.300 | Acc: 11.444% (5581/48768)\n",
            "99 100 Loss: 2.487 | Acc: 9.958% (1000/10000)\n",
            "\n",
            "Epoch: 28\n",
            "0 391 Loss: 2.284 | Acc: 13.281% (17/128)\n",
            "20 391 Loss: 2.298 | Acc: 11.235% (302/2688)\n",
            "40 391 Loss: 2.298 | Acc: 11.357% (596/5248)\n",
            "60 391 Loss: 2.298 | Acc: 11.360% (887/7808)\n",
            "80 391 Loss: 2.298 | Acc: 11.439% (1186/10368)\n",
            "100 391 Loss: 2.299 | Acc: 11.386% (1472/12928)\n",
            "120 391 Loss: 2.299 | Acc: 11.325% (1754/15488)\n",
            "140 391 Loss: 2.299 | Acc: 11.381% (2054/18048)\n",
            "160 391 Loss: 2.299 | Acc: 11.355% (2340/20608)\n",
            "180 391 Loss: 2.299 | Acc: 11.408% (2643/23168)\n",
            "200 391 Loss: 2.299 | Acc: 11.400% (2933/25728)\n",
            "220 391 Loss: 2.300 | Acc: 11.362% (3214/28288)\n",
            "240 391 Loss: 2.300 | Acc: 11.320% (3492/30848)\n",
            "260 391 Loss: 2.300 | Acc: 11.330% (3785/33408)\n",
            "280 391 Loss: 2.300 | Acc: 11.396% (4099/35968)\n",
            "300 391 Loss: 2.300 | Acc: 11.392% (4389/38528)\n",
            "320 391 Loss: 2.300 | Acc: 11.422% (4693/41088)\n",
            "340 391 Loss: 2.300 | Acc: 11.428% (4988/43648)\n",
            "360 391 Loss: 2.300 | Acc: 11.394% (5265/46208)\n",
            "380 391 Loss: 2.300 | Acc: 11.354% (5537/48768)\n",
            "99 100 Loss: 2.458 | Acc: 9.958% (1000/10000)\n",
            "\n",
            "Epoch: 29\n",
            "0 391 Loss: 2.302 | Acc: 11.719% (15/128)\n",
            "20 391 Loss: 2.306 | Acc: 9.933% (267/2688)\n",
            "40 391 Loss: 2.302 | Acc: 10.499% (551/5248)\n",
            "60 391 Loss: 2.301 | Acc: 10.745% (839/7808)\n",
            "80 391 Loss: 2.300 | Acc: 10.667% (1106/10368)\n",
            "100 391 Loss: 2.300 | Acc: 10.605% (1371/12928)\n",
            "120 391 Loss: 2.300 | Acc: 10.686% (1655/15488)\n",
            "140 391 Loss: 2.300 | Acc: 10.838% (1956/18048)\n",
            "160 391 Loss: 2.301 | Acc: 10.913% (2249/20608)\n",
            "180 391 Loss: 2.301 | Acc: 10.890% (2523/23168)\n",
            "200 391 Loss: 2.301 | Acc: 10.864% (2795/25728)\n",
            "220 391 Loss: 2.301 | Acc: 11.005% (3113/28288)\n",
            "240 391 Loss: 2.301 | Acc: 11.015% (3398/30848)\n",
            "260 391 Loss: 2.301 | Acc: 10.967% (3664/33408)\n",
            "280 391 Loss: 2.301 | Acc: 10.924% (3929/35968)\n",
            "300 391 Loss: 2.301 | Acc: 10.948% (4218/38528)\n",
            "320 391 Loss: 2.301 | Acc: 10.930% (4491/41088)\n",
            "340 391 Loss: 2.301 | Acc: 10.999% (4801/43648)\n",
            "360 391 Loss: 2.301 | Acc: 11.052% (5107/46208)\n",
            "380 391 Loss: 2.301 | Acc: 11.040% (5384/48768)\n",
            "99 100 Loss: 2.442 | Acc: 9.946% (999/10000)\n",
            "\n",
            "Epoch: 30\n",
            "0 391 Loss: 2.300 | Acc: 17.969% (23/128)\n",
            "20 391 Loss: 2.302 | Acc: 11.533% (310/2688)\n",
            "40 391 Loss: 2.299 | Acc: 12.043% (632/5248)\n",
            "60 391 Loss: 2.299 | Acc: 11.501% (898/7808)\n",
            "80 391 Loss: 2.299 | Acc: 11.468% (1189/10368)\n",
            "100 391 Loss: 2.299 | Acc: 11.533% (1491/12928)\n",
            "120 391 Loss: 2.299 | Acc: 11.615% (1799/15488)\n",
            "140 391 Loss: 2.300 | Acc: 11.625% (2098/18048)\n",
            "160 391 Loss: 2.300 | Acc: 11.593% (2389/20608)\n",
            "180 391 Loss: 2.299 | Acc: 11.525% (2670/23168)\n",
            "200 391 Loss: 2.300 | Acc: 11.489% (2956/25728)\n",
            "220 391 Loss: 2.300 | Acc: 11.447% (3238/28288)\n",
            "240 391 Loss: 2.300 | Acc: 11.414% (3521/30848)\n",
            "260 391 Loss: 2.300 | Acc: 11.342% (3789/33408)\n",
            "280 391 Loss: 2.300 | Acc: 11.327% (4074/35968)\n",
            "300 391 Loss: 2.300 | Acc: 11.272% (4343/38528)\n",
            "320 391 Loss: 2.300 | Acc: 11.232% (4615/41088)\n",
            "340 391 Loss: 2.300 | Acc: 11.215% (4895/43648)\n",
            "360 391 Loss: 2.300 | Acc: 11.258% (5202/46208)\n",
            "380 391 Loss: 2.300 | Acc: 11.210% (5467/48768)\n",
            "99 100 Loss: 2.469 | Acc: 11.392% (1090/10000)\n",
            "\n",
            "Epoch: 31\n",
            "0 391 Loss: 2.310 | Acc: 8.594% (11/128)\n",
            "20 391 Loss: 2.301 | Acc: 11.570% (311/2688)\n",
            "40 391 Loss: 2.301 | Acc: 11.014% (578/5248)\n",
            "60 391 Loss: 2.301 | Acc: 11.168% (872/7808)\n",
            "80 391 Loss: 2.300 | Acc: 11.468% (1189/10368)\n",
            "100 391 Loss: 2.300 | Acc: 11.696% (1512/12928)\n",
            "120 391 Loss: 2.301 | Acc: 11.699% (1812/15488)\n",
            "140 391 Loss: 2.301 | Acc: 11.514% (2078/18048)\n",
            "160 391 Loss: 2.301 | Acc: 11.588% (2388/20608)\n",
            "180 391 Loss: 2.301 | Acc: 11.676% (2705/23168)\n",
            "200 391 Loss: 2.301 | Acc: 11.598% (2984/25728)\n",
            "220 391 Loss: 2.301 | Acc: 11.613% (3285/28288)\n",
            "240 391 Loss: 2.300 | Acc: 11.673% (3601/30848)\n",
            "260 391 Loss: 2.300 | Acc: 11.647% (3891/33408)\n",
            "280 391 Loss: 2.300 | Acc: 11.608% (4175/35968)\n",
            "300 391 Loss: 2.300 | Acc: 11.628% (4480/38528)\n",
            "320 391 Loss: 2.300 | Acc: 11.612% (4771/41088)\n",
            "340 391 Loss: 2.300 | Acc: 11.620% (5072/43648)\n",
            "360 391 Loss: 2.300 | Acc: 11.600% (5360/46208)\n",
            "380 391 Loss: 2.300 | Acc: 11.612% (5663/48768)\n",
            "99 100 Loss: 2.492 | Acc: 9.958% (1000/10000)\n",
            "\n",
            "Epoch: 32\n",
            "0 391 Loss: 2.289 | Acc: 10.938% (14/128)\n",
            "20 391 Loss: 2.296 | Acc: 11.533% (310/2688)\n",
            "40 391 Loss: 2.298 | Acc: 11.185% (587/5248)\n",
            "60 391 Loss: 2.299 | Acc: 11.270% (880/7808)\n",
            "80 391 Loss: 2.299 | Acc: 11.372% (1179/10368)\n",
            "100 391 Loss: 2.300 | Acc: 11.417% (1476/12928)\n",
            "120 391 Loss: 2.300 | Acc: 11.364% (1760/15488)\n",
            "140 391 Loss: 2.300 | Acc: 11.220% (2025/18048)\n",
            "160 391 Loss: 2.300 | Acc: 11.200% (2308/20608)\n",
            "180 391 Loss: 2.300 | Acc: 11.184% (2591/23168)\n",
            "200 391 Loss: 2.299 | Acc: 11.229% (2889/25728)\n",
            "220 391 Loss: 2.299 | Acc: 11.309% (3199/28288)\n",
            "240 391 Loss: 2.299 | Acc: 11.343% (3499/30848)\n",
            "260 391 Loss: 2.300 | Acc: 11.354% (3793/33408)\n",
            "280 391 Loss: 2.300 | Acc: 11.204% (4030/35968)\n",
            "300 391 Loss: 2.300 | Acc: 11.218% (4322/38528)\n",
            "320 391 Loss: 2.300 | Acc: 11.247% (4621/41088)\n",
            "340 391 Loss: 2.300 | Acc: 11.212% (4894/43648)\n",
            "360 391 Loss: 2.300 | Acc: 11.191% (5171/46208)\n",
            "380 391 Loss: 2.300 | Acc: 11.165% (5445/48768)\n",
            "99 100 Loss: 2.451 | Acc: 9.958% (1000/10000)\n",
            "\n",
            "Epoch: 33\n",
            "0 391 Loss: 2.301 | Acc: 10.938% (14/128)\n",
            "20 391 Loss: 2.300 | Acc: 11.124% (299/2688)\n",
            "40 391 Loss: 2.299 | Acc: 11.623% (610/5248)\n",
            "60 391 Loss: 2.298 | Acc: 11.424% (892/7808)\n",
            "80 391 Loss: 2.299 | Acc: 11.516% (1194/10368)\n",
            "100 391 Loss: 2.300 | Acc: 11.347% (1467/12928)\n",
            "120 391 Loss: 2.301 | Acc: 11.299% (1750/15488)\n",
            "140 391 Loss: 2.300 | Acc: 11.287% (2037/18048)\n",
            "160 391 Loss: 2.300 | Acc: 11.408% (2351/20608)\n",
            "180 391 Loss: 2.300 | Acc: 11.421% (2646/23168)\n",
            "200 391 Loss: 2.300 | Acc: 11.447% (2945/25728)\n",
            "220 391 Loss: 2.300 | Acc: 11.485% (3249/28288)\n",
            "240 391 Loss: 2.300 | Acc: 11.469% (3538/30848)\n",
            "260 391 Loss: 2.300 | Acc: 11.413% (3813/33408)\n",
            "280 391 Loss: 2.300 | Acc: 11.352% (4083/35968)\n",
            "300 391 Loss: 2.300 | Acc: 11.314% (4359/38528)\n",
            "320 391 Loss: 2.300 | Acc: 11.359% (4667/41088)\n",
            "340 391 Loss: 2.300 | Acc: 11.377% (4966/43648)\n",
            "360 391 Loss: 2.300 | Acc: 11.316% (5229/46208)\n",
            "380 391 Loss: 2.300 | Acc: 11.344% (5532/48768)\n",
            "99 100 Loss: 2.465 | Acc: 9.946% (999/10000)\n",
            "\n",
            "Epoch: 34\n",
            "0 391 Loss: 2.293 | Acc: 17.188% (22/128)\n",
            "20 391 Loss: 2.295 | Acc: 11.756% (316/2688)\n",
            "40 391 Loss: 2.297 | Acc: 12.119% (636/5248)\n",
            "60 391 Loss: 2.297 | Acc: 12.039% (940/7808)\n",
            "80 391 Loss: 2.297 | Acc: 12.008% (1245/10368)\n",
            "100 391 Loss: 2.298 | Acc: 11.804% (1526/12928)\n",
            "120 391 Loss: 2.299 | Acc: 11.499% (1781/15488)\n",
            "140 391 Loss: 2.299 | Acc: 11.536% (2082/18048)\n",
            "160 391 Loss: 2.299 | Acc: 11.534% (2377/20608)\n",
            "180 391 Loss: 2.299 | Acc: 11.667% (2703/23168)\n",
            "200 391 Loss: 2.299 | Acc: 11.641% (2995/25728)\n",
            "220 391 Loss: 2.299 | Acc: 11.591% (3279/28288)\n",
            "240 391 Loss: 2.299 | Acc: 11.589% (3575/30848)\n",
            "260 391 Loss: 2.300 | Acc: 11.479% (3835/33408)\n",
            "280 391 Loss: 2.299 | Acc: 11.430% (4111/35968)\n",
            "300 391 Loss: 2.299 | Acc: 11.392% (4389/38528)\n",
            "320 391 Loss: 2.300 | Acc: 11.366% (4670/41088)\n",
            "340 391 Loss: 2.300 | Acc: 11.419% (4984/43648)\n",
            "360 391 Loss: 2.300 | Acc: 11.418% (5276/46208)\n",
            "380 391 Loss: 2.300 | Acc: 11.487% (5602/48768)\n",
            "99 100 Loss: 2.453 | Acc: 9.958% (1000/10000)\n",
            "\n",
            "Epoch: 35\n",
            "0 391 Loss: 2.321 | Acc: 7.031% (9/128)\n",
            "20 391 Loss: 2.306 | Acc: 9.859% (265/2688)\n",
            "40 391 Loss: 2.303 | Acc: 10.404% (546/5248)\n",
            "60 391 Loss: 2.302 | Acc: 10.733% (838/7808)\n",
            "80 391 Loss: 2.301 | Acc: 10.957% (1136/10368)\n",
            "100 391 Loss: 2.301 | Acc: 11.046% (1428/12928)\n",
            "120 391 Loss: 2.301 | Acc: 11.067% (1714/15488)\n",
            "140 391 Loss: 2.301 | Acc: 10.998% (1985/18048)\n",
            "160 391 Loss: 2.300 | Acc: 11.078% (2283/20608)\n",
            "180 391 Loss: 2.300 | Acc: 11.110% (2574/23168)\n",
            "200 391 Loss: 2.299 | Acc: 11.252% (2895/25728)\n",
            "220 391 Loss: 2.299 | Acc: 11.270% (3188/28288)\n",
            "240 391 Loss: 2.300 | Acc: 11.255% (3472/30848)\n",
            "260 391 Loss: 2.300 | Acc: 11.234% (3753/33408)\n",
            "280 391 Loss: 2.300 | Acc: 11.257% (4049/35968)\n",
            "300 391 Loss: 2.300 | Acc: 11.192% (4312/38528)\n",
            "320 391 Loss: 2.300 | Acc: 11.234% (4616/41088)\n",
            "340 391 Loss: 2.299 | Acc: 11.311% (4937/43648)\n",
            "360 391 Loss: 2.299 | Acc: 11.372% (5255/46208)\n",
            "380 391 Loss: 2.300 | Acc: 11.358% (5539/48768)\n",
            "99 100 Loss: 2.454 | Acc: 10.327% (1001/10000)\n",
            "\n",
            "Epoch: 36\n",
            "0 391 Loss: 2.306 | Acc: 13.281% (17/128)\n",
            "20 391 Loss: 2.305 | Acc: 10.826% (291/2688)\n",
            "40 391 Loss: 2.302 | Acc: 11.223% (589/5248)\n",
            "60 391 Loss: 2.301 | Acc: 11.155% (871/7808)\n",
            "80 391 Loss: 2.301 | Acc: 11.217% (1163/10368)\n",
            "100 391 Loss: 2.302 | Acc: 11.038% (1427/12928)\n",
            "120 391 Loss: 2.300 | Acc: 11.209% (1736/15488)\n",
            "140 391 Loss: 2.300 | Acc: 11.231% (2027/18048)\n",
            "160 391 Loss: 2.300 | Acc: 11.127% (2293/20608)\n",
            "180 391 Loss: 2.300 | Acc: 11.231% (2602/23168)\n",
            "200 391 Loss: 2.300 | Acc: 11.190% (2879/25728)\n",
            "220 391 Loss: 2.300 | Acc: 11.199% (3168/28288)\n",
            "240 391 Loss: 2.300 | Acc: 11.161% (3443/30848)\n",
            "260 391 Loss: 2.300 | Acc: 11.138% (3721/33408)\n",
            "280 391 Loss: 2.300 | Acc: 11.127% (4002/35968)\n",
            "300 391 Loss: 2.300 | Acc: 11.137% (4291/38528)\n",
            "320 391 Loss: 2.300 | Acc: 11.154% (4583/41088)\n",
            "340 391 Loss: 2.300 | Acc: 11.171% (4876/43648)\n",
            "360 391 Loss: 2.300 | Acc: 11.130% (5143/46208)\n",
            "380 391 Loss: 2.300 | Acc: 11.089% (5408/48768)\n",
            "99 100 Loss: 2.429 | Acc: 10.406% (1005/10000)\n",
            "\n",
            "Epoch: 37\n",
            "0 391 Loss: 2.304 | Acc: 11.719% (15/128)\n",
            "20 391 Loss: 2.301 | Acc: 10.975% (295/2688)\n",
            "40 391 Loss: 2.299 | Acc: 11.261% (591/5248)\n",
            "60 391 Loss: 2.298 | Acc: 11.539% (901/7808)\n",
            "80 391 Loss: 2.297 | Acc: 11.593% (1202/10368)\n",
            "100 391 Loss: 2.298 | Acc: 11.556% (1494/12928)\n",
            "120 391 Loss: 2.298 | Acc: 11.615% (1799/15488)\n",
            "140 391 Loss: 2.299 | Acc: 11.636% (2100/18048)\n",
            "160 391 Loss: 2.299 | Acc: 11.534% (2377/20608)\n",
            "180 391 Loss: 2.299 | Acc: 11.572% (2681/23168)\n",
            "200 391 Loss: 2.299 | Acc: 11.451% (2946/25728)\n",
            "220 391 Loss: 2.299 | Acc: 11.429% (3233/28288)\n",
            "240 391 Loss: 2.299 | Acc: 11.440% (3529/30848)\n",
            "260 391 Loss: 2.299 | Acc: 11.428% (3818/33408)\n",
            "280 391 Loss: 2.299 | Acc: 11.405% (4102/35968)\n",
            "300 391 Loss: 2.299 | Acc: 11.309% (4357/38528)\n",
            "320 391 Loss: 2.300 | Acc: 11.256% (4625/41088)\n",
            "340 391 Loss: 2.300 | Acc: 11.224% (4899/43648)\n",
            "360 391 Loss: 2.300 | Acc: 11.236% (5192/46208)\n",
            "380 391 Loss: 2.299 | Acc: 11.257% (5490/48768)\n",
            "99 100 Loss: 2.426 | Acc: 10.965% (1073/10000)\n",
            "\n",
            "Epoch: 38\n",
            "0 391 Loss: 2.297 | Acc: 11.719% (15/128)\n",
            "20 391 Loss: 2.303 | Acc: 10.156% (273/2688)\n",
            "40 391 Loss: 2.302 | Acc: 10.995% (577/5248)\n",
            "60 391 Loss: 2.301 | Acc: 11.002% (859/7808)\n",
            "80 391 Loss: 2.302 | Acc: 10.860% (1126/10368)\n",
            "100 391 Loss: 2.301 | Acc: 11.108% (1436/12928)\n",
            "120 391 Loss: 2.301 | Acc: 11.247% (1742/15488)\n",
            "140 391 Loss: 2.300 | Acc: 11.325% (2044/18048)\n",
            "160 391 Loss: 2.300 | Acc: 11.311% (2331/20608)\n",
            "180 391 Loss: 2.299 | Acc: 11.270% (2611/23168)\n",
            "200 391 Loss: 2.299 | Acc: 11.334% (2916/25728)\n",
            "220 391 Loss: 2.299 | Acc: 11.340% (3208/28288)\n",
            "240 391 Loss: 2.300 | Acc: 11.197% (3454/30848)\n",
            "260 391 Loss: 2.300 | Acc: 11.180% (3735/33408)\n",
            "280 391 Loss: 2.300 | Acc: 11.149% (4010/35968)\n",
            "300 391 Loss: 2.300 | Acc: 11.143% (4293/38528)\n",
            "320 391 Loss: 2.300 | Acc: 11.159% (4585/41088)\n",
            "340 391 Loss: 2.300 | Acc: 11.128% (4857/43648)\n",
            "360 391 Loss: 2.300 | Acc: 11.115% (5136/46208)\n",
            "380 391 Loss: 2.300 | Acc: 11.126% (5426/48768)\n",
            "99 100 Loss: 2.400 | Acc: 9.958% (1000/10000)\n",
            "\n",
            "Epoch: 39\n",
            "0 391 Loss: 2.314 | Acc: 9.375% (12/128)\n",
            "20 391 Loss: 2.296 | Acc: 11.421% (307/2688)\n",
            "40 391 Loss: 2.299 | Acc: 10.861% (570/5248)\n",
            "60 391 Loss: 2.300 | Acc: 10.809% (844/7808)\n",
            "80 391 Loss: 2.301 | Acc: 10.571% (1096/10368)\n",
            "100 391 Loss: 2.300 | Acc: 10.528% (1361/12928)\n",
            "120 391 Loss: 2.300 | Acc: 10.595% (1641/15488)\n",
            "140 391 Loss: 2.300 | Acc: 10.677% (1927/18048)\n",
            "160 391 Loss: 2.300 | Acc: 10.743% (2214/20608)\n",
            "180 391 Loss: 2.300 | Acc: 10.739% (2488/23168)\n",
            "200 391 Loss: 2.300 | Acc: 10.720% (2758/25728)\n",
            "220 391 Loss: 2.300 | Acc: 10.729% (3035/28288)\n",
            "240 391 Loss: 2.300 | Acc: 10.876% (3355/30848)\n",
            "260 391 Loss: 2.300 | Acc: 10.905% (3643/33408)\n",
            "280 391 Loss: 2.300 | Acc: 10.990% (3953/35968)\n",
            "300 391 Loss: 2.300 | Acc: 11.008% (4241/38528)\n",
            "320 391 Loss: 2.300 | Acc: 11.020% (4528/41088)\n",
            "340 391 Loss: 2.300 | Acc: 11.004% (4803/43648)\n",
            "360 391 Loss: 2.300 | Acc: 11.039% (5101/46208)\n",
            "380 391 Loss: 2.300 | Acc: 11.044% (5386/48768)\n",
            "99 100 Loss: 2.413 | Acc: 9.958% (1000/10000)\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "======= start test group: 4 =======\n",
            "\n",
            "Epoch: 0\n",
            "0 391 Loss: 2.168 | Acc: 21.875% (28/128)\n",
            "20 391 Loss: 2.178 | Acc: 23.475% (631/2688)\n",
            "40 391 Loss: 2.139 | Acc: 25.229% (1324/5248)\n",
            "60 391 Loss: 2.096 | Acc: 27.152% (2120/7808)\n",
            "80 391 Loss: 2.053 | Acc: 29.070% (3014/10368)\n",
            "100 391 Loss: 2.001 | Acc: 31.088% (4019/12928)\n",
            "120 391 Loss: 1.951 | Acc: 32.541% (5040/15488)\n",
            "140 391 Loss: 1.899 | Acc: 34.342% (6198/18048)\n",
            "160 391 Loss: 1.844 | Acc: 36.384% (7498/20608)\n",
            "180 391 Loss: 1.793 | Acc: 38.186% (8847/23168)\n",
            "200 391 Loss: 1.741 | Acc: 39.929% (10273/25728)\n",
            "220 391 Loss: 1.688 | Acc: 41.859% (11841/28288)\n",
            "240 391 Loss: 1.635 | Acc: 43.766% (13501/30848)\n",
            "260 391 Loss: 1.586 | Acc: 45.459% (15187/33408)\n",
            "280 391 Loss: 1.540 | Acc: 47.056% (16925/35968)\n",
            "300 391 Loss: 1.495 | Acc: 48.658% (18747/38528)\n",
            "320 391 Loss: 1.453 | Acc: 50.141% (20602/41088)\n",
            "340 391 Loss: 1.416 | Acc: 51.436% (22451/43648)\n",
            "360 391 Loss: 1.381 | Acc: 52.623% (24316/46208)\n",
            "380 391 Loss: 1.348 | Acc: 53.738% (26207/48768)\n",
            "99 100 Loss: 0.527 | Acc: 82.639% (8221/10000)\n",
            "\n",
            "Epoch: 1\n",
            "0 391 Loss: 0.519 | Acc: 86.719% (111/128)\n",
            "20 391 Loss: 0.706 | Acc: 76.786% (2064/2688)\n",
            "40 391 Loss: 0.685 | Acc: 77.039% (4043/5248)\n",
            "60 391 Loss: 0.684 | Acc: 77.126% (6022/7808)\n",
            "80 391 Loss: 0.673 | Acc: 77.488% (8034/10368)\n",
            "100 391 Loss: 0.671 | Acc: 77.429% (10010/12928)\n",
            "120 391 Loss: 0.664 | Acc: 77.679% (12031/15488)\n",
            "140 391 Loss: 0.661 | Acc: 77.820% (14045/18048)\n",
            "160 391 Loss: 0.658 | Acc: 77.819% (16037/20608)\n",
            "180 391 Loss: 0.652 | Acc: 77.965% (18063/23168)\n",
            "200 391 Loss: 0.647 | Acc: 78.125% (20100/25728)\n",
            "220 391 Loss: 0.645 | Acc: 78.136% (22103/28288)\n",
            "240 391 Loss: 0.644 | Acc: 78.232% (24133/30848)\n",
            "260 391 Loss: 0.639 | Acc: 78.394% (26190/33408)\n",
            "280 391 Loss: 0.636 | Acc: 78.475% (28226/35968)\n",
            "300 391 Loss: 0.635 | Acc: 78.520% (30252/38528)\n",
            "320 391 Loss: 0.633 | Acc: 78.641% (32312/41088)\n",
            "340 391 Loss: 0.629 | Acc: 78.730% (34364/43648)\n",
            "360 391 Loss: 0.627 | Acc: 78.792% (36408/46208)\n",
            "380 391 Loss: 0.623 | Acc: 78.972% (38513/48768)\n",
            "99 100 Loss: 0.392 | Acc: 86.798% (8652/10000)\n",
            "\n",
            "Epoch: 2\n",
            "0 391 Loss: 0.443 | Acc: 83.594% (107/128)\n",
            "20 391 Loss: 0.545 | Acc: 81.473% (2190/2688)\n",
            "40 391 Loss: 0.531 | Acc: 81.784% (4292/5248)\n",
            "60 391 Loss: 0.532 | Acc: 81.737% (6382/7808)\n",
            "80 391 Loss: 0.527 | Acc: 82.079% (8510/10368)\n",
            "100 391 Loss: 0.521 | Acc: 82.364% (10648/12928)\n",
            "120 391 Loss: 0.521 | Acc: 82.315% (12749/15488)\n",
            "140 391 Loss: 0.521 | Acc: 82.330% (14859/18048)\n",
            "160 391 Loss: 0.518 | Acc: 82.497% (17001/20608)\n",
            "180 391 Loss: 0.515 | Acc: 82.519% (19118/23168)\n",
            "200 391 Loss: 0.514 | Acc: 82.583% (21247/25728)\n",
            "220 391 Loss: 0.512 | Acc: 82.625% (23373/28288)\n",
            "240 391 Loss: 0.509 | Acc: 82.754% (25528/30848)\n",
            "260 391 Loss: 0.507 | Acc: 82.818% (27668/33408)\n",
            "280 391 Loss: 0.508 | Acc: 82.812% (29786/35968)\n",
            "300 391 Loss: 0.507 | Acc: 82.836% (31915/38528)\n",
            "320 391 Loss: 0.507 | Acc: 82.847% (34040/41088)\n",
            "340 391 Loss: 0.505 | Acc: 82.884% (36177/43648)\n",
            "360 391 Loss: 0.504 | Acc: 82.947% (38328/46208)\n",
            "380 391 Loss: 0.502 | Acc: 83.038% (40496/48768)\n",
            "99 100 Loss: 0.343 | Acc: 88.316% (8845/10000)\n",
            "\n",
            "Epoch: 3\n",
            "0 391 Loss: 0.326 | Acc: 89.844% (115/128)\n",
            "20 391 Loss: 0.421 | Acc: 85.826% (2307/2688)\n",
            "40 391 Loss: 0.422 | Acc: 85.766% (4501/5248)\n",
            "60 391 Loss: 0.422 | Acc: 85.720% (6693/7808)\n",
            "80 391 Loss: 0.429 | Acc: 85.484% (8863/10368)\n",
            "100 391 Loss: 0.434 | Acc: 85.342% (11033/12928)\n",
            "120 391 Loss: 0.431 | Acc: 85.505% (13243/15488)\n",
            "140 391 Loss: 0.433 | Acc: 85.428% (15418/18048)\n",
            "160 391 Loss: 0.431 | Acc: 85.501% (17620/20608)\n",
            "180 391 Loss: 0.431 | Acc: 85.441% (19795/23168)\n",
            "200 391 Loss: 0.433 | Acc: 85.327% (21953/25728)\n",
            "220 391 Loss: 0.434 | Acc: 85.298% (24129/28288)\n",
            "240 391 Loss: 0.434 | Acc: 85.296% (26312/30848)\n",
            "260 391 Loss: 0.436 | Acc: 85.234% (28475/33408)\n",
            "280 391 Loss: 0.435 | Acc: 85.301% (30681/35968)\n",
            "300 391 Loss: 0.434 | Acc: 85.356% (32886/38528)\n",
            "320 391 Loss: 0.433 | Acc: 85.370% (35077/41088)\n",
            "340 391 Loss: 0.432 | Acc: 85.401% (37276/43648)\n",
            "360 391 Loss: 0.431 | Acc: 85.427% (39474/46208)\n",
            "380 391 Loss: 0.431 | Acc: 85.417% (41656/48768)\n",
            "99 100 Loss: 0.325 | Acc: 88.746% (8861/10000)\n",
            "\n",
            "Epoch: 4\n",
            "0 391 Loss: 0.315 | Acc: 89.062% (114/128)\n",
            "20 391 Loss: 0.400 | Acc: 86.421% (2323/2688)\n",
            "40 391 Loss: 0.402 | Acc: 86.204% (4524/5248)\n",
            "60 391 Loss: 0.393 | Acc: 86.475% (6752/7808)\n",
            "80 391 Loss: 0.389 | Acc: 86.516% (8970/10368)\n",
            "100 391 Loss: 0.386 | Acc: 86.649% (11202/12928)\n",
            "120 391 Loss: 0.390 | Acc: 86.544% (13404/15488)\n",
            "140 391 Loss: 0.392 | Acc: 86.547% (15620/18048)\n",
            "160 391 Loss: 0.390 | Acc: 86.651% (17857/20608)\n",
            "180 391 Loss: 0.389 | Acc: 86.706% (20088/23168)\n",
            "200 391 Loss: 0.388 | Acc: 86.719% (22311/25728)\n",
            "220 391 Loss: 0.390 | Acc: 86.680% (24520/28288)\n",
            "240 391 Loss: 0.389 | Acc: 86.719% (26751/30848)\n",
            "260 391 Loss: 0.388 | Acc: 86.737% (28977/33408)\n",
            "280 391 Loss: 0.387 | Acc: 86.722% (31192/35968)\n",
            "300 391 Loss: 0.387 | Acc: 86.750% (33423/38528)\n",
            "320 391 Loss: 0.388 | Acc: 86.750% (35644/41088)\n",
            "340 391 Loss: 0.388 | Acc: 86.728% (37855/43648)\n",
            "360 391 Loss: 0.389 | Acc: 86.712% (40068/46208)\n",
            "380 391 Loss: 0.390 | Acc: 86.647% (42256/48768)\n",
            "99 100 Loss: 0.316 | Acc: 89.119% (8902/10000)\n",
            "\n",
            "Epoch: 5\n",
            "0 391 Loss: 0.358 | Acc: 87.500% (112/128)\n",
            "20 391 Loss: 0.364 | Acc: 88.616% (2382/2688)\n",
            "40 391 Loss: 0.366 | Acc: 88.148% (4626/5248)\n",
            "60 391 Loss: 0.362 | Acc: 88.140% (6882/7808)\n",
            "80 391 Loss: 0.351 | Acc: 88.407% (9166/10368)\n",
            "100 391 Loss: 0.355 | Acc: 88.243% (11408/12928)\n",
            "120 391 Loss: 0.357 | Acc: 88.197% (13660/15488)\n",
            "140 391 Loss: 0.356 | Acc: 88.143% (15908/18048)\n",
            "160 391 Loss: 0.358 | Acc: 88.082% (18152/20608)\n",
            "180 391 Loss: 0.357 | Acc: 88.117% (20415/23168)\n",
            "200 391 Loss: 0.356 | Acc: 88.149% (22679/25728)\n",
            "220 391 Loss: 0.354 | Acc: 88.239% (24961/28288)\n",
            "240 391 Loss: 0.351 | Acc: 88.346% (27253/30848)\n",
            "260 391 Loss: 0.351 | Acc: 88.320% (29506/33408)\n",
            "280 391 Loss: 0.353 | Acc: 88.284% (31754/35968)\n",
            "300 391 Loss: 0.351 | Acc: 88.305% (34022/38528)\n",
            "320 391 Loss: 0.350 | Acc: 88.374% (36311/41088)\n",
            "340 391 Loss: 0.350 | Acc: 88.355% (38565/43648)\n",
            "360 391 Loss: 0.350 | Acc: 88.335% (40818/46208)\n",
            "380 391 Loss: 0.349 | Acc: 88.341% (43082/48768)\n",
            "99 100 Loss: 0.311 | Acc: 89.722% (8967/10000)\n",
            "\n",
            "Epoch: 6\n",
            "0 391 Loss: 0.467 | Acc: 85.938% (110/128)\n",
            "20 391 Loss: 0.308 | Acc: 90.030% (2420/2688)\n",
            "40 391 Loss: 0.318 | Acc: 89.615% (4703/5248)\n",
            "60 391 Loss: 0.313 | Acc: 89.652% (7000/7808)\n",
            "80 391 Loss: 0.315 | Acc: 89.603% (9290/10368)\n",
            "100 391 Loss: 0.315 | Acc: 89.612% (11585/12928)\n",
            "120 391 Loss: 0.315 | Acc: 89.637% (13883/15488)\n",
            "140 391 Loss: 0.314 | Acc: 89.633% (16177/18048)\n",
            "160 391 Loss: 0.315 | Acc: 89.562% (18457/20608)\n",
            "180 391 Loss: 0.315 | Acc: 89.628% (20765/23168)\n",
            "200 391 Loss: 0.317 | Acc: 89.537% (23036/25728)\n",
            "220 391 Loss: 0.316 | Acc: 89.533% (25327/28288)\n",
            "240 391 Loss: 0.316 | Acc: 89.523% (27616/30848)\n",
            "260 391 Loss: 0.316 | Acc: 89.488% (29896/33408)\n",
            "280 391 Loss: 0.315 | Acc: 89.535% (32204/35968)\n",
            "300 391 Loss: 0.315 | Acc: 89.550% (34502/38528)\n",
            "320 391 Loss: 0.315 | Acc: 89.566% (36801/41088)\n",
            "340 391 Loss: 0.315 | Acc: 89.585% (39102/43648)\n",
            "360 391 Loss: 0.316 | Acc: 89.523% (41367/46208)\n",
            "380 391 Loss: 0.315 | Acc: 89.551% (43672/48768)\n",
            "99 100 Loss: 0.303 | Acc: 89.860% (8963/10000)\n",
            "\n",
            "Epoch: 7\n",
            "0 391 Loss: 0.188 | Acc: 94.531% (121/128)\n",
            "20 391 Loss: 0.270 | Acc: 91.406% (2457/2688)\n",
            "40 391 Loss: 0.284 | Acc: 90.434% (4746/5248)\n",
            "60 391 Loss: 0.286 | Acc: 90.420% (7060/7808)\n",
            "80 391 Loss: 0.287 | Acc: 90.365% (9369/10368)\n",
            "100 391 Loss: 0.291 | Acc: 90.300% (11674/12928)\n",
            "120 391 Loss: 0.287 | Acc: 90.567% (14027/15488)\n",
            "140 391 Loss: 0.287 | Acc: 90.486% (16331/18048)\n",
            "160 391 Loss: 0.286 | Acc: 90.470% (18644/20608)\n",
            "180 391 Loss: 0.286 | Acc: 90.487% (20964/23168)\n",
            "200 391 Loss: 0.284 | Acc: 90.505% (23285/25728)\n",
            "220 391 Loss: 0.285 | Acc: 90.406% (25574/28288)\n",
            "240 391 Loss: 0.287 | Acc: 90.395% (27885/30848)\n",
            "260 391 Loss: 0.287 | Acc: 90.412% (30205/33408)\n",
            "280 391 Loss: 0.288 | Acc: 90.383% (32509/35968)\n",
            "300 391 Loss: 0.287 | Acc: 90.399% (34829/38528)\n",
            "320 391 Loss: 0.286 | Acc: 90.430% (37156/41088)\n",
            "340 391 Loss: 0.287 | Acc: 90.407% (39461/43648)\n",
            "360 391 Loss: 0.287 | Acc: 90.417% (41780/46208)\n",
            "380 391 Loss: 0.286 | Acc: 90.471% (44121/48768)\n",
            "99 100 Loss: 0.300 | Acc: 89.843% (8965/10000)\n",
            "\n",
            "Epoch: 8\n",
            "0 391 Loss: 0.241 | Acc: 90.625% (116/128)\n",
            "20 391 Loss: 0.270 | Acc: 91.592% (2462/2688)\n",
            "40 391 Loss: 0.270 | Acc: 91.311% (4792/5248)\n",
            "60 391 Loss: 0.264 | Acc: 91.329% (7131/7808)\n",
            "80 391 Loss: 0.266 | Acc: 91.223% (9458/10368)\n",
            "100 391 Loss: 0.263 | Acc: 91.375% (11813/12928)\n",
            "120 391 Loss: 0.266 | Acc: 91.271% (14136/15488)\n",
            "140 391 Loss: 0.267 | Acc: 91.229% (16465/18048)\n",
            "160 391 Loss: 0.268 | Acc: 91.212% (18797/20608)\n",
            "180 391 Loss: 0.265 | Acc: 91.329% (21159/23168)\n",
            "200 391 Loss: 0.264 | Acc: 91.344% (23501/25728)\n",
            "220 391 Loss: 0.265 | Acc: 91.286% (25823/28288)\n",
            "240 391 Loss: 0.266 | Acc: 91.221% (28140/30848)\n",
            "260 391 Loss: 0.267 | Acc: 91.221% (30475/33408)\n",
            "280 391 Loss: 0.269 | Acc: 91.159% (32788/35968)\n",
            "300 391 Loss: 0.269 | Acc: 91.142% (35115/38528)\n",
            "320 391 Loss: 0.268 | Acc: 91.158% (37455/41088)\n",
            "340 391 Loss: 0.270 | Acc: 91.118% (39771/43648)\n",
            "360 391 Loss: 0.271 | Acc: 91.064% (42079/46208)\n",
            "380 391 Loss: 0.271 | Acc: 91.066% (44411/48768)\n",
            "99 100 Loss: 0.298 | Acc: 90.215% (8997/10000)\n",
            "\n",
            "Epoch: 9\n",
            "0 391 Loss: 0.213 | Acc: 92.969% (119/128)\n",
            "20 391 Loss: 0.264 | Acc: 90.923% (2444/2688)\n",
            "40 391 Loss: 0.253 | Acc: 91.597% (4807/5248)\n",
            "60 391 Loss: 0.249 | Acc: 91.534% (7147/7808)\n",
            "80 391 Loss: 0.248 | Acc: 91.599% (9497/10368)\n",
            "100 391 Loss: 0.250 | Acc: 91.576% (11839/12928)\n",
            "120 391 Loss: 0.253 | Acc: 91.516% (14174/15488)\n",
            "140 391 Loss: 0.251 | Acc: 91.578% (16528/18048)\n",
            "160 391 Loss: 0.252 | Acc: 91.494% (18855/20608)\n",
            "180 391 Loss: 0.251 | Acc: 91.557% (21212/23168)\n",
            "200 391 Loss: 0.252 | Acc: 91.535% (23550/25728)\n",
            "220 391 Loss: 0.255 | Acc: 91.470% (25875/28288)\n",
            "240 391 Loss: 0.257 | Acc: 91.409% (28198/30848)\n",
            "260 391 Loss: 0.257 | Acc: 91.382% (30529/33408)\n",
            "280 391 Loss: 0.257 | Acc: 91.412% (32879/35968)\n",
            "300 391 Loss: 0.257 | Acc: 91.432% (35227/38528)\n",
            "320 391 Loss: 0.257 | Acc: 91.411% (37559/41088)\n",
            "340 391 Loss: 0.258 | Acc: 91.402% (39895/43648)\n",
            "360 391 Loss: 0.258 | Acc: 91.356% (42214/46208)\n",
            "380 391 Loss: 0.258 | Acc: 91.341% (44545/48768)\n",
            "99 100 Loss: 0.307 | Acc: 90.242% (9005/10000)\n",
            "\n",
            "Epoch: 10\n",
            "0 391 Loss: 0.194 | Acc: 94.531% (121/128)\n",
            "20 391 Loss: 0.229 | Acc: 92.783% (2494/2688)\n",
            "40 391 Loss: 0.233 | Acc: 92.207% (4839/5248)\n",
            "60 391 Loss: 0.232 | Acc: 92.456% (7219/7808)\n",
            "80 391 Loss: 0.231 | Acc: 92.525% (9593/10368)\n",
            "100 391 Loss: 0.229 | Acc: 92.567% (11967/12928)\n",
            "120 391 Loss: 0.231 | Acc: 92.388% (14309/15488)\n",
            "140 391 Loss: 0.230 | Acc: 92.431% (16682/18048)\n",
            "160 391 Loss: 0.230 | Acc: 92.445% (19051/20608)\n",
            "180 391 Loss: 0.231 | Acc: 92.399% (21407/23168)\n",
            "200 391 Loss: 0.234 | Acc: 92.304% (23748/25728)\n",
            "220 391 Loss: 0.235 | Acc: 92.262% (26099/28288)\n",
            "240 391 Loss: 0.237 | Acc: 92.197% (28441/30848)\n",
            "260 391 Loss: 0.238 | Acc: 92.185% (30797/33408)\n",
            "280 391 Loss: 0.237 | Acc: 92.171% (33152/35968)\n",
            "300 391 Loss: 0.239 | Acc: 92.097% (35483/38528)\n",
            "320 391 Loss: 0.238 | Acc: 92.153% (37864/41088)\n",
            "340 391 Loss: 0.239 | Acc: 92.112% (40205/43648)\n",
            "360 391 Loss: 0.240 | Acc: 92.064% (42541/46208)\n",
            "380 391 Loss: 0.240 | Acc: 92.077% (44904/48768)\n",
            "99 100 Loss: 0.309 | Acc: 89.924% (8995/10000)\n",
            "\n",
            "Epoch: 11\n",
            "0 391 Loss: 0.246 | Acc: 92.969% (119/128)\n",
            "20 391 Loss: 0.227 | Acc: 92.299% (2481/2688)\n",
            "40 391 Loss: 0.227 | Acc: 92.435% (4851/5248)\n",
            "60 391 Loss: 0.225 | Acc: 92.649% (7234/7808)\n",
            "80 391 Loss: 0.222 | Acc: 92.805% (9622/10368)\n",
            "100 391 Loss: 0.223 | Acc: 92.713% (11986/12928)\n",
            "120 391 Loss: 0.226 | Acc: 92.594% (14341/15488)\n",
            "140 391 Loss: 0.226 | Acc: 92.653% (16722/18048)\n",
            "160 391 Loss: 0.226 | Acc: 92.648% (19093/20608)\n",
            "180 391 Loss: 0.224 | Acc: 92.766% (21492/23168)\n",
            "200 391 Loss: 0.225 | Acc: 92.747% (23862/25728)\n",
            "220 391 Loss: 0.224 | Acc: 92.746% (26236/28288)\n",
            "240 391 Loss: 0.226 | Acc: 92.680% (28590/30848)\n",
            "260 391 Loss: 0.225 | Acc: 92.684% (30964/33408)\n",
            "280 391 Loss: 0.226 | Acc: 92.618% (33313/35968)\n",
            "300 391 Loss: 0.226 | Acc: 92.577% (35668/38528)\n",
            "320 391 Loss: 0.226 | Acc: 92.565% (38033/41088)\n",
            "340 391 Loss: 0.226 | Acc: 92.570% (40405/43648)\n",
            "360 391 Loss: 0.226 | Acc: 92.566% (42773/46208)\n",
            "380 391 Loss: 0.227 | Acc: 92.524% (45122/48768)\n",
            "99 100 Loss: 0.316 | Acc: 89.803% (8945/10000)\n",
            "\n",
            "Epoch: 12\n",
            "0 391 Loss: 0.255 | Acc: 89.844% (115/128)\n",
            "20 391 Loss: 0.206 | Acc: 93.118% (2503/2688)\n",
            "40 391 Loss: 0.211 | Acc: 93.140% (4888/5248)\n",
            "60 391 Loss: 0.215 | Acc: 92.969% (7259/7808)\n",
            "80 391 Loss: 0.215 | Acc: 92.949% (9637/10368)\n",
            "100 391 Loss: 0.214 | Acc: 92.899% (12010/12928)\n",
            "120 391 Loss: 0.214 | Acc: 92.930% (14393/15488)\n",
            "140 391 Loss: 0.213 | Acc: 92.958% (16777/18048)\n",
            "160 391 Loss: 0.214 | Acc: 92.993% (19164/20608)\n",
            "180 391 Loss: 0.215 | Acc: 92.921% (21528/23168)\n",
            "200 391 Loss: 0.215 | Acc: 92.907% (23903/25728)\n",
            "220 391 Loss: 0.216 | Acc: 92.880% (26274/28288)\n",
            "240 391 Loss: 0.215 | Acc: 92.927% (28666/30848)\n",
            "260 391 Loss: 0.216 | Acc: 92.945% (31051/33408)\n",
            "280 391 Loss: 0.217 | Acc: 92.924% (33423/35968)\n",
            "300 391 Loss: 0.216 | Acc: 92.987% (35826/38528)\n",
            "320 391 Loss: 0.217 | Acc: 92.947% (38190/41088)\n",
            "340 391 Loss: 0.218 | Acc: 92.960% (40575/43648)\n",
            "360 391 Loss: 0.219 | Acc: 92.902% (42928/46208)\n",
            "380 391 Loss: 0.219 | Acc: 92.913% (45312/48768)\n",
            "99 100 Loss: 0.322 | Acc: 89.678% (8966/10000)\n",
            "\n",
            "Epoch: 13\n",
            "0 391 Loss: 0.184 | Acc: 94.531% (121/128)\n",
            "20 391 Loss: 0.195 | Acc: 93.676% (2518/2688)\n",
            "40 391 Loss: 0.185 | Acc: 94.188% (4943/5248)\n",
            "60 391 Loss: 0.192 | Acc: 93.904% (7332/7808)\n",
            "80 391 Loss: 0.191 | Acc: 93.866% (9732/10368)\n",
            "100 391 Loss: 0.193 | Acc: 93.634% (12105/12928)\n",
            "120 391 Loss: 0.198 | Acc: 93.505% (14482/15488)\n",
            "140 391 Loss: 0.202 | Acc: 93.346% (16847/18048)\n",
            "160 391 Loss: 0.203 | Acc: 93.318% (19231/20608)\n",
            "180 391 Loss: 0.204 | Acc: 93.258% (21606/23168)\n",
            "200 391 Loss: 0.205 | Acc: 93.291% (24002/25728)\n",
            "220 391 Loss: 0.206 | Acc: 93.230% (26373/28288)\n",
            "240 391 Loss: 0.207 | Acc: 93.199% (28750/30848)\n",
            "260 391 Loss: 0.206 | Acc: 93.268% (31159/33408)\n",
            "280 391 Loss: 0.206 | Acc: 93.236% (33535/35968)\n",
            "300 391 Loss: 0.206 | Acc: 93.239% (35923/38528)\n",
            "320 391 Loss: 0.207 | Acc: 93.190% (38290/41088)\n",
            "340 391 Loss: 0.208 | Acc: 93.186% (40674/43648)\n",
            "360 391 Loss: 0.208 | Acc: 93.170% (43052/46208)\n",
            "380 391 Loss: 0.208 | Acc: 93.172% (45438/48768)\n",
            "99 100 Loss: 0.304 | Acc: 90.509% (9022/10000)\n",
            "\n",
            "Epoch: 14\n",
            "0 391 Loss: 0.190 | Acc: 93.750% (120/128)\n",
            "20 391 Loss: 0.206 | Acc: 93.155% (2504/2688)\n",
            "40 391 Loss: 0.207 | Acc: 93.026% (4882/5248)\n",
            "60 391 Loss: 0.201 | Acc: 93.251% (7281/7808)\n",
            "80 391 Loss: 0.200 | Acc: 93.422% (9686/10368)\n",
            "100 391 Loss: 0.196 | Acc: 93.580% (12098/12928)\n",
            "120 391 Loss: 0.201 | Acc: 93.434% (14471/15488)\n",
            "140 391 Loss: 0.200 | Acc: 93.467% (16869/18048)\n",
            "160 391 Loss: 0.198 | Acc: 93.541% (19277/20608)\n",
            "180 391 Loss: 0.196 | Acc: 93.616% (21689/23168)\n",
            "200 391 Loss: 0.197 | Acc: 93.567% (24073/25728)\n",
            "220 391 Loss: 0.197 | Acc: 93.534% (26459/28288)\n",
            "240 391 Loss: 0.196 | Acc: 93.591% (28871/30848)\n",
            "260 391 Loss: 0.195 | Acc: 93.621% (31277/33408)\n",
            "280 391 Loss: 0.196 | Acc: 93.605% (33668/35968)\n",
            "300 391 Loss: 0.197 | Acc: 93.550% (36043/38528)\n",
            "320 391 Loss: 0.197 | Acc: 93.526% (38428/41088)\n",
            "340 391 Loss: 0.197 | Acc: 93.544% (40830/43648)\n",
            "360 391 Loss: 0.197 | Acc: 93.538% (43222/46208)\n",
            "380 391 Loss: 0.197 | Acc: 93.516% (45606/48768)\n",
            "99 100 Loss: 0.307 | Acc: 90.166% (9014/10000)\n",
            "\n",
            "Epoch: 15\n",
            "0 391 Loss: 0.154 | Acc: 94.531% (121/128)\n",
            "20 391 Loss: 0.213 | Acc: 93.266% (2507/2688)\n",
            "40 391 Loss: 0.196 | Acc: 93.636% (4914/5248)\n",
            "60 391 Loss: 0.190 | Acc: 93.724% (7318/7808)\n",
            "80 391 Loss: 0.188 | Acc: 93.760% (9721/10368)\n",
            "100 391 Loss: 0.186 | Acc: 93.881% (12137/12928)\n",
            "120 391 Loss: 0.185 | Acc: 93.905% (14544/15488)\n",
            "140 391 Loss: 0.189 | Acc: 93.783% (16926/18048)\n",
            "160 391 Loss: 0.189 | Acc: 93.750% (19320/20608)\n",
            "180 391 Loss: 0.190 | Acc: 93.746% (21719/23168)\n",
            "200 391 Loss: 0.190 | Acc: 93.754% (24121/25728)\n",
            "220 391 Loss: 0.190 | Acc: 93.761% (26523/28288)\n",
            "240 391 Loss: 0.191 | Acc: 93.756% (28922/30848)\n",
            "260 391 Loss: 0.191 | Acc: 93.723% (31311/33408)\n",
            "280 391 Loss: 0.192 | Acc: 93.692% (33699/35968)\n",
            "300 391 Loss: 0.194 | Acc: 93.644% (36079/38528)\n",
            "320 391 Loss: 0.193 | Acc: 93.694% (38497/41088)\n",
            "340 391 Loss: 0.192 | Acc: 93.690% (40894/43648)\n",
            "360 391 Loss: 0.193 | Acc: 93.696% (43295/46208)\n",
            "380 391 Loss: 0.193 | Acc: 93.703% (45697/48768)\n",
            "99 100 Loss: 0.304 | Acc: 90.433% (9078/10000)\n",
            "\n",
            "Epoch: 16\n",
            "0 391 Loss: 0.125 | Acc: 94.531% (121/128)\n",
            "20 391 Loss: 0.198 | Acc: 93.824% (2522/2688)\n",
            "40 391 Loss: 0.188 | Acc: 94.036% (4935/5248)\n",
            "60 391 Loss: 0.177 | Acc: 94.339% (7366/7808)\n",
            "80 391 Loss: 0.179 | Acc: 94.319% (9779/10368)\n",
            "100 391 Loss: 0.175 | Acc: 94.454% (12211/12928)\n",
            "120 391 Loss: 0.178 | Acc: 94.325% (14609/15488)\n",
            "140 391 Loss: 0.176 | Acc: 94.337% (17026/18048)\n",
            "160 391 Loss: 0.178 | Acc: 94.337% (19441/20608)\n",
            "180 391 Loss: 0.177 | Acc: 94.333% (21855/23168)\n",
            "200 391 Loss: 0.178 | Acc: 94.306% (24263/25728)\n",
            "220 391 Loss: 0.178 | Acc: 94.305% (26677/28288)\n",
            "240 391 Loss: 0.179 | Acc: 94.298% (29089/30848)\n",
            "260 391 Loss: 0.179 | Acc: 94.277% (31496/33408)\n",
            "280 391 Loss: 0.179 | Acc: 94.253% (33901/35968)\n",
            "300 391 Loss: 0.179 | Acc: 94.264% (36318/38528)\n",
            "320 391 Loss: 0.178 | Acc: 94.327% (38757/41088)\n",
            "340 391 Loss: 0.178 | Acc: 94.300% (41160/43648)\n",
            "360 391 Loss: 0.179 | Acc: 94.263% (43557/46208)\n",
            "380 391 Loss: 0.179 | Acc: 94.248% (45963/48768)\n",
            "99 100 Loss: 0.310 | Acc: 90.527% (9050/10000)\n",
            "\n",
            "Epoch: 17\n",
            "0 391 Loss: 0.144 | Acc: 95.312% (122/128)\n",
            "20 391 Loss: 0.169 | Acc: 94.457% (2539/2688)\n",
            "40 391 Loss: 0.175 | Acc: 94.341% (4951/5248)\n",
            "60 391 Loss: 0.181 | Acc: 94.070% (7345/7808)\n",
            "80 391 Loss: 0.182 | Acc: 94.078% (9754/10368)\n",
            "100 391 Loss: 0.183 | Acc: 93.998% (12152/12928)\n",
            "120 391 Loss: 0.184 | Acc: 93.957% (14552/15488)\n",
            "140 391 Loss: 0.184 | Acc: 93.916% (16950/18048)\n",
            "160 391 Loss: 0.184 | Acc: 93.910% (19353/20608)\n",
            "180 391 Loss: 0.183 | Acc: 93.983% (21774/23168)\n",
            "200 391 Loss: 0.183 | Acc: 94.003% (24185/25728)\n",
            "220 391 Loss: 0.182 | Acc: 94.079% (26613/28288)\n",
            "240 391 Loss: 0.181 | Acc: 94.107% (29030/30848)\n",
            "260 391 Loss: 0.180 | Acc: 94.148% (31453/33408)\n",
            "280 391 Loss: 0.180 | Acc: 94.181% (33875/35968)\n",
            "300 391 Loss: 0.180 | Acc: 94.173% (36283/38528)\n",
            "320 391 Loss: 0.180 | Acc: 94.142% (38681/41088)\n",
            "340 391 Loss: 0.180 | Acc: 94.174% (41105/43648)\n",
            "360 391 Loss: 0.178 | Acc: 94.239% (43546/46208)\n",
            "380 391 Loss: 0.179 | Acc: 94.232% (45955/48768)\n",
            "99 100 Loss: 0.303 | Acc: 90.665% (9027/10000)\n",
            "\n",
            "Epoch: 18\n",
            "0 391 Loss: 0.152 | Acc: 96.094% (123/128)\n",
            "20 391 Loss: 0.171 | Acc: 94.903% (2551/2688)\n",
            "40 391 Loss: 0.169 | Acc: 94.760% (4973/5248)\n",
            "60 391 Loss: 0.167 | Acc: 94.736% (7397/7808)\n",
            "80 391 Loss: 0.168 | Acc: 94.599% (9808/10368)\n",
            "100 391 Loss: 0.170 | Acc: 94.554% (12224/12928)\n",
            "120 391 Loss: 0.171 | Acc: 94.531% (14641/15488)\n",
            "140 391 Loss: 0.173 | Acc: 94.393% (17036/18048)\n",
            "160 391 Loss: 0.174 | Acc: 94.332% (19440/20608)\n",
            "180 391 Loss: 0.174 | Acc: 94.354% (21860/23168)\n",
            "200 391 Loss: 0.173 | Acc: 94.384% (24283/25728)\n",
            "220 391 Loss: 0.174 | Acc: 94.365% (26694/28288)\n",
            "240 391 Loss: 0.175 | Acc: 94.343% (29103/30848)\n",
            "260 391 Loss: 0.173 | Acc: 94.373% (31528/33408)\n",
            "280 391 Loss: 0.173 | Acc: 94.387% (33949/35968)\n",
            "300 391 Loss: 0.174 | Acc: 94.342% (36348/38528)\n",
            "320 391 Loss: 0.173 | Acc: 94.390% (38783/41088)\n",
            "340 391 Loss: 0.172 | Acc: 94.440% (41221/43648)\n",
            "360 391 Loss: 0.171 | Acc: 94.453% (43645/46208)\n",
            "380 391 Loss: 0.171 | Acc: 94.453% (46063/48768)\n",
            "99 100 Loss: 0.293 | Acc: 90.741% (9057/10000)\n",
            "\n",
            "Epoch: 19\n",
            "0 391 Loss: 0.126 | Acc: 94.531% (121/128)\n",
            "20 391 Loss: 0.173 | Acc: 94.457% (2539/2688)\n",
            "40 391 Loss: 0.169 | Acc: 94.703% (4970/5248)\n",
            "60 391 Loss: 0.167 | Acc: 94.736% (7397/7808)\n",
            "80 391 Loss: 0.168 | Acc: 94.666% (9815/10368)\n",
            "100 391 Loss: 0.166 | Acc: 94.717% (12245/12928)\n",
            "120 391 Loss: 0.164 | Acc: 94.764% (14677/15488)\n",
            "140 391 Loss: 0.165 | Acc: 94.736% (17098/18048)\n",
            "160 391 Loss: 0.165 | Acc: 94.788% (19534/20608)\n",
            "180 391 Loss: 0.164 | Acc: 94.743% (21950/23168)\n",
            "200 391 Loss: 0.165 | Acc: 94.741% (24375/25728)\n",
            "220 391 Loss: 0.165 | Acc: 94.747% (26802/28288)\n",
            "240 391 Loss: 0.166 | Acc: 94.706% (29215/30848)\n",
            "260 391 Loss: 0.167 | Acc: 94.657% (31623/33408)\n",
            "280 391 Loss: 0.166 | Acc: 94.698% (34061/35968)\n",
            "300 391 Loss: 0.166 | Acc: 94.705% (36488/38528)\n",
            "320 391 Loss: 0.165 | Acc: 94.721% (38919/41088)\n",
            "340 391 Loss: 0.166 | Acc: 94.657% (41316/43648)\n",
            "360 391 Loss: 0.167 | Acc: 94.620% (43722/46208)\n",
            "380 391 Loss: 0.167 | Acc: 94.607% (46138/48768)\n",
            "99 100 Loss: 0.309 | Acc: 90.619% (9073/10000)\n",
            "\n",
            "Epoch: 20\n",
            "0 391 Loss: 0.129 | Acc: 95.312% (122/128)\n",
            "20 391 Loss: 0.179 | Acc: 94.234% (2533/2688)\n",
            "40 391 Loss: 0.172 | Acc: 94.207% (4944/5248)\n",
            "60 391 Loss: 0.167 | Acc: 94.467% (7376/7808)\n",
            "80 391 Loss: 0.165 | Acc: 94.560% (9804/10368)\n",
            "100 391 Loss: 0.161 | Acc: 94.732% (12247/12928)\n",
            "120 391 Loss: 0.161 | Acc: 94.751% (14675/15488)\n",
            "140 391 Loss: 0.163 | Acc: 94.697% (17091/18048)\n",
            "160 391 Loss: 0.165 | Acc: 94.623% (19500/20608)\n",
            "180 391 Loss: 0.165 | Acc: 94.643% (21927/23168)\n",
            "200 391 Loss: 0.165 | Acc: 94.621% (24344/25728)\n",
            "220 391 Loss: 0.163 | Acc: 94.683% (26784/28288)\n",
            "240 391 Loss: 0.164 | Acc: 94.651% (29198/30848)\n",
            "260 391 Loss: 0.164 | Acc: 94.714% (31642/33408)\n",
            "280 391 Loss: 0.164 | Acc: 94.670% (34051/35968)\n",
            "300 391 Loss: 0.163 | Acc: 94.710% (36490/38528)\n",
            "320 391 Loss: 0.163 | Acc: 94.724% (38920/41088)\n",
            "340 391 Loss: 0.164 | Acc: 94.712% (41340/43648)\n",
            "360 391 Loss: 0.163 | Acc: 94.730% (43773/46208)\n",
            "380 391 Loss: 0.163 | Acc: 94.714% (46190/48768)\n",
            "99 100 Loss: 0.307 | Acc: 91.011% (9072/10000)\n",
            "\n",
            "Epoch: 21\n",
            "0 391 Loss: 0.123 | Acc: 96.094% (123/128)\n",
            "20 391 Loss: 0.152 | Acc: 95.350% (2563/2688)\n",
            "40 391 Loss: 0.154 | Acc: 95.122% (4992/5248)\n",
            "60 391 Loss: 0.156 | Acc: 95.133% (7428/7808)\n",
            "80 391 Loss: 0.158 | Acc: 95.052% (9855/10368)\n",
            "100 391 Loss: 0.161 | Acc: 94.957% (12276/12928)\n",
            "120 391 Loss: 0.159 | Acc: 95.022% (14717/15488)\n",
            "140 391 Loss: 0.160 | Acc: 95.002% (17146/18048)\n",
            "160 391 Loss: 0.159 | Acc: 95.012% (19580/20608)\n",
            "180 391 Loss: 0.159 | Acc: 94.980% (22005/23168)\n",
            "200 391 Loss: 0.159 | Acc: 94.963% (24432/25728)\n",
            "220 391 Loss: 0.160 | Acc: 94.938% (26856/28288)\n",
            "240 391 Loss: 0.160 | Acc: 94.898% (29274/30848)\n",
            "260 391 Loss: 0.160 | Acc: 94.941% (31718/33408)\n",
            "280 391 Loss: 0.160 | Acc: 94.940% (34148/35968)\n",
            "300 391 Loss: 0.160 | Acc: 94.913% (36568/38528)\n",
            "320 391 Loss: 0.160 | Acc: 94.899% (38992/41088)\n",
            "340 391 Loss: 0.160 | Acc: 94.896% (41420/43648)\n",
            "360 391 Loss: 0.160 | Acc: 94.871% (43838/46208)\n",
            "380 391 Loss: 0.161 | Acc: 94.847% (46255/48768)\n",
            "99 100 Loss: 0.306 | Acc: 90.710% (9044/10000)\n",
            "\n",
            "Epoch: 22\n",
            "0 391 Loss: 0.146 | Acc: 94.531% (121/128)\n",
            "20 391 Loss: 0.139 | Acc: 94.903% (2551/2688)\n",
            "40 391 Loss: 0.149 | Acc: 94.912% (4981/5248)\n",
            "60 391 Loss: 0.147 | Acc: 95.018% (7419/7808)\n",
            "80 391 Loss: 0.152 | Acc: 94.898% (9839/10368)\n",
            "100 391 Loss: 0.156 | Acc: 94.872% (12265/12928)\n",
            "120 391 Loss: 0.157 | Acc: 94.886% (14696/15488)\n",
            "140 391 Loss: 0.155 | Acc: 94.963% (17139/18048)\n",
            "160 391 Loss: 0.154 | Acc: 94.987% (19575/20608)\n",
            "180 391 Loss: 0.155 | Acc: 94.963% (22001/23168)\n",
            "200 391 Loss: 0.154 | Acc: 94.935% (24425/25728)\n",
            "220 391 Loss: 0.155 | Acc: 94.885% (26841/28288)\n",
            "240 391 Loss: 0.156 | Acc: 94.868% (29265/30848)\n",
            "260 391 Loss: 0.156 | Acc: 94.869% (31694/33408)\n",
            "280 391 Loss: 0.157 | Acc: 94.818% (34104/35968)\n",
            "300 391 Loss: 0.156 | Acc: 94.825% (36534/38528)\n",
            "320 391 Loss: 0.156 | Acc: 94.840% (38968/41088)\n",
            "340 391 Loss: 0.156 | Acc: 94.836% (41394/43648)\n",
            "360 391 Loss: 0.156 | Acc: 94.815% (43812/46208)\n",
            "380 391 Loss: 0.157 | Acc: 94.779% (46222/48768)\n",
            "99 100 Loss: 0.304 | Acc: 90.594% (9055/10000)\n",
            "\n",
            "Epoch: 23\n",
            "0 391 Loss: 0.179 | Acc: 95.312% (122/128)\n",
            "20 391 Loss: 0.159 | Acc: 94.680% (2545/2688)\n",
            "40 391 Loss: 0.146 | Acc: 95.255% (4999/5248)\n",
            "60 391 Loss: 0.146 | Acc: 95.248% (7437/7808)\n",
            "80 391 Loss: 0.153 | Acc: 95.052% (9855/10368)\n",
            "100 391 Loss: 0.153 | Acc: 95.050% (12288/12928)\n",
            "120 391 Loss: 0.152 | Acc: 95.093% (14728/15488)\n",
            "140 391 Loss: 0.152 | Acc: 95.124% (17168/18048)\n",
            "160 391 Loss: 0.155 | Acc: 95.055% (19589/20608)\n",
            "180 391 Loss: 0.154 | Acc: 95.101% (22033/23168)\n",
            "200 391 Loss: 0.153 | Acc: 95.122% (24473/25728)\n",
            "220 391 Loss: 0.152 | Acc: 95.115% (26906/28288)\n",
            "240 391 Loss: 0.153 | Acc: 95.089% (29333/30848)\n",
            "260 391 Loss: 0.154 | Acc: 95.067% (31760/33408)\n",
            "280 391 Loss: 0.152 | Acc: 95.110% (34209/35968)\n",
            "300 391 Loss: 0.153 | Acc: 95.102% (36641/38528)\n",
            "320 391 Loss: 0.154 | Acc: 95.059% (39058/41088)\n",
            "340 391 Loss: 0.154 | Acc: 95.063% (41493/43648)\n",
            "360 391 Loss: 0.154 | Acc: 95.092% (43940/46208)\n",
            "380 391 Loss: 0.153 | Acc: 95.097% (46377/48768)\n",
            "99 100 Loss: 0.297 | Acc: 90.498% (9055/10000)\n",
            "\n",
            "Epoch: 24\n",
            "0 391 Loss: 0.158 | Acc: 94.531% (121/128)\n",
            "20 391 Loss: 0.153 | Acc: 94.903% (2551/2688)\n",
            "40 391 Loss: 0.143 | Acc: 95.255% (4999/5248)\n",
            "60 391 Loss: 0.145 | Acc: 94.992% (7417/7808)\n",
            "80 391 Loss: 0.147 | Acc: 94.946% (9844/10368)\n",
            "100 391 Loss: 0.150 | Acc: 95.026% (12285/12928)\n",
            "120 391 Loss: 0.151 | Acc: 95.041% (14720/15488)\n",
            "140 391 Loss: 0.150 | Acc: 95.096% (17163/18048)\n",
            "160 391 Loss: 0.149 | Acc: 95.138% (19606/20608)\n",
            "180 391 Loss: 0.150 | Acc: 95.157% (22046/23168)\n",
            "200 391 Loss: 0.149 | Acc: 95.204% (24494/25728)\n",
            "220 391 Loss: 0.147 | Acc: 95.242% (26942/28288)\n",
            "240 391 Loss: 0.147 | Acc: 95.199% (29367/30848)\n",
            "260 391 Loss: 0.148 | Acc: 95.196% (31803/33408)\n",
            "280 391 Loss: 0.147 | Acc: 95.232% (34253/35968)\n",
            "300 391 Loss: 0.148 | Acc: 95.180% (36671/38528)\n",
            "320 391 Loss: 0.147 | Acc: 95.179% (39107/41088)\n",
            "340 391 Loss: 0.147 | Acc: 95.175% (41542/43648)\n",
            "360 391 Loss: 0.148 | Acc: 95.135% (43960/46208)\n",
            "380 391 Loss: 0.149 | Acc: 95.116% (46386/48768)\n",
            "99 100 Loss: 0.298 | Acc: 90.977% (9097/10000)\n",
            "\n",
            "Epoch: 25\n",
            "0 391 Loss: 0.138 | Acc: 96.875% (124/128)\n",
            "20 391 Loss: 0.151 | Acc: 95.126% (2557/2688)\n",
            "40 391 Loss: 0.151 | Acc: 95.312% (5002/5248)\n",
            "60 391 Loss: 0.147 | Acc: 95.428% (7451/7808)\n",
            "80 391 Loss: 0.142 | Acc: 95.602% (9912/10368)\n",
            "100 391 Loss: 0.141 | Acc: 95.560% (12354/12928)\n",
            "120 391 Loss: 0.145 | Acc: 95.364% (14770/15488)\n",
            "140 391 Loss: 0.143 | Acc: 95.385% (17215/18048)\n",
            "160 391 Loss: 0.143 | Acc: 95.351% (19650/20608)\n",
            "180 391 Loss: 0.142 | Acc: 95.416% (22106/23168)\n",
            "200 391 Loss: 0.144 | Acc: 95.363% (24535/25728)\n",
            "220 391 Loss: 0.143 | Acc: 95.401% (26987/28288)\n",
            "240 391 Loss: 0.144 | Acc: 95.410% (29432/30848)\n",
            "260 391 Loss: 0.144 | Acc: 95.390% (31868/33408)\n",
            "280 391 Loss: 0.144 | Acc: 95.407% (34316/35968)\n",
            "300 391 Loss: 0.145 | Acc: 95.359% (36740/38528)\n",
            "320 391 Loss: 0.145 | Acc: 95.383% (39191/41088)\n",
            "340 391 Loss: 0.144 | Acc: 95.384% (41633/43648)\n",
            "360 391 Loss: 0.145 | Acc: 95.358% (44063/46208)\n",
            "380 391 Loss: 0.145 | Acc: 95.327% (46489/48768)\n",
            "99 100 Loss: 0.290 | Acc: 91.061% (9094/10000)\n",
            "\n",
            "Epoch: 26\n",
            "0 391 Loss: 0.138 | Acc: 95.312% (122/128)\n",
            "20 391 Loss: 0.147 | Acc: 95.164% (2558/2688)\n",
            "40 391 Loss: 0.141 | Acc: 95.465% (5010/5248)\n",
            "60 391 Loss: 0.135 | Acc: 95.684% (7471/7808)\n",
            "80 391 Loss: 0.138 | Acc: 95.573% (9909/10368)\n",
            "100 391 Loss: 0.141 | Acc: 95.467% (12342/12928)\n",
            "120 391 Loss: 0.142 | Acc: 95.384% (14773/15488)\n",
            "140 391 Loss: 0.144 | Acc: 95.312% (17202/18048)\n",
            "160 391 Loss: 0.145 | Acc: 95.230% (19625/20608)\n",
            "180 391 Loss: 0.145 | Acc: 95.252% (22068/23168)\n",
            "200 391 Loss: 0.145 | Acc: 95.262% (24509/25728)\n",
            "220 391 Loss: 0.145 | Acc: 95.245% (26943/28288)\n",
            "240 391 Loss: 0.145 | Acc: 95.261% (29386/30848)\n",
            "260 391 Loss: 0.144 | Acc: 95.274% (31829/33408)\n",
            "280 391 Loss: 0.145 | Acc: 95.249% (34259/35968)\n",
            "300 391 Loss: 0.144 | Acc: 95.274% (36707/38528)\n",
            "320 391 Loss: 0.144 | Acc: 95.303% (39158/41088)\n",
            "340 391 Loss: 0.143 | Acc: 95.308% (41600/43648)\n",
            "360 391 Loss: 0.143 | Acc: 95.291% (44032/46208)\n",
            "380 391 Loss: 0.143 | Acc: 95.310% (46481/48768)\n",
            "99 100 Loss: 0.306 | Acc: 90.726% (9075/10000)\n",
            "\n",
            "Epoch: 27\n",
            "0 391 Loss: 0.060 | Acc: 97.656% (125/128)\n",
            "20 391 Loss: 0.137 | Acc: 95.610% (2570/2688)\n",
            "40 391 Loss: 0.136 | Acc: 95.427% (5008/5248)\n",
            "60 391 Loss: 0.134 | Acc: 95.569% (7462/7808)\n",
            "80 391 Loss: 0.136 | Acc: 95.621% (9914/10368)\n",
            "100 391 Loss: 0.137 | Acc: 95.575% (12356/12928)\n",
            "120 391 Loss: 0.136 | Acc: 95.629% (14811/15488)\n",
            "140 391 Loss: 0.138 | Acc: 95.556% (17246/18048)\n",
            "160 391 Loss: 0.138 | Acc: 95.550% (19691/20608)\n",
            "180 391 Loss: 0.137 | Acc: 95.515% (22129/23168)\n",
            "200 391 Loss: 0.138 | Acc: 95.472% (24563/25728)\n",
            "220 391 Loss: 0.139 | Acc: 95.436% (26997/28288)\n",
            "240 391 Loss: 0.139 | Acc: 95.442% (29442/30848)\n",
            "260 391 Loss: 0.139 | Acc: 95.411% (31875/33408)\n",
            "280 391 Loss: 0.139 | Acc: 95.390% (34310/35968)\n",
            "300 391 Loss: 0.139 | Acc: 95.424% (36765/38528)\n",
            "320 391 Loss: 0.139 | Acc: 95.424% (39208/41088)\n",
            "340 391 Loss: 0.139 | Acc: 95.425% (41651/43648)\n",
            "360 391 Loss: 0.139 | Acc: 95.425% (44094/46208)\n",
            "380 391 Loss: 0.140 | Acc: 95.423% (46536/48768)\n",
            "99 100 Loss: 0.296 | Acc: 90.997% (9086/10000)\n",
            "\n",
            "Epoch: 28\n",
            "0 391 Loss: 0.103 | Acc: 96.094% (123/128)\n",
            "20 391 Loss: 0.144 | Acc: 95.238% (2560/2688)\n",
            "40 391 Loss: 0.136 | Acc: 95.427% (5008/5248)\n",
            "60 391 Loss: 0.137 | Acc: 95.415% (7450/7808)\n",
            "80 391 Loss: 0.137 | Acc: 95.496% (9901/10368)\n",
            "100 391 Loss: 0.135 | Acc: 95.599% (12359/12928)\n",
            "120 391 Loss: 0.137 | Acc: 95.538% (14797/15488)\n",
            "140 391 Loss: 0.137 | Acc: 95.567% (17248/18048)\n",
            "160 391 Loss: 0.137 | Acc: 95.570% (19695/20608)\n",
            "180 391 Loss: 0.136 | Acc: 95.567% (22141/23168)\n",
            "200 391 Loss: 0.135 | Acc: 95.565% (24587/25728)\n",
            "220 391 Loss: 0.133 | Acc: 95.645% (27056/28288)\n",
            "240 391 Loss: 0.133 | Acc: 95.643% (29504/30848)\n",
            "260 391 Loss: 0.134 | Acc: 95.612% (31942/33408)\n",
            "280 391 Loss: 0.133 | Acc: 95.643% (34401/35968)\n",
            "300 391 Loss: 0.134 | Acc: 95.616% (36839/38528)\n",
            "320 391 Loss: 0.133 | Acc: 95.648% (39300/41088)\n",
            "340 391 Loss: 0.132 | Acc: 95.677% (41761/43648)\n",
            "360 391 Loss: 0.133 | Acc: 95.685% (44214/46208)\n",
            "380 391 Loss: 0.133 | Acc: 95.669% (46656/48768)\n",
            "99 100 Loss: 0.291 | Acc: 91.412% (9142/10000)\n",
            "\n",
            "Epoch: 29\n",
            "0 391 Loss: 0.065 | Acc: 99.219% (127/128)\n",
            "20 391 Loss: 0.120 | Acc: 96.094% (2583/2688)\n",
            "40 391 Loss: 0.131 | Acc: 95.503% (5012/5248)\n",
            "60 391 Loss: 0.126 | Acc: 95.812% (7481/7808)\n",
            "80 391 Loss: 0.128 | Acc: 95.785% (9931/10368)\n",
            "100 391 Loss: 0.127 | Acc: 95.784% (12383/12928)\n",
            "120 391 Loss: 0.124 | Acc: 95.913% (14855/15488)\n",
            "140 391 Loss: 0.128 | Acc: 95.756% (17282/18048)\n",
            "160 391 Loss: 0.128 | Acc: 95.754% (19733/20608)\n",
            "180 391 Loss: 0.127 | Acc: 95.843% (22205/23168)\n",
            "200 391 Loss: 0.126 | Acc: 95.841% (24658/25728)\n",
            "220 391 Loss: 0.128 | Acc: 95.818% (27105/28288)\n",
            "240 391 Loss: 0.127 | Acc: 95.821% (29559/30848)\n",
            "260 391 Loss: 0.128 | Acc: 95.803% (32006/33408)\n",
            "280 391 Loss: 0.131 | Acc: 95.732% (34433/35968)\n",
            "300 391 Loss: 0.131 | Acc: 95.730% (36883/38528)\n",
            "320 391 Loss: 0.131 | Acc: 95.702% (39322/41088)\n",
            "340 391 Loss: 0.132 | Acc: 95.697% (41770/43648)\n",
            "360 391 Loss: 0.131 | Acc: 95.711% (44226/46208)\n",
            "380 391 Loss: 0.131 | Acc: 95.710% (46676/48768)\n",
            "99 100 Loss: 0.305 | Acc: 90.828% (9085/10000)\n",
            "\n",
            "Epoch: 30\n",
            "0 391 Loss: 0.107 | Acc: 97.656% (125/128)\n",
            "20 391 Loss: 0.115 | Acc: 96.540% (2595/2688)\n",
            "40 391 Loss: 0.130 | Acc: 95.865% (5031/5248)\n",
            "60 391 Loss: 0.133 | Acc: 95.710% (7473/7808)\n",
            "80 391 Loss: 0.134 | Acc: 95.718% (9924/10368)\n",
            "100 391 Loss: 0.135 | Acc: 95.707% (12373/12928)\n",
            "120 391 Loss: 0.133 | Acc: 95.797% (14837/15488)\n",
            "140 391 Loss: 0.133 | Acc: 95.800% (17290/18048)\n",
            "160 391 Loss: 0.133 | Acc: 95.773% (19737/20608)\n",
            "180 391 Loss: 0.131 | Acc: 95.843% (22205/23168)\n",
            "200 391 Loss: 0.131 | Acc: 95.872% (24666/25728)\n",
            "220 391 Loss: 0.131 | Acc: 95.903% (27129/28288)\n",
            "240 391 Loss: 0.132 | Acc: 95.867% (29573/30848)\n",
            "260 391 Loss: 0.132 | Acc: 95.869% (32028/33408)\n",
            "280 391 Loss: 0.132 | Acc: 95.849% (34475/35968)\n",
            "300 391 Loss: 0.132 | Acc: 95.847% (36928/38528)\n",
            "320 391 Loss: 0.133 | Acc: 95.821% (39371/41088)\n",
            "340 391 Loss: 0.132 | Acc: 95.828% (41827/43648)\n",
            "360 391 Loss: 0.133 | Acc: 95.786% (44261/46208)\n",
            "380 391 Loss: 0.132 | Acc: 95.807% (46723/48768)\n",
            "99 100 Loss: 0.297 | Acc: 90.980% (9115/10000)\n",
            "\n",
            "Epoch: 31\n",
            "0 391 Loss: 0.224 | Acc: 92.969% (119/128)\n",
            "20 391 Loss: 0.135 | Acc: 95.499% (2567/2688)\n",
            "40 391 Loss: 0.132 | Acc: 95.503% (5012/5248)\n",
            "60 391 Loss: 0.129 | Acc: 95.633% (7467/7808)\n",
            "80 391 Loss: 0.131 | Acc: 95.602% (9912/10368)\n",
            "100 391 Loss: 0.130 | Acc: 95.715% (12374/12928)\n",
            "120 391 Loss: 0.130 | Acc: 95.771% (14833/15488)\n",
            "140 391 Loss: 0.128 | Acc: 95.817% (17293/18048)\n",
            "160 391 Loss: 0.130 | Acc: 95.744% (19731/20608)\n",
            "180 391 Loss: 0.130 | Acc: 95.740% (22181/23168)\n",
            "200 391 Loss: 0.129 | Acc: 95.771% (24640/25728)\n",
            "220 391 Loss: 0.127 | Acc: 95.846% (27113/28288)\n",
            "240 391 Loss: 0.127 | Acc: 95.847% (29567/30848)\n",
            "260 391 Loss: 0.127 | Acc: 95.863% (32026/33408)\n",
            "280 391 Loss: 0.127 | Acc: 95.841% (34472/35968)\n",
            "300 391 Loss: 0.128 | Acc: 95.798% (36909/38528)\n",
            "320 391 Loss: 0.128 | Acc: 95.785% (39356/41088)\n",
            "340 391 Loss: 0.128 | Acc: 95.778% (41805/43648)\n",
            "360 391 Loss: 0.127 | Acc: 95.795% (44265/46208)\n",
            "380 391 Loss: 0.127 | Acc: 95.786% (46713/48768)\n",
            "99 100 Loss: 0.302 | Acc: 90.913% (9088/10000)\n",
            "\n",
            "Epoch: 32\n",
            "0 391 Loss: 0.082 | Acc: 97.656% (125/128)\n",
            "20 391 Loss: 0.129 | Acc: 95.833% (2576/2688)\n",
            "40 391 Loss: 0.123 | Acc: 96.208% (5049/5248)\n",
            "60 391 Loss: 0.120 | Acc: 96.311% (7520/7808)\n",
            "80 391 Loss: 0.123 | Acc: 96.190% (9973/10368)\n",
            "100 391 Loss: 0.124 | Acc: 96.094% (12423/12928)\n",
            "120 391 Loss: 0.125 | Acc: 96.042% (14875/15488)\n",
            "140 391 Loss: 0.126 | Acc: 95.883% (17305/18048)\n",
            "160 391 Loss: 0.125 | Acc: 95.880% (19759/20608)\n",
            "180 391 Loss: 0.124 | Acc: 95.891% (22216/23168)\n",
            "200 391 Loss: 0.124 | Acc: 95.911% (24676/25728)\n",
            "220 391 Loss: 0.124 | Acc: 95.917% (27133/28288)\n",
            "240 391 Loss: 0.125 | Acc: 95.851% (29568/30848)\n",
            "260 391 Loss: 0.125 | Acc: 95.854% (32023/33408)\n",
            "280 391 Loss: 0.126 | Acc: 95.827% (34467/35968)\n",
            "300 391 Loss: 0.126 | Acc: 95.847% (36928/38528)\n",
            "320 391 Loss: 0.126 | Acc: 95.848% (39382/41088)\n",
            "340 391 Loss: 0.125 | Acc: 95.853% (41838/43648)\n",
            "360 391 Loss: 0.126 | Acc: 95.825% (44279/46208)\n",
            "380 391 Loss: 0.125 | Acc: 95.848% (46743/48768)\n",
            "99 100 Loss: 0.300 | Acc: 91.105% (9114/10000)\n",
            "\n",
            "Epoch: 33\n",
            "0 391 Loss: 0.146 | Acc: 96.094% (123/128)\n",
            "20 391 Loss: 0.124 | Acc: 96.019% (2581/2688)\n",
            "40 391 Loss: 0.126 | Acc: 96.094% (5043/5248)\n",
            "60 391 Loss: 0.125 | Acc: 96.017% (7497/7808)\n",
            "80 391 Loss: 0.124 | Acc: 96.017% (9955/10368)\n",
            "100 391 Loss: 0.121 | Acc: 96.132% (12428/12928)\n",
            "120 391 Loss: 0.121 | Acc: 96.081% (14881/15488)\n",
            "140 391 Loss: 0.122 | Acc: 96.038% (17333/18048)\n",
            "160 391 Loss: 0.122 | Acc: 96.045% (19793/20608)\n",
            "180 391 Loss: 0.124 | Acc: 95.943% (22228/23168)\n",
            "200 391 Loss: 0.123 | Acc: 95.977% (24693/25728)\n",
            "220 391 Loss: 0.123 | Acc: 95.977% (27150/28288)\n",
            "240 391 Loss: 0.123 | Acc: 95.941% (29596/30848)\n",
            "260 391 Loss: 0.124 | Acc: 95.908% (32041/33408)\n",
            "280 391 Loss: 0.123 | Acc: 95.924% (34502/35968)\n",
            "300 391 Loss: 0.124 | Acc: 95.902% (36949/38528)\n",
            "320 391 Loss: 0.125 | Acc: 95.880% (39395/41088)\n",
            "340 391 Loss: 0.124 | Acc: 95.885% (41852/43648)\n",
            "360 391 Loss: 0.125 | Acc: 95.886% (44307/46208)\n",
            "380 391 Loss: 0.125 | Acc: 95.870% (46754/48768)\n",
            "99 100 Loss: 0.294 | Acc: 90.971% (9106/10000)\n",
            "\n",
            "Epoch: 34\n",
            "0 391 Loss: 0.190 | Acc: 92.969% (119/128)\n",
            "20 391 Loss: 0.116 | Acc: 96.094% (2583/2688)\n",
            "40 391 Loss: 0.121 | Acc: 95.960% (5036/5248)\n",
            "60 391 Loss: 0.125 | Acc: 95.863% (7485/7808)\n",
            "80 391 Loss: 0.125 | Acc: 95.920% (9945/10368)\n",
            "100 391 Loss: 0.125 | Acc: 95.885% (12396/12928)\n",
            "120 391 Loss: 0.124 | Acc: 95.900% (14853/15488)\n",
            "140 391 Loss: 0.123 | Acc: 95.994% (17325/18048)\n",
            "160 391 Loss: 0.123 | Acc: 95.982% (19780/20608)\n",
            "180 391 Loss: 0.123 | Acc: 96.025% (22247/23168)\n",
            "200 391 Loss: 0.124 | Acc: 95.954% (24687/25728)\n",
            "220 391 Loss: 0.124 | Acc: 95.991% (27154/28288)\n",
            "240 391 Loss: 0.122 | Acc: 96.045% (29628/30848)\n",
            "260 391 Loss: 0.122 | Acc: 96.058% (32091/33408)\n",
            "280 391 Loss: 0.122 | Acc: 96.008% (34532/35968)\n",
            "300 391 Loss: 0.123 | Acc: 96.008% (36990/38528)\n",
            "320 391 Loss: 0.123 | Acc: 96.011% (39449/41088)\n",
            "340 391 Loss: 0.122 | Acc: 96.041% (41920/43648)\n",
            "360 391 Loss: 0.122 | Acc: 96.037% (44377/46208)\n",
            "380 391 Loss: 0.122 | Acc: 96.036% (46835/48768)\n",
            "99 100 Loss: 0.296 | Acc: 90.970% (9105/10000)\n",
            "\n",
            "Epoch: 35\n",
            "0 391 Loss: 0.129 | Acc: 96.875% (124/128)\n",
            "20 391 Loss: 0.108 | Acc: 96.652% (2598/2688)\n",
            "40 391 Loss: 0.107 | Acc: 96.723% (5076/5248)\n",
            "60 391 Loss: 0.114 | Acc: 96.478% (7533/7808)\n",
            "80 391 Loss: 0.115 | Acc: 96.393% (9994/10368)\n",
            "100 391 Loss: 0.117 | Acc: 96.303% (12450/12928)\n",
            "120 391 Loss: 0.117 | Acc: 96.300% (14915/15488)\n",
            "140 391 Loss: 0.119 | Acc: 96.199% (17362/18048)\n",
            "160 391 Loss: 0.117 | Acc: 96.244% (19834/20608)\n",
            "180 391 Loss: 0.118 | Acc: 96.167% (22280/23168)\n",
            "200 391 Loss: 0.117 | Acc: 96.195% (24749/25728)\n",
            "220 391 Loss: 0.118 | Acc: 96.179% (27207/28288)\n",
            "240 391 Loss: 0.118 | Acc: 96.210% (29679/30848)\n",
            "260 391 Loss: 0.119 | Acc: 96.172% (32129/33408)\n",
            "280 391 Loss: 0.119 | Acc: 96.160% (34587/35968)\n",
            "300 391 Loss: 0.119 | Acc: 96.143% (37042/38528)\n",
            "320 391 Loss: 0.119 | Acc: 96.179% (39518/41088)\n",
            "340 391 Loss: 0.120 | Acc: 96.137% (41962/43648)\n",
            "360 391 Loss: 0.120 | Acc: 96.126% (44418/46208)\n",
            "380 391 Loss: 0.121 | Acc: 96.102% (46867/48768)\n",
            "99 100 Loss: 0.296 | Acc: 91.323% (9110/10000)\n",
            "\n",
            "Epoch: 36\n",
            "0 391 Loss: 0.071 | Acc: 99.219% (127/128)\n",
            "20 391 Loss: 0.107 | Acc: 96.577% (2596/2688)\n",
            "40 391 Loss: 0.106 | Acc: 96.513% (5065/5248)\n",
            "60 391 Loss: 0.107 | Acc: 96.632% (7545/7808)\n",
            "80 391 Loss: 0.110 | Acc: 96.547% (10010/10368)\n",
            "100 391 Loss: 0.110 | Acc: 96.519% (12478/12928)\n",
            "120 391 Loss: 0.112 | Acc: 96.442% (14937/15488)\n",
            "140 391 Loss: 0.115 | Acc: 96.315% (17383/18048)\n",
            "160 391 Loss: 0.118 | Acc: 96.215% (19828/20608)\n",
            "180 391 Loss: 0.116 | Acc: 96.279% (22306/23168)\n",
            "200 391 Loss: 0.118 | Acc: 96.210% (24753/25728)\n",
            "220 391 Loss: 0.117 | Acc: 96.207% (27215/28288)\n",
            "240 391 Loss: 0.116 | Acc: 96.236% (29687/30848)\n",
            "260 391 Loss: 0.116 | Acc: 96.264% (32160/33408)\n",
            "280 391 Loss: 0.117 | Acc: 96.199% (34601/35968)\n",
            "300 391 Loss: 0.118 | Acc: 96.166% (37051/38528)\n",
            "320 391 Loss: 0.117 | Acc: 96.179% (39518/41088)\n",
            "340 391 Loss: 0.117 | Acc: 96.204% (41991/43648)\n",
            "360 391 Loss: 0.117 | Acc: 96.200% (44452/46208)\n",
            "380 391 Loss: 0.117 | Acc: 96.188% (46909/48768)\n",
            "99 100 Loss: 0.294 | Acc: 91.229% (9120/10000)\n",
            "\n",
            "Epoch: 37\n",
            "0 391 Loss: 0.119 | Acc: 96.094% (123/128)\n",
            "20 391 Loss: 0.133 | Acc: 95.312% (2562/2688)\n",
            "40 391 Loss: 0.123 | Acc: 95.865% (5031/5248)\n",
            "60 391 Loss: 0.120 | Acc: 95.940% (7491/7808)\n",
            "80 391 Loss: 0.123 | Acc: 95.891% (9942/10368)\n",
            "100 391 Loss: 0.122 | Acc: 95.970% (12407/12928)\n",
            "120 391 Loss: 0.124 | Acc: 95.932% (14858/15488)\n",
            "140 391 Loss: 0.121 | Acc: 96.016% (17329/18048)\n",
            "160 391 Loss: 0.121 | Acc: 96.021% (19788/20608)\n",
            "180 391 Loss: 0.119 | Acc: 96.076% (22259/23168)\n",
            "200 391 Loss: 0.119 | Acc: 96.086% (24721/25728)\n",
            "220 391 Loss: 0.119 | Acc: 96.094% (27183/28288)\n",
            "240 391 Loss: 0.119 | Acc: 96.133% (29655/30848)\n",
            "260 391 Loss: 0.118 | Acc: 96.139% (32118/33408)\n",
            "280 391 Loss: 0.119 | Acc: 96.124% (34574/35968)\n",
            "300 391 Loss: 0.118 | Acc: 96.140% (37041/38528)\n",
            "320 391 Loss: 0.118 | Acc: 96.145% (39504/41088)\n",
            "340 391 Loss: 0.118 | Acc: 96.172% (41977/43648)\n",
            "360 391 Loss: 0.118 | Acc: 96.163% (44435/46208)\n",
            "380 391 Loss: 0.118 | Acc: 96.157% (46894/48768)\n",
            "99 100 Loss: 0.291 | Acc: 91.175% (9131/10000)\n",
            "\n",
            "Epoch: 38\n",
            "0 391 Loss: 0.139 | Acc: 95.312% (122/128)\n",
            "20 391 Loss: 0.108 | Acc: 96.503% (2594/2688)\n",
            "40 391 Loss: 0.110 | Acc: 96.361% (5057/5248)\n",
            "60 391 Loss: 0.114 | Acc: 96.209% (7512/7808)\n",
            "80 391 Loss: 0.118 | Acc: 96.200% (9974/10368)\n",
            "100 391 Loss: 0.114 | Acc: 96.295% (12449/12928)\n",
            "120 391 Loss: 0.115 | Acc: 96.287% (14913/15488)\n",
            "140 391 Loss: 0.116 | Acc: 96.238% (17369/18048)\n",
            "160 391 Loss: 0.117 | Acc: 96.239% (19833/20608)\n",
            "180 391 Loss: 0.116 | Acc: 96.305% (22312/23168)\n",
            "200 391 Loss: 0.116 | Acc: 96.284% (24772/25728)\n",
            "220 391 Loss: 0.116 | Acc: 96.271% (27233/28288)\n",
            "240 391 Loss: 0.115 | Acc: 96.285% (29702/30848)\n",
            "260 391 Loss: 0.114 | Acc: 96.300% (32172/33408)\n",
            "280 391 Loss: 0.115 | Acc: 96.291% (34634/35968)\n",
            "300 391 Loss: 0.115 | Acc: 96.278% (37094/38528)\n",
            "320 391 Loss: 0.114 | Acc: 96.291% (39564/41088)\n",
            "340 391 Loss: 0.114 | Acc: 96.286% (42027/43648)\n",
            "360 391 Loss: 0.114 | Acc: 96.291% (44494/46208)\n",
            "380 391 Loss: 0.114 | Acc: 96.278% (46953/48768)\n",
            "99 100 Loss: 0.291 | Acc: 91.430% (9141/10000)\n",
            "\n",
            "Epoch: 39\n",
            "0 391 Loss: 0.076 | Acc: 99.219% (127/128)\n",
            "20 391 Loss: 0.103 | Acc: 97.061% (2609/2688)\n",
            "40 391 Loss: 0.105 | Acc: 96.970% (5089/5248)\n",
            "60 391 Loss: 0.107 | Acc: 96.760% (7555/7808)\n",
            "80 391 Loss: 0.109 | Acc: 96.750% (10031/10368)\n",
            "100 391 Loss: 0.111 | Acc: 96.620% (12491/12928)\n",
            "120 391 Loss: 0.110 | Acc: 96.520% (14949/15488)\n",
            "140 391 Loss: 0.110 | Acc: 96.487% (17414/18048)\n",
            "160 391 Loss: 0.110 | Acc: 96.501% (19887/20608)\n",
            "180 391 Loss: 0.111 | Acc: 96.469% (22350/23168)\n",
            "200 391 Loss: 0.111 | Acc: 96.447% (24814/25728)\n",
            "220 391 Loss: 0.110 | Acc: 96.461% (27287/28288)\n",
            "240 391 Loss: 0.109 | Acc: 96.515% (29773/30848)\n",
            "260 391 Loss: 0.109 | Acc: 96.453% (32223/33408)\n",
            "280 391 Loss: 0.108 | Acc: 96.472% (34699/35968)\n",
            "300 391 Loss: 0.109 | Acc: 96.468% (37167/38528)\n",
            "320 391 Loss: 0.110 | Acc: 96.425% (39619/41088)\n",
            "340 391 Loss: 0.111 | Acc: 96.380% (42068/43648)\n",
            "360 391 Loss: 0.111 | Acc: 96.390% (44540/46208)\n",
            "380 391 Loss: 0.112 | Acc: 96.371% (46998/48768)\n",
            "99 100 Loss: 0.285 | Acc: 91.451% (9140/10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pDhrASDOB1WN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(avg_best_accs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQQ7uurFzGgh",
        "outputId": "9980687d-4a5f-4b88-a2d3-b7378657a5e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[87.80170226399171, 73.19278701708485, 10.867333203964202, 91.36902079859581]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(zip(test_names, avg_best_accs)))"
      ],
      "metadata": {
        "id": "9b0Jno7JfHdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5fce7d-f8c3-4b7d-ef91-2de6478dfc87"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 87.80170226399171), ('B', 73.19278701708485), ('C', 10.867333203964202), ('D', 91.36902079859581)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-a5N53YBBN8o"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}