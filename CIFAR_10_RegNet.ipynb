{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train CIFAR10 with PyTorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torchsummary\n",
    "import os\n",
    "import time\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
    "# parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
    "# parser.add_argument('--resume', '-r', action='store_true',\n",
    "#                     help='resume from checkpoint')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fd3fe8fbcc45e1b9de0f3f9d8b3208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print('==> Building model..')\n",
    "class SE(nn.Module):\n",
    "    '''Squeeze-and-Excitation block.'''\n",
    "\n",
    "    def __init__(self, in_planes, se_planes):\n",
    "        super(SE, self).__init__()\n",
    "        self.se1 = nn.Conv2d(in_planes, se_planes, kernel_size=1, bias=True)\n",
    "        self.se2 = nn.Conv2d(se_planes, in_planes, kernel_size=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        out = F.relu(self.se1(out)) # squeeze\n",
    "        out = self.se2(out).sigmoid() # excitation\n",
    "        out = x * out # inception and resnet(reweight)\n",
    "        return out\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, w_in, w_out, stride, group_width, bottleneck_ratio, se_ratio):\n",
    "        super(Block, self).__init__()\n",
    "        # 1x1\n",
    "        w_b = int(round(w_out * bottleneck_ratio)) # the middle layer is slim, to end is thick so it is like a bottleneck\n",
    "        self.conv1 = nn.Conv2d(w_in, w_b, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(w_b)\n",
    "        # 3x3\n",
    "        num_groups = w_b // group_width #Group the corresponding input channels and number of output channels\n",
    "        self.conv2 = nn.Conv2d(w_b, w_b, kernel_size=3,\n",
    "                               stride=stride, padding=1, groups=num_groups, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(w_b)\n",
    "        # se\n",
    "        self.with_se = se_ratio > 0\n",
    "        if self.with_se:\n",
    "            w_se = int(round(w_in * se_ratio))\n",
    "            self.se = SE(w_b, w_se)\n",
    "        # 1x1\n",
    "        self.conv3 = nn.Conv2d(w_b, w_out, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(w_out)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or w_in != w_out:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(w_in, w_out,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(w_out)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        if self.with_se:\n",
    "            out = self.se(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class RegNet(nn.Module):\n",
    "    def __init__(self, cfg, num_classes=10):\n",
    "        super(RegNet, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(0)\n",
    "        self.layer2 = self._make_layer(1)\n",
    "        self.layer3 = self._make_layer(2)\n",
    "        self.layer4 = self._make_layer(3)\n",
    "        self.linear = nn.Linear(self.cfg['widths'][-1], num_classes)\n",
    "\n",
    "    def _make_layer(self, idx):\n",
    "        depth = self.cfg['depths'][idx]\n",
    "        width = self.cfg['widths'][idx]\n",
    "        stride = self.cfg['strides'][idx]\n",
    "        group_width = self.cfg['group_width']\n",
    "        bottleneck_ratio = self.cfg['bottleneck_ratio']\n",
    "        se_ratio = self.cfg['se_ratio']\n",
    "\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            s = stride if i == 0 else 1\n",
    "            layers.append(Block(self.in_planes, width,\n",
    "                                s, group_width, bottleneck_ratio, se_ratio))\n",
    "            self.in_planes = width\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'depths': [1, 4, 8, 12],\n",
    "    'widths': [32, 64, 160, 416],\n",
    "    'strides': [1, 1, 2, 2],\n",
    "    'group_width': 16,\n",
    "    'bottleneck_ratio': 0.5,\n",
    "    'se_ratio': 0.25,\n",
    "    'lr': 0.01,\n",
    "    'resume': False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 16, 32, 32]           1,024\n",
      "       BatchNorm2d-4           [-1, 16, 32, 32]              32\n",
      "            Conv2d-5           [-1, 16, 32, 32]           2,304\n",
      "       BatchNorm2d-6           [-1, 16, 32, 32]              32\n",
      "            Conv2d-7             [-1, 16, 1, 1]             272\n",
      "            Conv2d-8             [-1, 16, 1, 1]             272\n",
      "                SE-9           [-1, 16, 32, 32]               0\n",
      "           Conv2d-10           [-1, 32, 32, 32]             512\n",
      "      BatchNorm2d-11           [-1, 32, 32, 32]              64\n",
      "           Conv2d-12           [-1, 32, 32, 32]           2,048\n",
      "      BatchNorm2d-13           [-1, 32, 32, 32]              64\n",
      "            Block-14           [-1, 32, 32, 32]               0\n",
      "           Conv2d-15           [-1, 32, 32, 32]           1,024\n",
      "      BatchNorm2d-16           [-1, 32, 32, 32]              64\n",
      "           Conv2d-17           [-1, 32, 32, 32]           4,608\n",
      "      BatchNorm2d-18           [-1, 32, 32, 32]              64\n",
      "           Conv2d-19              [-1, 8, 1, 1]             264\n",
      "           Conv2d-20             [-1, 32, 1, 1]             288\n",
      "               SE-21           [-1, 32, 32, 32]               0\n",
      "           Conv2d-22           [-1, 64, 32, 32]           2,048\n",
      "      BatchNorm2d-23           [-1, 64, 32, 32]             128\n",
      "           Conv2d-24           [-1, 64, 32, 32]           2,048\n",
      "      BatchNorm2d-25           [-1, 64, 32, 32]             128\n",
      "            Block-26           [-1, 64, 32, 32]               0\n",
      "           Conv2d-27           [-1, 32, 32, 32]           2,048\n",
      "      BatchNorm2d-28           [-1, 32, 32, 32]              64\n",
      "           Conv2d-29           [-1, 32, 32, 32]           4,608\n",
      "      BatchNorm2d-30           [-1, 32, 32, 32]              64\n",
      "           Conv2d-31             [-1, 16, 1, 1]             528\n",
      "           Conv2d-32             [-1, 32, 1, 1]             544\n",
      "               SE-33           [-1, 32, 32, 32]               0\n",
      "           Conv2d-34           [-1, 64, 32, 32]           2,048\n",
      "      BatchNorm2d-35           [-1, 64, 32, 32]             128\n",
      "            Block-36           [-1, 64, 32, 32]               0\n",
      "           Conv2d-37           [-1, 32, 32, 32]           2,048\n",
      "      BatchNorm2d-38           [-1, 32, 32, 32]              64\n",
      "           Conv2d-39           [-1, 32, 32, 32]           4,608\n",
      "      BatchNorm2d-40           [-1, 32, 32, 32]              64\n",
      "           Conv2d-41             [-1, 16, 1, 1]             528\n",
      "           Conv2d-42             [-1, 32, 1, 1]             544\n",
      "               SE-43           [-1, 32, 32, 32]               0\n",
      "           Conv2d-44           [-1, 64, 32, 32]           2,048\n",
      "      BatchNorm2d-45           [-1, 64, 32, 32]             128\n",
      "            Block-46           [-1, 64, 32, 32]               0\n",
      "           Conv2d-47           [-1, 32, 32, 32]           2,048\n",
      "      BatchNorm2d-48           [-1, 32, 32, 32]              64\n",
      "           Conv2d-49           [-1, 32, 32, 32]           4,608\n",
      "      BatchNorm2d-50           [-1, 32, 32, 32]              64\n",
      "           Conv2d-51             [-1, 16, 1, 1]             528\n",
      "           Conv2d-52             [-1, 32, 1, 1]             544\n",
      "               SE-53           [-1, 32, 32, 32]               0\n",
      "           Conv2d-54           [-1, 64, 32, 32]           2,048\n",
      "      BatchNorm2d-55           [-1, 64, 32, 32]             128\n",
      "            Block-56           [-1, 64, 32, 32]               0\n",
      "           Conv2d-57           [-1, 80, 32, 32]           5,120\n",
      "      BatchNorm2d-58           [-1, 80, 32, 32]             160\n",
      "           Conv2d-59           [-1, 80, 16, 16]          11,520\n",
      "      BatchNorm2d-60           [-1, 80, 16, 16]             160\n",
      "           Conv2d-61             [-1, 16, 1, 1]           1,296\n",
      "           Conv2d-62             [-1, 80, 1, 1]           1,360\n",
      "               SE-63           [-1, 80, 16, 16]               0\n",
      "           Conv2d-64          [-1, 160, 16, 16]          12,800\n",
      "      BatchNorm2d-65          [-1, 160, 16, 16]             320\n",
      "           Conv2d-66          [-1, 160, 16, 16]          10,240\n",
      "      BatchNorm2d-67          [-1, 160, 16, 16]             320\n",
      "            Block-68          [-1, 160, 16, 16]               0\n",
      "           Conv2d-69           [-1, 80, 16, 16]          12,800\n",
      "      BatchNorm2d-70           [-1, 80, 16, 16]             160\n",
      "           Conv2d-71           [-1, 80, 16, 16]          11,520\n",
      "      BatchNorm2d-72           [-1, 80, 16, 16]             160\n",
      "           Conv2d-73             [-1, 40, 1, 1]           3,240\n",
      "           Conv2d-74             [-1, 80, 1, 1]           3,280\n",
      "               SE-75           [-1, 80, 16, 16]               0\n",
      "           Conv2d-76          [-1, 160, 16, 16]          12,800\n",
      "      BatchNorm2d-77          [-1, 160, 16, 16]             320\n",
      "            Block-78          [-1, 160, 16, 16]               0\n",
      "           Conv2d-79           [-1, 80, 16, 16]          12,800\n",
      "      BatchNorm2d-80           [-1, 80, 16, 16]             160\n",
      "           Conv2d-81           [-1, 80, 16, 16]          11,520\n",
      "      BatchNorm2d-82           [-1, 80, 16, 16]             160\n",
      "           Conv2d-83             [-1, 40, 1, 1]           3,240\n",
      "           Conv2d-84             [-1, 80, 1, 1]           3,280\n",
      "               SE-85           [-1, 80, 16, 16]               0\n",
      "           Conv2d-86          [-1, 160, 16, 16]          12,800\n",
      "      BatchNorm2d-87          [-1, 160, 16, 16]             320\n",
      "            Block-88          [-1, 160, 16, 16]               0\n",
      "           Conv2d-89           [-1, 80, 16, 16]          12,800\n",
      "      BatchNorm2d-90           [-1, 80, 16, 16]             160\n",
      "           Conv2d-91           [-1, 80, 16, 16]          11,520\n",
      "      BatchNorm2d-92           [-1, 80, 16, 16]             160\n",
      "           Conv2d-93             [-1, 40, 1, 1]           3,240\n",
      "           Conv2d-94             [-1, 80, 1, 1]           3,280\n",
      "               SE-95           [-1, 80, 16, 16]               0\n",
      "           Conv2d-96          [-1, 160, 16, 16]          12,800\n",
      "      BatchNorm2d-97          [-1, 160, 16, 16]             320\n",
      "            Block-98          [-1, 160, 16, 16]               0\n",
      "           Conv2d-99           [-1, 80, 16, 16]          12,800\n",
      "     BatchNorm2d-100           [-1, 80, 16, 16]             160\n",
      "          Conv2d-101           [-1, 80, 16, 16]          11,520\n",
      "     BatchNorm2d-102           [-1, 80, 16, 16]             160\n",
      "          Conv2d-103             [-1, 40, 1, 1]           3,240\n",
      "          Conv2d-104             [-1, 80, 1, 1]           3,280\n",
      "              SE-105           [-1, 80, 16, 16]               0\n",
      "          Conv2d-106          [-1, 160, 16, 16]          12,800\n",
      "     BatchNorm2d-107          [-1, 160, 16, 16]             320\n",
      "           Block-108          [-1, 160, 16, 16]               0\n",
      "          Conv2d-109           [-1, 80, 16, 16]          12,800\n",
      "     BatchNorm2d-110           [-1, 80, 16, 16]             160\n",
      "          Conv2d-111           [-1, 80, 16, 16]          11,520\n",
      "     BatchNorm2d-112           [-1, 80, 16, 16]             160\n",
      "          Conv2d-113             [-1, 40, 1, 1]           3,240\n",
      "          Conv2d-114             [-1, 80, 1, 1]           3,280\n",
      "              SE-115           [-1, 80, 16, 16]               0\n",
      "          Conv2d-116          [-1, 160, 16, 16]          12,800\n",
      "     BatchNorm2d-117          [-1, 160, 16, 16]             320\n",
      "           Block-118          [-1, 160, 16, 16]               0\n",
      "          Conv2d-119           [-1, 80, 16, 16]          12,800\n",
      "     BatchNorm2d-120           [-1, 80, 16, 16]             160\n",
      "          Conv2d-121           [-1, 80, 16, 16]          11,520\n",
      "     BatchNorm2d-122           [-1, 80, 16, 16]             160\n",
      "          Conv2d-123             [-1, 40, 1, 1]           3,240\n",
      "          Conv2d-124             [-1, 80, 1, 1]           3,280\n",
      "              SE-125           [-1, 80, 16, 16]               0\n",
      "          Conv2d-126          [-1, 160, 16, 16]          12,800\n",
      "     BatchNorm2d-127          [-1, 160, 16, 16]             320\n",
      "           Block-128          [-1, 160, 16, 16]               0\n",
      "          Conv2d-129           [-1, 80, 16, 16]          12,800\n",
      "     BatchNorm2d-130           [-1, 80, 16, 16]             160\n",
      "          Conv2d-131           [-1, 80, 16, 16]          11,520\n",
      "     BatchNorm2d-132           [-1, 80, 16, 16]             160\n",
      "          Conv2d-133             [-1, 40, 1, 1]           3,240\n",
      "          Conv2d-134             [-1, 80, 1, 1]           3,280\n",
      "              SE-135           [-1, 80, 16, 16]               0\n",
      "          Conv2d-136          [-1, 160, 16, 16]          12,800\n",
      "     BatchNorm2d-137          [-1, 160, 16, 16]             320\n",
      "           Block-138          [-1, 160, 16, 16]               0\n",
      "          Conv2d-139          [-1, 208, 16, 16]          33,280\n",
      "     BatchNorm2d-140          [-1, 208, 16, 16]             416\n",
      "          Conv2d-141            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-142            [-1, 208, 8, 8]             416\n",
      "          Conv2d-143             [-1, 40, 1, 1]           8,360\n",
      "          Conv2d-144            [-1, 208, 1, 1]           8,528\n",
      "              SE-145            [-1, 208, 8, 8]               0\n",
      "          Conv2d-146            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-147            [-1, 416, 8, 8]             832\n",
      "          Conv2d-148            [-1, 416, 8, 8]          66,560\n",
      "     BatchNorm2d-149            [-1, 416, 8, 8]             832\n",
      "           Block-150            [-1, 416, 8, 8]               0\n",
      "          Conv2d-151            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-152            [-1, 208, 8, 8]             416\n",
      "          Conv2d-153            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-154            [-1, 208, 8, 8]             416\n",
      "          Conv2d-155            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-156            [-1, 208, 1, 1]          21,840\n",
      "              SE-157            [-1, 208, 8, 8]               0\n",
      "          Conv2d-158            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-159            [-1, 416, 8, 8]             832\n",
      "           Block-160            [-1, 416, 8, 8]               0\n",
      "          Conv2d-161            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-162            [-1, 208, 8, 8]             416\n",
      "          Conv2d-163            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-164            [-1, 208, 8, 8]             416\n",
      "          Conv2d-165            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-166            [-1, 208, 1, 1]          21,840\n",
      "              SE-167            [-1, 208, 8, 8]               0\n",
      "          Conv2d-168            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-169            [-1, 416, 8, 8]             832\n",
      "           Block-170            [-1, 416, 8, 8]               0\n",
      "          Conv2d-171            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-172            [-1, 208, 8, 8]             416\n",
      "          Conv2d-173            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-174            [-1, 208, 8, 8]             416\n",
      "          Conv2d-175            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-176            [-1, 208, 1, 1]          21,840\n",
      "              SE-177            [-1, 208, 8, 8]               0\n",
      "          Conv2d-178            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-179            [-1, 416, 8, 8]             832\n",
      "           Block-180            [-1, 416, 8, 8]               0\n",
      "          Conv2d-181            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-182            [-1, 208, 8, 8]             416\n",
      "          Conv2d-183            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-184            [-1, 208, 8, 8]             416\n",
      "          Conv2d-185            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-186            [-1, 208, 1, 1]          21,840\n",
      "              SE-187            [-1, 208, 8, 8]               0\n",
      "          Conv2d-188            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-189            [-1, 416, 8, 8]             832\n",
      "           Block-190            [-1, 416, 8, 8]               0\n",
      "          Conv2d-191            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-192            [-1, 208, 8, 8]             416\n",
      "          Conv2d-193            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-194            [-1, 208, 8, 8]             416\n",
      "          Conv2d-195            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-196            [-1, 208, 1, 1]          21,840\n",
      "              SE-197            [-1, 208, 8, 8]               0\n",
      "          Conv2d-198            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-199            [-1, 416, 8, 8]             832\n",
      "           Block-200            [-1, 416, 8, 8]               0\n",
      "          Conv2d-201            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-202            [-1, 208, 8, 8]             416\n",
      "          Conv2d-203            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-204            [-1, 208, 8, 8]             416\n",
      "          Conv2d-205            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-206            [-1, 208, 1, 1]          21,840\n",
      "              SE-207            [-1, 208, 8, 8]               0\n",
      "          Conv2d-208            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-209            [-1, 416, 8, 8]             832\n",
      "           Block-210            [-1, 416, 8, 8]               0\n",
      "          Conv2d-211            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-212            [-1, 208, 8, 8]             416\n",
      "          Conv2d-213            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-214            [-1, 208, 8, 8]             416\n",
      "          Conv2d-215            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-216            [-1, 208, 1, 1]          21,840\n",
      "              SE-217            [-1, 208, 8, 8]               0\n",
      "          Conv2d-218            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-219            [-1, 416, 8, 8]             832\n",
      "           Block-220            [-1, 416, 8, 8]               0\n",
      "          Conv2d-221            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-222            [-1, 208, 8, 8]             416\n",
      "          Conv2d-223            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-224            [-1, 208, 8, 8]             416\n",
      "          Conv2d-225            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-226            [-1, 208, 1, 1]          21,840\n",
      "              SE-227            [-1, 208, 8, 8]               0\n",
      "          Conv2d-228            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-229            [-1, 416, 8, 8]             832\n",
      "           Block-230            [-1, 416, 8, 8]               0\n",
      "          Conv2d-231            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-232            [-1, 208, 8, 8]             416\n",
      "          Conv2d-233            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-234            [-1, 208, 8, 8]             416\n",
      "          Conv2d-235            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-236            [-1, 208, 1, 1]          21,840\n",
      "              SE-237            [-1, 208, 8, 8]               0\n",
      "          Conv2d-238            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-239            [-1, 416, 8, 8]             832\n",
      "           Block-240            [-1, 416, 8, 8]               0\n",
      "          Conv2d-241            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-242            [-1, 208, 8, 8]             416\n",
      "          Conv2d-243            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-244            [-1, 208, 8, 8]             416\n",
      "          Conv2d-245            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-246            [-1, 208, 1, 1]          21,840\n",
      "              SE-247            [-1, 208, 8, 8]               0\n",
      "          Conv2d-248            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-249            [-1, 416, 8, 8]             832\n",
      "           Block-250            [-1, 416, 8, 8]               0\n",
      "          Conv2d-251            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-252            [-1, 208, 8, 8]             416\n",
      "          Conv2d-253            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-254            [-1, 208, 8, 8]             416\n",
      "          Conv2d-255            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-256            [-1, 208, 1, 1]          21,840\n",
      "              SE-257            [-1, 208, 8, 8]               0\n",
      "          Conv2d-258            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-259            [-1, 416, 8, 8]             832\n",
      "           Block-260            [-1, 416, 8, 8]               0\n",
      "          Linear-261                   [-1, 10]           4,170\n",
      "================================================================\n",
      "Total params: 3,373,098\n",
      "Trainable params: 3,373,098\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 44.65\n",
      "Params size (MB): 12.87\n",
      "Estimated Total Size (MB): 57.53\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Specify the net\n",
    "net = RegNet(cfg)\n",
    "net = net.to(device)\n",
    "torchsummary.summary(net,input_size=(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "if cfg['resume']:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=args.lr, betas=(0.9, 0.999),\n",
    "#                        eps=1e-08,weight_decay=0,amsgrad=False)       \n",
    "optimizer = optim.SGD(net.parameters(), lr=cfg['lr'],\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 30, T_mult=2, eta_min = 0, last_epoch = -1, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "\n",
    "    # print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        # progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        #              % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc = 100.*correct / total\n",
    "\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            # progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            #              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        test_loss = test_loss / len(testloader)\n",
    "        test_acc = 100.*correct/total\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "Epoch: 01 | Epoch Time: 1m 12s\n",
      "\tTrain Loss: 2.585 | Train Acc: 13.97%\n",
      "\t Test. Loss: 2.856 |  Test. Acc: 11.73 | Best Acc:11.73%\n",
      "Saving..\n",
      "Epoch: 02 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 1.984 | Train Acc: 22.32%\n",
      "\t Test. Loss: 2.034 |  Test. Acc: 19.27 | Best Acc:19.27%\n",
      "Saving..\n",
      "Epoch: 03 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 1.801 | Train Acc: 29.79%\n",
      "\t Test. Loss: 2.018 |  Test. Acc: 31.11 | Best Acc:31.11%\n",
      "Epoch: 04 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 1.574 | Train Acc: 40.79%\n",
      "\t Test. Loss: 2.863 |  Test. Acc: 29.46 | Best Acc:31.11%\n",
      "Saving..\n",
      "Epoch: 05 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 1.368 | Train Acc: 50.08%\n",
      "\t Test. Loss: 1.954 |  Test. Acc: 50.63 | Best Acc:50.63%\n",
      "Epoch: 06 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 1.164 | Train Acc: 58.25%\n",
      "\t Test. Loss: 2.125 |  Test. Acc: 50.22 | Best Acc:50.63%\n",
      "Saving..\n",
      "Epoch: 07 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 1.011 | Train Acc: 64.09%\n",
      "\t Test. Loss: 1.196 |  Test. Acc: 60.83 | Best Acc:60.83%\n",
      "Saving..\n",
      "Epoch: 08 | Epoch Time: 1m 9s\n",
      "\tTrain Loss: 0.872 | Train Acc: 69.36%\n",
      "\t Test. Loss: 1.030 |  Test. Acc: 64.59 | Best Acc:64.59%\n",
      "Saving..\n",
      "Epoch: 09 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.753 | Train Acc: 73.61%\n",
      "\t Test. Loss: 0.877 |  Test. Acc: 70.50 | Best Acc:70.50%\n",
      "Saving..\n",
      "Epoch: 10 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.650 | Train Acc: 77.52%\n",
      "\t Test. Loss: 0.687 |  Test. Acc: 76.10 | Best Acc:76.10%\n",
      "Saving..\n",
      "Epoch: 11 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.570 | Train Acc: 80.25%\n",
      "\t Test. Loss: 0.620 |  Test. Acc: 78.84 | Best Acc:78.84%\n",
      "Saving..\n",
      "Epoch: 12 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.505 | Train Acc: 82.53%\n",
      "\t Test. Loss: 0.566 |  Test. Acc: 80.59 | Best Acc:80.59%\n",
      "Saving..\n",
      "Epoch: 13 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.455 | Train Acc: 84.27%\n",
      "\t Test. Loss: 0.478 |  Test. Acc: 83.51 | Best Acc:83.51%\n",
      "Epoch: 14 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.403 | Train Acc: 86.22%\n",
      "\t Test. Loss: 0.523 |  Test. Acc: 82.06 | Best Acc:83.51%\n",
      "Saving..\n",
      "Epoch: 15 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.354 | Train Acc: 87.91%\n",
      "\t Test. Loss: 0.447 |  Test. Acc: 84.25 | Best Acc:84.25%\n",
      "Saving..\n",
      "Epoch: 16 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.304 | Train Acc: 89.56%\n",
      "\t Test. Loss: 0.373 |  Test. Acc: 87.20 | Best Acc:87.20%\n",
      "Saving..\n",
      "Epoch: 17 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.257 | Train Acc: 91.35%\n",
      "\t Test. Loss: 0.351 |  Test. Acc: 87.94 | Best Acc:87.94%\n",
      "Saving..\n",
      "Epoch: 18 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.218 | Train Acc: 92.75%\n",
      "\t Test. Loss: 0.307 |  Test. Acc: 89.71 | Best Acc:89.71%\n",
      "Saving..\n",
      "Epoch: 19 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.187 | Train Acc: 93.81%\n",
      "\t Test. Loss: 0.297 |  Test. Acc: 90.04 | Best Acc:90.04%\n",
      "Saving..\n",
      "Epoch: 20 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.170 | Train Acc: 94.40%\n",
      "\t Test. Loss: 0.293 |  Test. Acc: 90.20 | Best Acc:90.20%\n",
      "Saving..\n",
      "Epoch: 21 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.165 | Train Acc: 94.66%\n",
      "\t Test. Loss: 0.293 |  Test. Acc: 90.42 | Best Acc:90.42%\n",
      "Epoch: 22 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.169 | Train Acc: 94.46%\n",
      "\t Test. Loss: 0.293 |  Test. Acc: 90.15 | Best Acc:90.42%\n",
      "Epoch: 23 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.171 | Train Acc: 94.38%\n",
      "\t Test. Loss: 0.299 |  Test. Acc: 90.04 | Best Acc:90.42%\n",
      "Epoch: 24 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.183 | Train Acc: 93.87%\n",
      "\t Test. Loss: 0.338 |  Test. Acc: 88.52 | Best Acc:90.42%\n",
      "Epoch: 25 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.218 | Train Acc: 92.48%\n",
      "\t Test. Loss: 0.353 |  Test. Acc: 88.32 | Best Acc:90.42%\n",
      "Epoch: 26 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.263 | Train Acc: 90.76%\n",
      "\t Test. Loss: 0.402 |  Test. Acc: 86.62 | Best Acc:90.42%\n",
      "Epoch: 27 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.313 | Train Acc: 89.09%\n",
      "\t Test. Loss: 0.433 |  Test. Acc: 85.10 | Best Acc:90.42%\n",
      "Epoch: 28 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.357 | Train Acc: 87.64%\n",
      "\t Test. Loss: 0.508 |  Test. Acc: 82.87 | Best Acc:90.42%\n",
      "Epoch: 29 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.378 | Train Acc: 86.92%\n",
      "\t Test. Loss: 0.449 |  Test. Acc: 84.39 | Best Acc:90.42%\n",
      "Epoch: 30 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.399 | Train Acc: 86.12%\n",
      "\t Test. Loss: 0.466 |  Test. Acc: 83.85 | Best Acc:90.42%\n",
      "Epoch: 31 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.420 | Train Acc: 85.62%\n",
      "\t Test. Loss: 0.524 |  Test. Acc: 82.47 | Best Acc:90.42%\n",
      "Epoch: 32 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.434 | Train Acc: 85.11%\n",
      "\t Test. Loss: 0.547 |  Test. Acc: 81.31 | Best Acc:90.42%\n",
      "Epoch: 33 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.435 | Train Acc: 85.03%\n",
      "\t Test. Loss: 0.642 |  Test. Acc: 79.00 | Best Acc:90.42%\n",
      "Epoch: 34 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.437 | Train Acc: 84.94%\n",
      "\t Test. Loss: 0.545 |  Test. Acc: 81.54 | Best Acc:90.42%\n",
      "Epoch: 35 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.447 | Train Acc: 84.51%\n",
      "\t Test. Loss: 0.603 |  Test. Acc: 79.64 | Best Acc:90.42%\n",
      "Epoch: 36 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.445 | Train Acc: 84.80%\n",
      "\t Test. Loss: 0.891 |  Test. Acc: 75.73 | Best Acc:90.42%\n",
      "Epoch: 37 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.442 | Train Acc: 84.82%\n",
      "\t Test. Loss: 0.567 |  Test. Acc: 81.02 | Best Acc:90.42%\n",
      "Epoch: 38 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.449 | Train Acc: 84.54%\n",
      "\t Test. Loss: 0.563 |  Test. Acc: 80.76 | Best Acc:90.42%\n",
      "Epoch: 39 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.438 | Train Acc: 84.96%\n",
      "\t Test. Loss: 0.630 |  Test. Acc: 78.94 | Best Acc:90.42%\n",
      "Epoch: 40 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.434 | Train Acc: 85.25%\n",
      "\t Test. Loss: 0.508 |  Test. Acc: 82.55 | Best Acc:90.42%\n",
      "Epoch: 41 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.435 | Train Acc: 84.93%\n",
      "\t Test. Loss: 0.559 |  Test. Acc: 80.74 | Best Acc:90.42%\n",
      "Epoch: 42 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.424 | Train Acc: 85.31%\n",
      "\t Test. Loss: 0.508 |  Test. Acc: 82.40 | Best Acc:90.42%\n",
      "Epoch: 43 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.413 | Train Acc: 85.75%\n",
      "\t Test. Loss: 0.524 |  Test. Acc: 81.78 | Best Acc:90.42%\n",
      "Epoch: 44 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.402 | Train Acc: 86.15%\n",
      "\t Test. Loss: 0.621 |  Test. Acc: 79.18 | Best Acc:90.42%\n",
      "Epoch: 45 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.388 | Train Acc: 86.66%\n",
      "\t Test. Loss: 0.474 |  Test. Acc: 83.71 | Best Acc:90.42%\n",
      "Epoch: 46 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.371 | Train Acc: 87.32%\n",
      "\t Test. Loss: 0.522 |  Test. Acc: 82.31 | Best Acc:90.42%\n",
      "Epoch: 47 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.354 | Train Acc: 87.68%\n",
      "\t Test. Loss: 0.495 |  Test. Acc: 83.59 | Best Acc:90.42%\n",
      "Epoch: 48 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.333 | Train Acc: 88.46%\n",
      "\t Test. Loss: 0.541 |  Test. Acc: 82.53 | Best Acc:90.42%\n",
      "Epoch: 49 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.318 | Train Acc: 88.91%\n",
      "\t Test. Loss: 0.592 |  Test. Acc: 79.95 | Best Acc:90.42%\n",
      "Epoch: 50 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.296 | Train Acc: 89.72%\n",
      "\t Test. Loss: 0.586 |  Test. Acc: 81.07 | Best Acc:90.42%\n",
      "Epoch: 51 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.268 | Train Acc: 90.77%\n",
      "\t Test. Loss: 0.378 |  Test. Acc: 87.14 | Best Acc:90.42%\n",
      "Epoch: 52 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.232 | Train Acc: 92.02%\n",
      "\t Test. Loss: 0.331 |  Test. Acc: 88.72 | Best Acc:90.42%\n",
      "Epoch: 53 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.205 | Train Acc: 92.93%\n",
      "\t Test. Loss: 0.310 |  Test. Acc: 89.96 | Best Acc:90.42%\n",
      "Saving..\n",
      "Epoch: 54 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.169 | Train Acc: 94.30%\n",
      "\t Test. Loss: 0.267 |  Test. Acc: 91.14 | Best Acc:91.14%\n",
      "Epoch: 55 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.139 | Train Acc: 95.35%\n",
      "\t Test. Loss: 0.259 |  Test. Acc: 90.96 | Best Acc:91.14%\n",
      "Saving..\n",
      "Epoch: 56 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.110 | Train Acc: 96.30%\n",
      "\t Test. Loss: 0.221 |  Test. Acc: 92.83 | Best Acc:92.83%\n",
      "Saving..\n",
      "Epoch: 57 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.083 | Train Acc: 97.28%\n",
      "\t Test. Loss: 0.203 |  Test. Acc: 93.32 | Best Acc:93.32%\n",
      "Saving..\n",
      "Epoch: 58 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.063 | Train Acc: 98.09%\n",
      "\t Test. Loss: 0.188 |  Test. Acc: 93.87 | Best Acc:93.87%\n",
      "Saving..\n",
      "Epoch: 59 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.050 | Train Acc: 98.67%\n",
      "\t Test. Loss: 0.183 |  Test. Acc: 93.98 | Best Acc:93.98%\n",
      "Saving..\n",
      "Epoch: 60 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.045 | Train Acc: 98.79%\n",
      "\t Test. Loss: 0.179 |  Test. Acc: 94.21 | Best Acc:94.21%\n",
      "Epoch: 61 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.90%\n",
      "\t Test. Loss: 0.178 |  Test. Acc: 94.20 | Best Acc:94.21%\n",
      "Epoch: 62 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.82%\n",
      "\t Test. Loss: 0.180 |  Test. Acc: 94.16 | Best Acc:94.21%\n",
      "Saving..\n",
      "Epoch: 63 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.81%\n",
      "\t Test. Loss: 0.180 |  Test. Acc: 94.22 | Best Acc:94.22%\n",
      "Epoch: 64 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.047 | Train Acc: 98.66%\n",
      "\t Test. Loss: 0.187 |  Test. Acc: 93.89 | Best Acc:94.22%\n",
      "Epoch: 65 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.059 | Train Acc: 98.21%\n",
      "\t Test. Loss: 0.213 |  Test. Acc: 92.97 | Best Acc:94.22%\n",
      "Epoch: 66 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.085 | Train Acc: 97.23%\n",
      "\t Test. Loss: 0.287 |  Test. Acc: 91.16 | Best Acc:94.22%\n",
      "Epoch: 67 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.144 | Train Acc: 95.00%\n",
      "\t Test. Loss: 0.282 |  Test. Acc: 90.83 | Best Acc:94.22%\n",
      "Epoch: 68 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.203 | Train Acc: 92.98%\n",
      "\t Test. Loss: 0.366 |  Test. Acc: 87.66 | Best Acc:94.22%\n",
      "Epoch: 69 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.250 | Train Acc: 91.27%\n",
      "\t Test. Loss: 0.371 |  Test. Acc: 87.49 | Best Acc:94.22%\n",
      "Epoch: 70 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.273 | Train Acc: 90.44%\n",
      "\t Test. Loss: 0.457 |  Test. Acc: 84.85 | Best Acc:94.22%\n",
      "Epoch: 71 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.301 | Train Acc: 89.59%\n",
      "\t Test. Loss: 0.398 |  Test. Acc: 86.38 | Best Acc:94.22%\n",
      "Epoch: 72 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.314 | Train Acc: 89.16%\n",
      "\t Test. Loss: 0.436 |  Test. Acc: 85.16 | Best Acc:94.22%\n",
      "Epoch: 73 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.329 | Train Acc: 88.55%\n",
      "\t Test. Loss: 0.455 |  Test. Acc: 84.71 | Best Acc:94.22%\n",
      "Epoch: 74 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.339 | Train Acc: 88.19%\n",
      "\t Test. Loss: 0.596 |  Test. Acc: 79.84 | Best Acc:94.22%\n",
      "Epoch: 75 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.344 | Train Acc: 88.08%\n",
      "\t Test. Loss: 0.755 |  Test. Acc: 77.34 | Best Acc:94.22%\n",
      "Epoch: 76 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.360 | Train Acc: 87.61%\n",
      "\t Test. Loss: 0.663 |  Test. Acc: 78.90 | Best Acc:94.22%\n",
      "Epoch: 77 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.366 | Train Acc: 87.27%\n",
      "\t Test. Loss: 0.450 |  Test. Acc: 84.40 | Best Acc:94.22%\n",
      "Epoch: 78 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.362 | Train Acc: 87.38%\n",
      "\t Test. Loss: 0.500 |  Test. Acc: 83.18 | Best Acc:94.22%\n",
      "Epoch: 79 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.370 | Train Acc: 87.21%\n",
      "\t Test. Loss: 0.721 |  Test. Acc: 76.35 | Best Acc:94.22%\n",
      "Epoch: 80 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.363 | Train Acc: 87.51%\n",
      "\t Test. Loss: 0.573 |  Test. Acc: 81.38 | Best Acc:94.22%\n",
      "Epoch: 81 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.362 | Train Acc: 87.50%\n",
      "\t Test. Loss: 0.529 |  Test. Acc: 82.10 | Best Acc:94.22%\n",
      "Epoch: 82 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.360 | Train Acc: 87.74%\n",
      "\t Test. Loss: 0.458 |  Test. Acc: 84.50 | Best Acc:94.22%\n",
      "Epoch: 83 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.352 | Train Acc: 87.84%\n",
      "\t Test. Loss: 0.562 |  Test. Acc: 80.93 | Best Acc:94.22%\n",
      "Epoch: 84 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.349 | Train Acc: 88.04%\n",
      "\t Test. Loss: 0.533 |  Test. Acc: 82.63 | Best Acc:94.22%\n",
      "Epoch: 85 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.340 | Train Acc: 88.22%\n",
      "\t Test. Loss: 0.426 |  Test. Acc: 85.33 | Best Acc:94.22%\n",
      "Epoch: 86 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.319 | Train Acc: 89.00%\n",
      "\t Test. Loss: 0.511 |  Test. Acc: 82.74 | Best Acc:94.22%\n",
      "Epoch: 87 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.307 | Train Acc: 89.28%\n",
      "\t Test. Loss: 0.452 |  Test. Acc: 84.68 | Best Acc:94.22%\n",
      "Epoch: 88 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.295 | Train Acc: 89.80%\n",
      "\t Test. Loss: 0.438 |  Test. Acc: 85.17 | Best Acc:94.22%\n",
      "Epoch: 89 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.274 | Train Acc: 90.59%\n",
      "\t Test. Loss: 0.479 |  Test. Acc: 84.15 | Best Acc:94.22%\n",
      "Epoch: 90 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.248 | Train Acc: 91.46%\n",
      "\t Test. Loss: 0.400 |  Test. Acc: 86.75 | Best Acc:94.22%\n",
      "Epoch: 91 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.223 | Train Acc: 92.37%\n",
      "\t Test. Loss: 0.376 |  Test. Acc: 87.54 | Best Acc:94.22%\n",
      "Epoch: 92 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.194 | Train Acc: 93.42%\n",
      "\t Test. Loss: 0.306 |  Test. Acc: 89.31 | Best Acc:94.22%\n",
      "Epoch: 93 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.167 | Train Acc: 94.27%\n",
      "\t Test. Loss: 0.287 |  Test. Acc: 90.06 | Best Acc:94.22%\n",
      "Epoch: 94 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.138 | Train Acc: 95.29%\n",
      "\t Test. Loss: 0.248 |  Test. Acc: 91.82 | Best Acc:94.22%\n",
      "Epoch: 95 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.105 | Train Acc: 96.54%\n",
      "\t Test. Loss: 0.230 |  Test. Acc: 92.08 | Best Acc:94.22%\n",
      "Epoch: 96 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.077 | Train Acc: 97.58%\n",
      "\t Test. Loss: 0.202 |  Test. Acc: 93.11 | Best Acc:94.22%\n",
      "Epoch: 97 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.057 | Train Acc: 98.30%\n",
      "\t Test. Loss: 0.186 |  Test. Acc: 93.93 | Best Acc:94.22%\n",
      "Epoch: 98 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.86%\n",
      "\t Test. Loss: 0.178 |  Test. Acc: 94.13 | Best Acc:94.22%\n",
      "Epoch: 99 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.033 | Train Acc: 99.18%\n",
      "\t Test. Loss: 0.175 |  Test. Acc: 94.02 | Best Acc:94.22%\n",
      "Epoch: 100 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.029 | Train Acc: 99.33%\n",
      "\t Test. Loss: 0.174 |  Test. Acc: 94.13 | Best Acc:94.22%\n",
      "Epoch: 101 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.029 | Train Acc: 99.38%\n",
      "\t Test. Loss: 0.175 |  Test. Acc: 94.10 | Best Acc:94.22%\n",
      "Epoch: 102 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.029 | Train Acc: 99.35%\n",
      "\t Test. Loss: 0.174 |  Test. Acc: 94.14 | Best Acc:94.22%\n",
      "Epoch: 103 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.030 | Train Acc: 99.27%\n",
      "\t Test. Loss: 0.173 |  Test. Acc: 94.14 | Best Acc:94.22%\n",
      "Epoch: 104 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.031 | Train Acc: 99.25%\n",
      "\t Test. Loss: 0.180 |  Test. Acc: 94.08 | Best Acc:94.22%\n",
      "Epoch: 105 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.96%\n",
      "\t Test. Loss: 0.205 |  Test. Acc: 93.45 | Best Acc:94.22%\n",
      "Epoch: 106 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.055 | Train Acc: 98.31%\n",
      "\t Test. Loss: 0.238 |  Test. Acc: 92.78 | Best Acc:94.22%\n",
      "Epoch: 107 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.111 | Train Acc: 96.24%\n",
      "\t Test. Loss: 0.303 |  Test. Acc: 89.92 | Best Acc:94.22%\n",
      "Epoch: 108 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.174 | Train Acc: 93.96%\n",
      "\t Test. Loss: 0.385 |  Test. Acc: 87.60 | Best Acc:94.22%\n",
      "Epoch: 109 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.216 | Train Acc: 92.39%\n",
      "\t Test. Loss: 0.441 |  Test. Acc: 86.55 | Best Acc:94.22%\n",
      "Epoch: 110 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.254 | Train Acc: 91.12%\n",
      "\t Test. Loss: 0.439 |  Test. Acc: 85.46 | Best Acc:94.22%\n",
      "Epoch: 111 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.268 | Train Acc: 90.75%\n",
      "\t Test. Loss: 0.393 |  Test. Acc: 87.05 | Best Acc:94.22%\n",
      "Epoch: 112 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.294 | Train Acc: 89.84%\n",
      "\t Test. Loss: 0.506 |  Test. Acc: 83.55 | Best Acc:94.22%\n",
      "Epoch: 113 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.307 | Train Acc: 89.33%\n",
      "\t Test. Loss: 0.498 |  Test. Acc: 83.67 | Best Acc:94.22%\n",
      "Epoch: 114 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.309 | Train Acc: 89.31%\n",
      "\t Test. Loss: 0.413 |  Test. Acc: 86.00 | Best Acc:94.22%\n",
      "Epoch: 115 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.319 | Train Acc: 88.94%\n",
      "\t Test. Loss: 0.433 |  Test. Acc: 85.41 | Best Acc:94.22%\n",
      "Epoch: 116 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.334 | Train Acc: 88.29%\n",
      "\t Test. Loss: 0.611 |  Test. Acc: 80.33 | Best Acc:94.22%\n",
      "Epoch: 117 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.329 | Train Acc: 88.61%\n",
      "\t Test. Loss: 0.742 |  Test. Acc: 77.26 | Best Acc:94.22%\n",
      "Epoch: 118 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.338 | Train Acc: 88.41%\n",
      "\t Test. Loss: 0.549 |  Test. Acc: 81.59 | Best Acc:94.22%\n",
      "Epoch: 119 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.347 | Train Acc: 88.00%\n",
      "\t Test. Loss: 0.442 |  Test. Acc: 84.60 | Best Acc:94.22%\n",
      "Epoch: 120 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.343 | Train Acc: 88.19%\n",
      "\t Test. Loss: 0.790 |  Test. Acc: 73.36 | Best Acc:94.22%\n",
      "Epoch: 121 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.343 | Train Acc: 88.00%\n",
      "\t Test. Loss: 0.490 |  Test. Acc: 82.96 | Best Acc:94.22%\n",
      "Epoch: 122 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.343 | Train Acc: 88.15%\n",
      "\t Test. Loss: 0.907 |  Test. Acc: 73.04 | Best Acc:94.22%\n",
      "Epoch: 123 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.342 | Train Acc: 88.01%\n",
      "\t Test. Loss: 0.636 |  Test. Acc: 79.38 | Best Acc:94.22%\n",
      "Epoch: 124 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.326 | Train Acc: 88.72%\n",
      "\t Test. Loss: 0.509 |  Test. Acc: 82.56 | Best Acc:94.22%\n",
      "Epoch: 125 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.324 | Train Acc: 88.81%\n",
      "\t Test. Loss: 0.762 |  Test. Acc: 76.10 | Best Acc:94.22%\n",
      "Epoch: 126 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.304 | Train Acc: 89.49%\n",
      "\t Test. Loss: 0.367 |  Test. Acc: 87.32 | Best Acc:94.22%\n",
      "Epoch: 127 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.289 | Train Acc: 89.99%\n",
      "\t Test. Loss: 0.529 |  Test. Acc: 82.55 | Best Acc:94.22%\n",
      "Epoch: 128 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.273 | Train Acc: 90.62%\n",
      "\t Test. Loss: 0.461 |  Test. Acc: 84.80 | Best Acc:94.22%\n",
      "Epoch: 129 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.260 | Train Acc: 90.98%\n",
      "\t Test. Loss: 0.363 |  Test. Acc: 88.14 | Best Acc:94.22%\n",
      "Epoch: 130 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.238 | Train Acc: 91.82%\n",
      "\t Test. Loss: 0.489 |  Test. Acc: 83.29 | Best Acc:94.22%\n",
      "Epoch: 131 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.209 | Train Acc: 92.71%\n",
      "\t Test. Loss: 0.384 |  Test. Acc: 87.42 | Best Acc:94.22%\n",
      "Epoch: 132 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.179 | Train Acc: 93.86%\n",
      "\t Test. Loss: 0.319 |  Test. Acc: 89.14 | Best Acc:94.22%\n",
      "Epoch: 133 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.151 | Train Acc: 94.76%\n",
      "\t Test. Loss: 0.260 |  Test. Acc: 91.50 | Best Acc:94.22%\n",
      "Epoch: 134 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.126 | Train Acc: 95.79%\n",
      "\t Test. Loss: 0.242 |  Test. Acc: 91.77 | Best Acc:94.22%\n",
      "Epoch: 135 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.094 | Train Acc: 96.96%\n",
      "\t Test. Loss: 0.204 |  Test. Acc: 93.36 | Best Acc:94.22%\n",
      "Epoch: 136 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.067 | Train Acc: 97.91%\n",
      "\t Test. Loss: 0.188 |  Test. Acc: 93.82 | Best Acc:94.22%\n",
      "Epoch: 137 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.049 | Train Acc: 98.58%\n",
      "\t Test. Loss: 0.181 |  Test. Acc: 94.17 | Best Acc:94.22%\n",
      "Saving..\n",
      "Epoch: 138 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.036 | Train Acc: 99.08%\n",
      "\t Test. Loss: 0.168 |  Test. Acc: 94.65 | Best Acc:94.65%\n",
      "Saving..\n",
      "Epoch: 139 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.029 | Train Acc: 99.29%\n",
      "\t Test. Loss: 0.164 |  Test. Acc: 94.84 | Best Acc:94.84%\n",
      "Epoch: 140 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.026 | Train Acc: 99.46%\n",
      "\t Test. Loss: 0.164 |  Test. Acc: 94.84 | Best Acc:94.84%\n",
      "Epoch: 141 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.025 | Train Acc: 99.47%\n",
      "\t Test. Loss: 0.163 |  Test. Acc: 94.76 | Best Acc:94.84%\n",
      "Saving..\n",
      "Epoch: 142 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.025 | Train Acc: 99.52%\n",
      "\t Test. Loss: 0.163 |  Test. Acc: 94.87 | Best Acc:94.87%\n",
      "Epoch: 143 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.024 | Train Acc: 99.51%\n",
      "\t Test. Loss: 0.166 |  Test. Acc: 94.76 | Best Acc:94.87%\n",
      "Epoch: 144 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.024 | Train Acc: 99.46%\n",
      "\t Test. Loss: 0.169 |  Test. Acc: 94.49 | Best Acc:94.87%\n",
      "Epoch: 145 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.031 | Train Acc: 99.17%\n",
      "\t Test. Loss: 0.185 |  Test. Acc: 94.14 | Best Acc:94.87%\n",
      "Epoch: 146 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.049 | Train Acc: 98.51%\n",
      "\t Test. Loss: 0.243 |  Test. Acc: 92.16 | Best Acc:94.87%\n",
      "Epoch: 147 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.102 | Train Acc: 96.42%\n",
      "\t Test. Loss: 0.272 |  Test. Acc: 91.26 | Best Acc:94.87%\n",
      "Epoch: 148 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.161 | Train Acc: 94.52%\n",
      "\t Test. Loss: 0.354 |  Test. Acc: 88.81 | Best Acc:94.87%\n",
      "Epoch: 149 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.206 | Train Acc: 92.88%\n",
      "\t Test. Loss: 0.436 |  Test. Acc: 85.75 | Best Acc:94.87%\n",
      "Epoch: 150 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.239 | Train Acc: 91.67%\n",
      "\t Test. Loss: 0.406 |  Test. Acc: 86.75 | Best Acc:94.87%\n",
      "Epoch: 151 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.266 | Train Acc: 90.79%\n",
      "\t Test. Loss: 0.361 |  Test. Acc: 87.62 | Best Acc:94.87%\n",
      "Epoch: 152 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.279 | Train Acc: 90.34%\n",
      "\t Test. Loss: 0.428 |  Test. Acc: 85.65 | Best Acc:94.87%\n",
      "Epoch: 153 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.290 | Train Acc: 90.06%\n",
      "\t Test. Loss: 0.444 |  Test. Acc: 85.50 | Best Acc:94.87%\n",
      "Epoch: 154 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.299 | Train Acc: 89.75%\n",
      "\t Test. Loss: 0.543 |  Test. Acc: 82.05 | Best Acc:94.87%\n",
      "Epoch: 155 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.313 | Train Acc: 89.25%\n",
      "\t Test. Loss: 0.649 |  Test. Acc: 77.85 | Best Acc:94.87%\n",
      "Epoch: 156 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.323 | Train Acc: 88.98%\n",
      "\t Test. Loss: 0.549 |  Test. Acc: 81.98 | Best Acc:94.87%\n",
      "Epoch: 157 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.326 | Train Acc: 88.61%\n",
      "\t Test. Loss: 0.516 |  Test. Acc: 83.21 | Best Acc:94.87%\n",
      "Epoch: 158 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.324 | Train Acc: 88.68%\n",
      "\t Test. Loss: 0.682 |  Test. Acc: 78.45 | Best Acc:94.87%\n",
      "Epoch: 159 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.335 | Train Acc: 88.28%\n",
      "\t Test. Loss: 0.742 |  Test. Acc: 76.27 | Best Acc:94.87%\n",
      "Epoch: 160 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.332 | Train Acc: 88.63%\n",
      "\t Test. Loss: 0.627 |  Test. Acc: 79.75 | Best Acc:94.87%\n",
      "Epoch: 161 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.333 | Train Acc: 88.66%\n",
      "\t Test. Loss: 0.553 |  Test. Acc: 82.71 | Best Acc:94.87%\n",
      "Epoch: 162 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.335 | Train Acc: 88.49%\n",
      "\t Test. Loss: 0.674 |  Test. Acc: 78.65 | Best Acc:94.87%\n",
      "Epoch: 163 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.322 | Train Acc: 88.99%\n",
      "\t Test. Loss: 0.539 |  Test. Acc: 82.36 | Best Acc:94.87%\n",
      "Epoch: 164 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.323 | Train Acc: 88.98%\n",
      "\t Test. Loss: 0.746 |  Test. Acc: 75.98 | Best Acc:94.87%\n",
      "Epoch: 165 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.312 | Train Acc: 89.18%\n",
      "\t Test. Loss: 0.485 |  Test. Acc: 83.87 | Best Acc:94.87%\n",
      "Epoch: 166 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.295 | Train Acc: 89.79%\n",
      "\t Test. Loss: 0.520 |  Test. Acc: 83.15 | Best Acc:94.87%\n",
      "Epoch: 167 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.284 | Train Acc: 90.19%\n",
      "\t Test. Loss: 0.482 |  Test. Acc: 84.45 | Best Acc:94.87%\n",
      "Epoch: 168 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.267 | Train Acc: 90.79%\n",
      "\t Test. Loss: 0.576 |  Test. Acc: 81.58 | Best Acc:94.87%\n",
      "Epoch: 169 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.244 | Train Acc: 91.60%\n",
      "\t Test. Loss: 0.481 |  Test. Acc: 83.95 | Best Acc:94.87%\n",
      "Epoch: 170 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.220 | Train Acc: 92.61%\n",
      "\t Test. Loss: 0.345 |  Test. Acc: 88.58 | Best Acc:94.87%\n",
      "Epoch: 171 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.199 | Train Acc: 93.08%\n",
      "\t Test. Loss: 0.311 |  Test. Acc: 89.51 | Best Acc:94.87%\n",
      "Epoch: 172 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.175 | Train Acc: 93.91%\n",
      "\t Test. Loss: 0.316 |  Test. Acc: 89.57 | Best Acc:94.87%\n",
      "Epoch: 173 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.143 | Train Acc: 95.19%\n",
      "\t Test. Loss: 0.258 |  Test. Acc: 91.30 | Best Acc:94.87%\n",
      "Epoch: 174 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.114 | Train Acc: 96.06%\n",
      "\t Test. Loss: 0.224 |  Test. Acc: 92.57 | Best Acc:94.87%\n",
      "Epoch: 175 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.085 | Train Acc: 97.14%\n",
      "\t Test. Loss: 0.214 |  Test. Acc: 93.04 | Best Acc:94.87%\n",
      "Epoch: 176 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.060 | Train Acc: 98.21%\n",
      "\t Test. Loss: 0.184 |  Test. Acc: 94.11 | Best Acc:94.87%\n",
      "Epoch: 177 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.90%\n",
      "\t Test. Loss: 0.172 |  Test. Acc: 94.61 | Best Acc:94.87%\n",
      "Epoch: 178 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.031 | Train Acc: 99.18%\n",
      "\t Test. Loss: 0.169 |  Test. Acc: 94.82 | Best Acc:94.87%\n",
      "Epoch: 179 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.024 | Train Acc: 99.47%\n",
      "\t Test. Loss: 0.165 |  Test. Acc: 94.86 | Best Acc:94.87%\n",
      "Epoch: 180 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.022 | Train Acc: 99.55%\n",
      "\t Test. Loss: 0.163 |  Test. Acc: 94.81 | Best Acc:94.87%\n",
      "Epoch: 181 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.022 | Train Acc: 99.53%\n",
      "\t Test. Loss: 0.165 |  Test. Acc: 94.87 | Best Acc:94.87%\n",
      "Saving..\n",
      "Epoch: 182 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.022 | Train Acc: 99.58%\n",
      "\t Test. Loss: 0.163 |  Test. Acc: 95.00 | Best Acc:95.00%\n",
      "Epoch: 183 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.021 | Train Acc: 99.57%\n",
      "\t Test. Loss: 0.168 |  Test. Acc: 94.77 | Best Acc:95.00%\n",
      "Epoch: 184 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.022 | Train Acc: 99.55%\n",
      "\t Test. Loss: 0.168 |  Test. Acc: 94.89 | Best Acc:95.00%\n",
      "Epoch: 185 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.027 | Train Acc: 99.34%\n",
      "\t Test. Loss: 0.182 |  Test. Acc: 94.38 | Best Acc:95.00%\n",
      "Epoch: 186 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.045 | Train Acc: 98.65%\n",
      "\t Test. Loss: 0.219 |  Test. Acc: 93.19 | Best Acc:95.00%\n",
      "Epoch: 187 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.091 | Train Acc: 96.84%\n",
      "\t Test. Loss: 0.272 |  Test. Acc: 91.45 | Best Acc:95.00%\n",
      "Epoch: 188 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.158 | Train Acc: 94.50%\n",
      "\t Test. Loss: 0.307 |  Test. Acc: 89.60 | Best Acc:95.00%\n",
      "Epoch: 189 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.204 | Train Acc: 92.90%\n",
      "\t Test. Loss: 0.342 |  Test. Acc: 88.43 | Best Acc:95.00%\n",
      "Epoch: 190 | Epoch Time: 1m 12s\n",
      "\tTrain Loss: 0.235 | Train Acc: 91.87%\n",
      "\t Test. Loss: 0.352 |  Test. Acc: 88.06 | Best Acc:95.00%\n",
      "Epoch: 191 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.259 | Train Acc: 90.89%\n",
      "\t Test. Loss: 0.446 |  Test. Acc: 85.30 | Best Acc:95.00%\n",
      "Epoch: 192 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.275 | Train Acc: 90.47%\n",
      "\t Test. Loss: 0.495 |  Test. Acc: 83.87 | Best Acc:95.00%\n",
      "Epoch: 193 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.290 | Train Acc: 90.01%\n",
      "\t Test. Loss: 0.606 |  Test. Acc: 81.10 | Best Acc:95.00%\n",
      "Epoch: 194 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.292 | Train Acc: 89.95%\n",
      "\t Test. Loss: 0.609 |  Test. Acc: 81.83 | Best Acc:95.00%\n",
      "Epoch: 195 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.304 | Train Acc: 89.51%\n",
      "\t Test. Loss: 0.433 |  Test. Acc: 85.32 | Best Acc:95.00%\n",
      "Epoch: 196 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.316 | Train Acc: 89.19%\n",
      "\t Test. Loss: 0.519 |  Test. Acc: 82.17 | Best Acc:95.00%\n",
      "Epoch: 197 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.315 | Train Acc: 89.04%\n",
      "\t Test. Loss: 0.461 |  Test. Acc: 84.50 | Best Acc:95.00%\n",
      "Epoch: 198 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.319 | Train Acc: 89.03%\n",
      "\t Test. Loss: 0.560 |  Test. Acc: 81.22 | Best Acc:95.00%\n",
      "Epoch: 199 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.323 | Train Acc: 88.92%\n",
      "\t Test. Loss: 0.426 |  Test. Acc: 85.81 | Best Acc:95.00%\n",
      "Epoch: 200 | Epoch Time: 1m 12s\n",
      "\tTrain Loss: 0.333 | Train Acc: 88.38%\n",
      "\t Test. Loss: 0.520 |  Test. Acc: 82.21 | Best Acc:95.00%\n"
     ]
    }
   ],
   "source": [
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "EPOCHS = 200\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    train_loss, train_acc = train(epoch)\n",
    "    test_loss, test_acc = test(epoch)\n",
    "    train_loss_history.append(train_loss)\n",
    "    test_loss_history.append(test_loss)\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    scheduler.step()\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')\n",
    "    print(f'\\t Test. Loss: {test_loss:.3f} |  Test. Acc: {test_acc:.2f} | Best Acc:{best_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"output\\lr\\lr=0.1_t0=20.txt\"\n",
    "output = open('data.txt','w',encoding = 'gbk')\n",
    "for i in range(len(train_loss_history)):\n",
    "    rowtxt = '{},{}'.format(train_loss_history[i],test_loss_history[i])\n",
    "    output.write(rowtxt)\n",
    "    output.write('\\n')\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.163 | Test Acc: 95.00%\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "best_acc = checkpoint['acc']\n",
    "start_epoch = checkpoint['epoch']\n",
    "test_loss, test_acc = test(1)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x162aa9dbdf0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(start_epoch + 200), train_loss_history, '-', linewidth=3, label = 'Train error')\n",
    "plt.plot(range(start_epoch + 200), test_loss_history, '-', linewidth=3, label = 'Test error')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac8eaebebc05f64b54d631ff4a3b0f7c44e3b1a3f62a32c174bc0acd3614f160"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
