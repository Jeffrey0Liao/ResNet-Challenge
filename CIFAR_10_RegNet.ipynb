{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train CIFAR10 with PyTorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "from models import *\n",
    "from utils import progress_bar\n",
    "import torchsummary\n",
    "import os\n",
    "import time\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
    "# parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
    "# parser.add_argument('--resume', '-r', action='store_true',\n",
    "#                     help='resume from checkpoint')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print('==> Building model..')\n",
    "class SE(nn.Module):\n",
    "    '''Squeeze-and-Excitation block.'''\n",
    "\n",
    "    def __init__(self, in_planes, se_planes):\n",
    "        super(SE, self).__init__()\n",
    "        self.se1 = nn.Conv2d(in_planes, se_planes, kernel_size=1, bias=True)\n",
    "        self.se2 = nn.Conv2d(se_planes, in_planes, kernel_size=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        out = F.relu(self.se1(out)) # squeeze\n",
    "        out = self.se2(out).sigmoid() # excitation\n",
    "        out = x * out # inception and resnet(reweight)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, w_in, w_out, stride, group_width, bottleneck_ratio, se_ratio):\n",
    "        super(Block, self).__init__()\n",
    "        # 1x1\n",
    "        w_b = int(round(w_out * bottleneck_ratio)) # the middle layer is slim, to end is thick so it is like a bottleneck\n",
    "        self.conv1 = nn.Conv2d(w_in, w_b, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(w_b)\n",
    "        # 3x3\n",
    "        num_groups = w_b // group_width #Group the corresponding input channels and number of output channels\n",
    "        self.conv2 = nn.Conv2d(w_b, w_b, kernel_size=3,\n",
    "                               stride=stride, padding=1, groups=num_groups, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(w_b)\n",
    "        # se\n",
    "        self.with_se = se_ratio > 0\n",
    "        if self.with_se:\n",
    "            w_se = int(round(w_in * se_ratio))\n",
    "            self.se = SE(w_b, w_se)\n",
    "        # 1x1\n",
    "        self.conv3 = nn.Conv2d(w_b, w_out, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(w_out)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or w_in != w_out:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(w_in, w_out,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(w_out)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        if self.with_se:\n",
    "            out = self.se(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegNet(nn.Module):\n",
    "    def __init__(self, cfg, num_classes=10):\n",
    "        super(RegNet, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(0)\n",
    "        self.layer2 = self._make_layer(1)\n",
    "        self.layer3 = self._make_layer(2)\n",
    "        self.layer4 = self._make_layer(3)\n",
    "        self.linear = nn.Linear(self.cfg['widths'][-1], num_classes)\n",
    "\n",
    "    def _make_layer(self, idx):\n",
    "        depth = self.cfg['depths'][idx]\n",
    "        width = self.cfg['widths'][idx]\n",
    "        stride = self.cfg['strides'][idx]\n",
    "        group_width = self.cfg['group_width']\n",
    "        bottleneck_ratio = self.cfg['bottleneck_ratio']\n",
    "        se_ratio = self.cfg['se_ratio']\n",
    "\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            s = stride if i == 0 else 1\n",
    "            layers.append(Block(self.in_planes, width,\n",
    "                                s, group_width, bottleneck_ratio, se_ratio))\n",
    "            self.in_planes = width\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'depths': [1, 4, 8, 12],\n",
    "    'widths': [32, 64, 160, 416],\n",
    "    'strides': [1, 1, 2, 2],\n",
    "    'group_width': 16,\n",
    "    'bottleneck_ratio': 0.5,\n",
    "    'se_ratio': 0.25,\n",
    "    'lr': 0.1,\n",
    "    'resume': False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x0000021713552D30>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\92333\\.conda\\envs\\pytorch\\lib\\site-packages\\tqdm\\std.py\", line 1162, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\92333\\.conda\\envs\\pytorch\\lib\\site-packages\\tqdm\\notebook.py\", line 288, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 16, 32, 32]           1,024\n",
      "       BatchNorm2d-4           [-1, 16, 32, 32]              32\n",
      "            Conv2d-5           [-1, 16, 32, 32]           2,304\n",
      "       BatchNorm2d-6           [-1, 16, 32, 32]              32\n",
      "            Conv2d-7             [-1, 16, 1, 1]             272\n",
      "            Conv2d-8             [-1, 16, 1, 1]             272\n",
      "                SE-9           [-1, 16, 32, 32]               0\n",
      "           Conv2d-10           [-1, 32, 32, 32]             512\n",
      "      BatchNorm2d-11           [-1, 32, 32, 32]              64\n",
      "           Conv2d-12           [-1, 32, 32, 32]           2,048\n",
      "      BatchNorm2d-13           [-1, 32, 32, 32]              64\n",
      "            Block-14           [-1, 32, 32, 32]               0\n",
      "           Conv2d-15           [-1, 32, 32, 32]           1,024\n",
      "      BatchNorm2d-16           [-1, 32, 32, 32]              64\n",
      "           Conv2d-17           [-1, 32, 32, 32]           4,608\n",
      "      BatchNorm2d-18           [-1, 32, 32, 32]              64\n",
      "           Conv2d-19              [-1, 8, 1, 1]             264\n",
      "           Conv2d-20             [-1, 32, 1, 1]             288\n",
      "               SE-21           [-1, 32, 32, 32]               0\n",
      "           Conv2d-22           [-1, 64, 32, 32]           2,048\n",
      "      BatchNorm2d-23           [-1, 64, 32, 32]             128\n",
      "           Conv2d-24           [-1, 64, 32, 32]           2,048\n",
      "      BatchNorm2d-25           [-1, 64, 32, 32]             128\n",
      "            Block-26           [-1, 64, 32, 32]               0\n",
      "           Conv2d-27           [-1, 32, 32, 32]           2,048\n",
      "      BatchNorm2d-28           [-1, 32, 32, 32]              64\n",
      "           Conv2d-29           [-1, 32, 32, 32]           4,608\n",
      "      BatchNorm2d-30           [-1, 32, 32, 32]              64\n",
      "           Conv2d-31             [-1, 16, 1, 1]             528\n",
      "           Conv2d-32             [-1, 32, 1, 1]             544\n",
      "               SE-33           [-1, 32, 32, 32]               0\n",
      "           Conv2d-34           [-1, 64, 32, 32]           2,048\n",
      "      BatchNorm2d-35           [-1, 64, 32, 32]             128\n",
      "            Block-36           [-1, 64, 32, 32]               0\n",
      "           Conv2d-37           [-1, 32, 32, 32]           2,048\n",
      "      BatchNorm2d-38           [-1, 32, 32, 32]              64\n",
      "           Conv2d-39           [-1, 32, 32, 32]           4,608\n",
      "      BatchNorm2d-40           [-1, 32, 32, 32]              64\n",
      "           Conv2d-41             [-1, 16, 1, 1]             528\n",
      "           Conv2d-42             [-1, 32, 1, 1]             544\n",
      "               SE-43           [-1, 32, 32, 32]               0\n",
      "           Conv2d-44           [-1, 64, 32, 32]           2,048\n",
      "      BatchNorm2d-45           [-1, 64, 32, 32]             128\n",
      "            Block-46           [-1, 64, 32, 32]               0\n",
      "           Conv2d-47           [-1, 32, 32, 32]           2,048\n",
      "      BatchNorm2d-48           [-1, 32, 32, 32]              64\n",
      "           Conv2d-49           [-1, 32, 32, 32]           4,608\n",
      "      BatchNorm2d-50           [-1, 32, 32, 32]              64\n",
      "           Conv2d-51             [-1, 16, 1, 1]             528\n",
      "           Conv2d-52             [-1, 32, 1, 1]             544\n",
      "               SE-53           [-1, 32, 32, 32]               0\n",
      "           Conv2d-54           [-1, 64, 32, 32]           2,048\n",
      "      BatchNorm2d-55           [-1, 64, 32, 32]             128\n",
      "            Block-56           [-1, 64, 32, 32]               0\n",
      "           Conv2d-57           [-1, 80, 32, 32]           5,120\n",
      "      BatchNorm2d-58           [-1, 80, 32, 32]             160\n",
      "           Conv2d-59           [-1, 80, 16, 16]          11,520\n",
      "      BatchNorm2d-60           [-1, 80, 16, 16]             160\n",
      "           Conv2d-61             [-1, 16, 1, 1]           1,296\n",
      "           Conv2d-62             [-1, 80, 1, 1]           1,360\n",
      "               SE-63           [-1, 80, 16, 16]               0\n",
      "           Conv2d-64          [-1, 160, 16, 16]          12,800\n",
      "      BatchNorm2d-65          [-1, 160, 16, 16]             320\n",
      "           Conv2d-66          [-1, 160, 16, 16]          10,240\n",
      "      BatchNorm2d-67          [-1, 160, 16, 16]             320\n",
      "            Block-68          [-1, 160, 16, 16]               0\n",
      "           Conv2d-69           [-1, 80, 16, 16]          12,800\n",
      "      BatchNorm2d-70           [-1, 80, 16, 16]             160\n",
      "           Conv2d-71           [-1, 80, 16, 16]          11,520\n",
      "      BatchNorm2d-72           [-1, 80, 16, 16]             160\n",
      "           Conv2d-73             [-1, 40, 1, 1]           3,240\n",
      "           Conv2d-74             [-1, 80, 1, 1]           3,280\n",
      "               SE-75           [-1, 80, 16, 16]               0\n",
      "           Conv2d-76          [-1, 160, 16, 16]          12,800\n",
      "      BatchNorm2d-77          [-1, 160, 16, 16]             320\n",
      "            Block-78          [-1, 160, 16, 16]               0\n",
      "           Conv2d-79           [-1, 80, 16, 16]          12,800\n",
      "      BatchNorm2d-80           [-1, 80, 16, 16]             160\n",
      "           Conv2d-81           [-1, 80, 16, 16]          11,520\n",
      "      BatchNorm2d-82           [-1, 80, 16, 16]             160\n",
      "           Conv2d-83             [-1, 40, 1, 1]           3,240\n",
      "           Conv2d-84             [-1, 80, 1, 1]           3,280\n",
      "               SE-85           [-1, 80, 16, 16]               0\n",
      "           Conv2d-86          [-1, 160, 16, 16]          12,800\n",
      "      BatchNorm2d-87          [-1, 160, 16, 16]             320\n",
      "            Block-88          [-1, 160, 16, 16]               0\n",
      "           Conv2d-89           [-1, 80, 16, 16]          12,800\n",
      "      BatchNorm2d-90           [-1, 80, 16, 16]             160\n",
      "           Conv2d-91           [-1, 80, 16, 16]          11,520\n",
      "      BatchNorm2d-92           [-1, 80, 16, 16]             160\n",
      "           Conv2d-93             [-1, 40, 1, 1]           3,240\n",
      "           Conv2d-94             [-1, 80, 1, 1]           3,280\n",
      "               SE-95           [-1, 80, 16, 16]               0\n",
      "           Conv2d-96          [-1, 160, 16, 16]          12,800\n",
      "      BatchNorm2d-97          [-1, 160, 16, 16]             320\n",
      "            Block-98          [-1, 160, 16, 16]               0\n",
      "           Conv2d-99           [-1, 80, 16, 16]          12,800\n",
      "     BatchNorm2d-100           [-1, 80, 16, 16]             160\n",
      "          Conv2d-101           [-1, 80, 16, 16]          11,520\n",
      "     BatchNorm2d-102           [-1, 80, 16, 16]             160\n",
      "          Conv2d-103             [-1, 40, 1, 1]           3,240\n",
      "          Conv2d-104             [-1, 80, 1, 1]           3,280\n",
      "              SE-105           [-1, 80, 16, 16]               0\n",
      "          Conv2d-106          [-1, 160, 16, 16]          12,800\n",
      "     BatchNorm2d-107          [-1, 160, 16, 16]             320\n",
      "           Block-108          [-1, 160, 16, 16]               0\n",
      "          Conv2d-109           [-1, 80, 16, 16]          12,800\n",
      "     BatchNorm2d-110           [-1, 80, 16, 16]             160\n",
      "          Conv2d-111           [-1, 80, 16, 16]          11,520\n",
      "     BatchNorm2d-112           [-1, 80, 16, 16]             160\n",
      "          Conv2d-113             [-1, 40, 1, 1]           3,240\n",
      "          Conv2d-114             [-1, 80, 1, 1]           3,280\n",
      "              SE-115           [-1, 80, 16, 16]               0\n",
      "          Conv2d-116          [-1, 160, 16, 16]          12,800\n",
      "     BatchNorm2d-117          [-1, 160, 16, 16]             320\n",
      "           Block-118          [-1, 160, 16, 16]               0\n",
      "          Conv2d-119           [-1, 80, 16, 16]          12,800\n",
      "     BatchNorm2d-120           [-1, 80, 16, 16]             160\n",
      "          Conv2d-121           [-1, 80, 16, 16]          11,520\n",
      "     BatchNorm2d-122           [-1, 80, 16, 16]             160\n",
      "          Conv2d-123             [-1, 40, 1, 1]           3,240\n",
      "          Conv2d-124             [-1, 80, 1, 1]           3,280\n",
      "              SE-125           [-1, 80, 16, 16]               0\n",
      "          Conv2d-126          [-1, 160, 16, 16]          12,800\n",
      "     BatchNorm2d-127          [-1, 160, 16, 16]             320\n",
      "           Block-128          [-1, 160, 16, 16]               0\n",
      "          Conv2d-129           [-1, 80, 16, 16]          12,800\n",
      "     BatchNorm2d-130           [-1, 80, 16, 16]             160\n",
      "          Conv2d-131           [-1, 80, 16, 16]          11,520\n",
      "     BatchNorm2d-132           [-1, 80, 16, 16]             160\n",
      "          Conv2d-133             [-1, 40, 1, 1]           3,240\n",
      "          Conv2d-134             [-1, 80, 1, 1]           3,280\n",
      "              SE-135           [-1, 80, 16, 16]               0\n",
      "          Conv2d-136          [-1, 160, 16, 16]          12,800\n",
      "     BatchNorm2d-137          [-1, 160, 16, 16]             320\n",
      "           Block-138          [-1, 160, 16, 16]               0\n",
      "          Conv2d-139          [-1, 208, 16, 16]          33,280\n",
      "     BatchNorm2d-140          [-1, 208, 16, 16]             416\n",
      "          Conv2d-141            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-142            [-1, 208, 8, 8]             416\n",
      "          Conv2d-143             [-1, 40, 1, 1]           8,360\n",
      "          Conv2d-144            [-1, 208, 1, 1]           8,528\n",
      "              SE-145            [-1, 208, 8, 8]               0\n",
      "          Conv2d-146            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-147            [-1, 416, 8, 8]             832\n",
      "          Conv2d-148            [-1, 416, 8, 8]          66,560\n",
      "     BatchNorm2d-149            [-1, 416, 8, 8]             832\n",
      "           Block-150            [-1, 416, 8, 8]               0\n",
      "          Conv2d-151            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-152            [-1, 208, 8, 8]             416\n",
      "          Conv2d-153            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-154            [-1, 208, 8, 8]             416\n",
      "          Conv2d-155            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-156            [-1, 208, 1, 1]          21,840\n",
      "              SE-157            [-1, 208, 8, 8]               0\n",
      "          Conv2d-158            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-159            [-1, 416, 8, 8]             832\n",
      "           Block-160            [-1, 416, 8, 8]               0\n",
      "          Conv2d-161            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-162            [-1, 208, 8, 8]             416\n",
      "          Conv2d-163            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-164            [-1, 208, 8, 8]             416\n",
      "          Conv2d-165            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-166            [-1, 208, 1, 1]          21,840\n",
      "              SE-167            [-1, 208, 8, 8]               0\n",
      "          Conv2d-168            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-169            [-1, 416, 8, 8]             832\n",
      "           Block-170            [-1, 416, 8, 8]               0\n",
      "          Conv2d-171            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-172            [-1, 208, 8, 8]             416\n",
      "          Conv2d-173            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-174            [-1, 208, 8, 8]             416\n",
      "          Conv2d-175            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-176            [-1, 208, 1, 1]          21,840\n",
      "              SE-177            [-1, 208, 8, 8]               0\n",
      "          Conv2d-178            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-179            [-1, 416, 8, 8]             832\n",
      "           Block-180            [-1, 416, 8, 8]               0\n",
      "          Conv2d-181            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-182            [-1, 208, 8, 8]             416\n",
      "          Conv2d-183            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-184            [-1, 208, 8, 8]             416\n",
      "          Conv2d-185            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-186            [-1, 208, 1, 1]          21,840\n",
      "              SE-187            [-1, 208, 8, 8]               0\n",
      "          Conv2d-188            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-189            [-1, 416, 8, 8]             832\n",
      "           Block-190            [-1, 416, 8, 8]               0\n",
      "          Conv2d-191            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-192            [-1, 208, 8, 8]             416\n",
      "          Conv2d-193            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-194            [-1, 208, 8, 8]             416\n",
      "          Conv2d-195            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-196            [-1, 208, 1, 1]          21,840\n",
      "              SE-197            [-1, 208, 8, 8]               0\n",
      "          Conv2d-198            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-199            [-1, 416, 8, 8]             832\n",
      "           Block-200            [-1, 416, 8, 8]               0\n",
      "          Conv2d-201            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-202            [-1, 208, 8, 8]             416\n",
      "          Conv2d-203            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-204            [-1, 208, 8, 8]             416\n",
      "          Conv2d-205            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-206            [-1, 208, 1, 1]          21,840\n",
      "              SE-207            [-1, 208, 8, 8]               0\n",
      "          Conv2d-208            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-209            [-1, 416, 8, 8]             832\n",
      "           Block-210            [-1, 416, 8, 8]               0\n",
      "          Conv2d-211            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-212            [-1, 208, 8, 8]             416\n",
      "          Conv2d-213            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-214            [-1, 208, 8, 8]             416\n",
      "          Conv2d-215            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-216            [-1, 208, 1, 1]          21,840\n",
      "              SE-217            [-1, 208, 8, 8]               0\n",
      "          Conv2d-218            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-219            [-1, 416, 8, 8]             832\n",
      "           Block-220            [-1, 416, 8, 8]               0\n",
      "          Conv2d-221            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-222            [-1, 208, 8, 8]             416\n",
      "          Conv2d-223            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-224            [-1, 208, 8, 8]             416\n",
      "          Conv2d-225            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-226            [-1, 208, 1, 1]          21,840\n",
      "              SE-227            [-1, 208, 8, 8]               0\n",
      "          Conv2d-228            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-229            [-1, 416, 8, 8]             832\n",
      "           Block-230            [-1, 416, 8, 8]               0\n",
      "          Conv2d-231            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-232            [-1, 208, 8, 8]             416\n",
      "          Conv2d-233            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-234            [-1, 208, 8, 8]             416\n",
      "          Conv2d-235            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-236            [-1, 208, 1, 1]          21,840\n",
      "              SE-237            [-1, 208, 8, 8]               0\n",
      "          Conv2d-238            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-239            [-1, 416, 8, 8]             832\n",
      "           Block-240            [-1, 416, 8, 8]               0\n",
      "          Conv2d-241            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-242            [-1, 208, 8, 8]             416\n",
      "          Conv2d-243            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-244            [-1, 208, 8, 8]             416\n",
      "          Conv2d-245            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-246            [-1, 208, 1, 1]          21,840\n",
      "              SE-247            [-1, 208, 8, 8]               0\n",
      "          Conv2d-248            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-249            [-1, 416, 8, 8]             832\n",
      "           Block-250            [-1, 416, 8, 8]               0\n",
      "          Conv2d-251            [-1, 208, 8, 8]          86,528\n",
      "     BatchNorm2d-252            [-1, 208, 8, 8]             416\n",
      "          Conv2d-253            [-1, 208, 8, 8]          29,952\n",
      "     BatchNorm2d-254            [-1, 208, 8, 8]             416\n",
      "          Conv2d-255            [-1, 104, 1, 1]          21,736\n",
      "          Conv2d-256            [-1, 208, 1, 1]          21,840\n",
      "              SE-257            [-1, 208, 8, 8]               0\n",
      "          Conv2d-258            [-1, 416, 8, 8]          86,528\n",
      "     BatchNorm2d-259            [-1, 416, 8, 8]             832\n",
      "           Block-260            [-1, 416, 8, 8]               0\n",
      "          Linear-261                   [-1, 10]           4,170\n",
      "================================================================\n",
      "Total params: 3,373,098\n",
      "Trainable params: 3,373,098\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 44.65\n",
      "Params size (MB): 12.87\n",
      "Estimated Total Size (MB): 57.53\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Specify the net\n",
    "net = RegNet(cfg)\n",
    "net = net.to(device)\n",
    "torchsummary.summary(net,input_size=(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "if cfg['resume']:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=args.lr, betas=(0.9, 0.999),\n",
    "#                        eps=1e-08,weight_decay=0,amsgrad=False)       \n",
    "optimizer = optim.SGD(net.parameters(), lr=cfg['lr'],\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "\n",
    "    # print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        # progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        #              % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc = 100.*correct / total\n",
    "\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            # progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            #              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        test_loss = test_loss / len(testloader)\n",
    "        test_acc = 100.*correct/total\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Saving..\n",
      "Epoch: 01 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 1.320 | Train Acc: 52.33%\n",
      "\t Test. Loss: 1.275 |  Test. Acc: 54.95%\n",
      "\n",
      "Epoch: 1\n",
      "Saving..\n",
      "Epoch: 02 | Epoch Time: 1m 13s\n",
      "\tTrain Loss: 1.149 | Train Acc: 59.26%\n",
      "\t Test. Loss: 1.124 |  Test. Acc: 61.12%\n",
      "\n",
      "Epoch: 2\n",
      "Saving..\n",
      "Epoch: 03 | Epoch Time: 1m 13s\n",
      "\tTrain Loss: 0.985 | Train Acc: 65.51%\n",
      "\t Test. Loss: 0.915 |  Test. Acc: 68.68%\n",
      "\n",
      "Epoch: 3\n",
      "Saving..\n",
      "Epoch: 04 | Epoch Time: 1m 12s\n",
      "\tTrain Loss: 0.825 | Train Acc: 71.29%\n",
      "\t Test. Loss: 0.896 |  Test. Acc: 68.85%\n",
      "\n",
      "Epoch: 4\n",
      "Saving..\n",
      "Epoch: 05 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.727 | Train Acc: 74.81%\n",
      "\t Test. Loss: 0.781 |  Test. Acc: 72.87%\n",
      "\n",
      "Epoch: 5\n",
      "Saving..\n",
      "Epoch: 06 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.660 | Train Acc: 77.01%\n",
      "\t Test. Loss: 0.781 |  Test. Acc: 74.19%\n",
      "\n",
      "Epoch: 6\n",
      "Saving..\n",
      "Epoch: 07 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.606 | Train Acc: 78.99%\n",
      "\t Test. Loss: 0.765 |  Test. Acc: 75.06%\n",
      "\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x0000021713552D30>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\92333\\.conda\\envs\\pytorch\\lib\\site-packages\\tqdm\\std.py\", line 1162, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\92333\\.conda\\envs\\pytorch\\lib\\site-packages\\tqdm\\notebook.py\", line 288, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n",
      "Exception ignored in: <function tqdm.__del__ at 0x0000021713552D30>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\92333\\.conda\\envs\\pytorch\\lib\\site-packages\\tqdm\\std.py\", line 1162, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\92333\\.conda\\envs\\pytorch\\lib\\site-packages\\tqdm\\notebook.py\", line 288, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n",
      "Exception ignored in: <function tqdm.__del__ at 0x0000021713552D30>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\92333\\.conda\\envs\\pytorch\\lib\\site-packages\\tqdm\\std.py\", line 1162, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\92333\\.conda\\envs\\pytorch\\lib\\site-packages\\tqdm\\notebook.py\", line 288, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n",
      "Exception ignored in: <function tqdm.__del__ at 0x0000021713552D30>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\92333\\.conda\\envs\\pytorch\\lib\\site-packages\\tqdm\\std.py\", line 1162, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\92333\\.conda\\envs\\pytorch\\lib\\site-packages\\tqdm\\notebook.py\", line 288, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "Epoch: 08 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.548 | Train Acc: 81.23%\n",
      "\t Test. Loss: 0.646 |  Test. Acc: 77.93%\n",
      "\n",
      "Epoch: 8\n",
      "Saving..\n",
      "Epoch: 09 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.507 | Train Acc: 82.41%\n",
      "\t Test. Loss: 0.571 |  Test. Acc: 80.94%\n",
      "\n",
      "Epoch: 9\n",
      "Epoch: 10 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.465 | Train Acc: 83.86%\n",
      "\t Test. Loss: 0.622 |  Test. Acc: 78.42%\n",
      "\n",
      "Epoch: 10\n",
      "Saving..\n",
      "Epoch: 11 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.416 | Train Acc: 85.70%\n",
      "\t Test. Loss: 0.509 |  Test. Acc: 83.01%\n",
      "\n",
      "Epoch: 11\n",
      "Saving..\n",
      "Epoch: 12 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.378 | Train Acc: 86.88%\n",
      "\t Test. Loss: 0.456 |  Test. Acc: 84.51%\n",
      "\n",
      "Epoch: 12\n",
      "Epoch: 13 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.330 | Train Acc: 88.59%\n",
      "\t Test. Loss: 0.566 |  Test. Acc: 81.74%\n",
      "\n",
      "Epoch: 13\n",
      "Saving..\n",
      "Epoch: 14 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.289 | Train Acc: 89.95%\n",
      "\t Test. Loss: 0.379 |  Test. Acc: 87.44%\n",
      "\n",
      "Epoch: 14\n",
      "Saving..\n",
      "Epoch: 15 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.245 | Train Acc: 91.64%\n",
      "\t Test. Loss: 0.322 |  Test. Acc: 89.17%\n",
      "\n",
      "Epoch: 15\n",
      "Saving..\n",
      "Epoch: 16 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.204 | Train Acc: 93.05%\n",
      "\t Test. Loss: 0.312 |  Test. Acc: 89.40%\n",
      "\n",
      "Epoch: 16\n",
      "Saving..\n",
      "Epoch: 17 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.165 | Train Acc: 94.37%\n",
      "\t Test. Loss: 0.300 |  Test. Acc: 90.24%\n",
      "\n",
      "Epoch: 17\n",
      "Saving..\n",
      "Epoch: 18 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.130 | Train Acc: 95.75%\n",
      "\t Test. Loss: 0.282 |  Test. Acc: 91.07%\n",
      "\n",
      "Epoch: 18\n",
      "Saving..\n",
      "Epoch: 19 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.107 | Train Acc: 96.49%\n",
      "\t Test. Loss: 0.265 |  Test. Acc: 91.42%\n",
      "\n",
      "Epoch: 19\n",
      "Saving..\n",
      "Epoch: 20 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.095 | Train Acc: 96.99%\n",
      "\t Test. Loss: 0.259 |  Test. Acc: 91.78%\n",
      "\n",
      "Epoch: 20\n",
      "Saving..\n",
      "Epoch: 21 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.089 | Train Acc: 97.30%\n",
      "\t Test. Loss: 0.260 |  Test. Acc: 91.79%\n",
      "\n",
      "Epoch: 21\n",
      "Epoch: 22 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.090 | Train Acc: 97.12%\n",
      "\t Test. Loss: 0.260 |  Test. Acc: 91.72%\n",
      "\n",
      "Epoch: 22\n",
      "Epoch: 23 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.092 | Train Acc: 97.09%\n",
      "\t Test. Loss: 0.264 |  Test. Acc: 91.47%\n",
      "\n",
      "Epoch: 23\n",
      "Epoch: 24 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.103 | Train Acc: 96.62%\n",
      "\t Test. Loss: 0.287 |  Test. Acc: 90.94%\n",
      "\n",
      "Epoch: 24\n",
      "Epoch: 25 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.133 | Train Acc: 95.47%\n",
      "\t Test. Loss: 0.301 |  Test. Acc: 90.73%\n",
      "\n",
      "Epoch: 25\n",
      "Epoch: 26 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.184 | Train Acc: 93.56%\n",
      "\t Test. Loss: 0.352 |  Test. Acc: 88.55%\n",
      "\n",
      "Epoch: 26\n",
      "Epoch: 27 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.232 | Train Acc: 91.84%\n",
      "\t Test. Loss: 0.434 |  Test. Acc: 86.29%\n",
      "\n",
      "Epoch: 27\n",
      "Epoch: 28 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.279 | Train Acc: 90.25%\n",
      "\t Test. Loss: 0.425 |  Test. Acc: 86.22%\n",
      "\n",
      "Epoch: 28\n",
      "Epoch: 29 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.316 | Train Acc: 89.03%\n",
      "\t Test. Loss: 0.440 |  Test. Acc: 85.08%\n",
      "\n",
      "Epoch: 29\n",
      "Epoch: 30 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.350 | Train Acc: 87.78%\n",
      "\t Test. Loss: 0.447 |  Test. Acc: 85.19%\n",
      "\n",
      "Epoch: 30\n",
      "Epoch: 31 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.375 | Train Acc: 87.12%\n",
      "\t Test. Loss: 0.518 |  Test. Acc: 82.48%\n",
      "\n",
      "Epoch: 31\n",
      "Epoch: 32 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.380 | Train Acc: 86.68%\n",
      "\t Test. Loss: 0.503 |  Test. Acc: 83.43%\n",
      "\n",
      "Epoch: 32\n",
      "Epoch: 33 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.389 | Train Acc: 86.66%\n",
      "\t Test. Loss: 0.545 |  Test. Acc: 81.88%\n",
      "\n",
      "Epoch: 33\n",
      "Epoch: 34 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.402 | Train Acc: 86.08%\n",
      "\t Test. Loss: 0.572 |  Test. Acc: 80.50%\n",
      "\n",
      "Epoch: 34\n",
      "Epoch: 35 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.403 | Train Acc: 86.19%\n",
      "\t Test. Loss: 0.515 |  Test. Acc: 83.23%\n",
      "\n",
      "Epoch: 35\n",
      "Epoch: 36 | Epoch Time: 1m 12s\n",
      "\tTrain Loss: 0.404 | Train Acc: 86.09%\n",
      "\t Test. Loss: 0.554 |  Test. Acc: 82.61%\n",
      "\n",
      "Epoch: 36\n",
      "Epoch: 37 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.416 | Train Acc: 85.66%\n",
      "\t Test. Loss: 0.557 |  Test. Acc: 80.33%\n",
      "\n",
      "Epoch: 37\n",
      "Epoch: 38 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.413 | Train Acc: 85.81%\n",
      "\t Test. Loss: 0.707 |  Test. Acc: 78.30%\n",
      "\n",
      "Epoch: 38\n",
      "Epoch: 39 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.411 | Train Acc: 85.84%\n",
      "\t Test. Loss: 0.666 |  Test. Acc: 78.52%\n",
      "\n",
      "Epoch: 39\n",
      "Epoch: 40 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.411 | Train Acc: 85.78%\n",
      "\t Test. Loss: 0.538 |  Test. Acc: 81.42%\n",
      "\n",
      "Epoch: 40\n",
      "Epoch: 41 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.399 | Train Acc: 86.40%\n",
      "\t Test. Loss: 0.809 |  Test. Acc: 74.79%\n",
      "\n",
      "Epoch: 41\n",
      "Epoch: 42 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.400 | Train Acc: 86.21%\n",
      "\t Test. Loss: 0.791 |  Test. Acc: 76.44%\n",
      "\n",
      "Epoch: 42\n",
      "Epoch: 43 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.391 | Train Acc: 86.57%\n",
      "\t Test. Loss: 0.684 |  Test. Acc: 78.24%\n",
      "\n",
      "Epoch: 43\n",
      "Epoch: 44 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.382 | Train Acc: 86.92%\n",
      "\t Test. Loss: 0.677 |  Test. Acc: 77.39%\n",
      "\n",
      "Epoch: 44\n",
      "Epoch: 45 | Epoch Time: 1m 12s\n",
      "\tTrain Loss: 0.369 | Train Acc: 87.27%\n",
      "\t Test. Loss: 0.521 |  Test. Acc: 82.80%\n",
      "\n",
      "Epoch: 45\n",
      "Epoch: 46 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.360 | Train Acc: 87.56%\n",
      "\t Test. Loss: 0.608 |  Test. Acc: 79.87%\n",
      "\n",
      "Epoch: 46\n",
      "Epoch: 47 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.338 | Train Acc: 88.45%\n",
      "\t Test. Loss: 0.651 |  Test. Acc: 78.98%\n",
      "\n",
      "Epoch: 47\n",
      "Epoch: 48 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.320 | Train Acc: 89.18%\n",
      "\t Test. Loss: 0.465 |  Test. Acc: 84.17%\n",
      "\n",
      "Epoch: 48\n",
      "Epoch: 49 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.293 | Train Acc: 89.86%\n",
      "\t Test. Loss: 0.433 |  Test. Acc: 85.80%\n",
      "\n",
      "Epoch: 49\n",
      "Epoch: 50 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.274 | Train Acc: 90.56%\n",
      "\t Test. Loss: 0.392 |  Test. Acc: 87.02%\n",
      "\n",
      "Epoch: 50\n",
      "Epoch: 51 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.244 | Train Acc: 91.60%\n",
      "\t Test. Loss: 0.413 |  Test. Acc: 86.25%\n",
      "\n",
      "Epoch: 51\n",
      "Epoch: 52 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.219 | Train Acc: 92.45%\n",
      "\t Test. Loss: 0.383 |  Test. Acc: 87.12%\n",
      "\n",
      "Epoch: 52\n",
      "Epoch: 53 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.187 | Train Acc: 93.54%\n",
      "\t Test. Loss: 0.409 |  Test. Acc: 87.58%\n",
      "\n",
      "Epoch: 53\n",
      "Epoch: 54 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.154 | Train Acc: 94.67%\n",
      "\t Test. Loss: 0.275 |  Test. Acc: 91.41%\n",
      "\n",
      "Epoch: 54\n",
      "Epoch: 55 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.120 | Train Acc: 95.88%\n",
      "\t Test. Loss: 0.257 |  Test. Acc: 91.49%\n",
      "\n",
      "Epoch: 55\n",
      "Saving..\n",
      "Epoch: 56 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.090 | Train Acc: 97.05%\n",
      "\t Test. Loss: 0.234 |  Test. Acc: 92.40%\n",
      "\n",
      "Epoch: 56\n",
      "Saving..\n",
      "Epoch: 57 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.064 | Train Acc: 97.96%\n",
      "\t Test. Loss: 0.211 |  Test. Acc: 93.36%\n",
      "\n",
      "Epoch: 57\n",
      "Saving..\n",
      "Epoch: 58 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.047 | Train Acc: 98.62%\n",
      "\t Test. Loss: 0.199 |  Test. Acc: 93.76%\n",
      "\n",
      "Epoch: 58\n",
      "Saving..\n",
      "Epoch: 59 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.036 | Train Acc: 99.01%\n",
      "\t Test. Loss: 0.194 |  Test. Acc: 93.86%\n",
      "\n",
      "Epoch: 59\n",
      "Epoch: 60 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.032 | Train Acc: 99.19%\n",
      "\t Test. Loss: 0.194 |  Test. Acc: 93.83%\n",
      "\n",
      "Epoch: 60\n",
      "Epoch: 61 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.030 | Train Acc: 99.22%\n",
      "\t Test. Loss: 0.194 |  Test. Acc: 93.83%\n",
      "\n",
      "Epoch: 61\n",
      "Epoch: 62 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.031 | Train Acc: 99.28%\n",
      "\t Test. Loss: 0.193 |  Test. Acc: 93.86%\n",
      "\n",
      "Epoch: 62\n",
      "Saving..\n",
      "Epoch: 63 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.031 | Train Acc: 99.19%\n",
      "\t Test. Loss: 0.196 |  Test. Acc: 93.99%\n",
      "\n",
      "Epoch: 63\n",
      "Epoch: 64 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.032 | Train Acc: 99.14%\n",
      "\t Test. Loss: 0.201 |  Test. Acc: 93.79%\n",
      "\n",
      "Epoch: 64\n",
      "Epoch: 65 | Epoch Time: 1m 12s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.83%\n",
      "\t Test. Loss: 0.222 |  Test. Acc: 93.43%\n",
      "\n",
      "Epoch: 65\n",
      "Epoch: 66 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.073 | Train Acc: 97.52%\n",
      "\t Test. Loss: 0.240 |  Test. Acc: 92.33%\n",
      "\n",
      "Epoch: 66\n",
      "Epoch: 67 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.132 | Train Acc: 95.41%\n",
      "\t Test. Loss: 0.312 |  Test. Acc: 90.13%\n",
      "\n",
      "Epoch: 67\n"
     ]
    }
   ],
   "source": [
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "EPOCHS = 200\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    train_loss, train_acc = train(epoch)\n",
    "    test_loss, test_acc = test(epoch)\n",
    "    train_loss_history.append(train_loss)\n",
    "    test_loss_history.append(test_loss)\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    scheduler.step()\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')\n",
    "    print(f'\\t Test. Loss: {test_loss:.3f} |  Test. Acc: {test_acc:.2f} | Best Acc:{best_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(start_epoch + 200), train_loss_history, '-', linewidth=3, label = 'Train error')\n",
    "plt.plot(range(start_epoch + 200), test_loss_history, '-', linewidth=3, label = 'Test error')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac8eaebebc05f64b54d631ff4a3b0f7c44e3b1a3f62a32c174bc0acd3614f160"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
